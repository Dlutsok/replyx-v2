"""
AI Providers Manager - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç OpenAI, YandexGPT, GigaChat, Claude —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback
"""

import os
import json
import httpx
import asyncio
from typing import List, Dict, Optional, Any
from datetime import datetime, timedelta
import logging
from enum import Enum

logger = logging.getLogger(__name__)

class AIProvider(Enum):
    OPENAI = "openai"
    YANDEX = "yandex"  
    GIGACHAT = "gigachat"
    CLAUDE = "claude"
    LOCAL_LLM = "local"

class AIProvidersManager:
    def __init__(self):
        self.providers = {}
        self.proxy_config = None
        self.initialize_providers()
    
    def initialize_providers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ OpenAI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏"""
        
        # üåê –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–∫—Å–∏ –¥–ª—è OpenAI
        proxy_url = os.getenv('AI_PROXY_URL')  # http://username:password@proxy:port
        if proxy_url:
            self.proxy_config = {
                "http://": proxy_url,
                "https://": proxy_url
            }
            logger.info(f"üåê –ù–∞—Å—Ç—Ä–æ–µ–Ω –ø—Ä–æ–∫—Å–∏ –¥–ª—è OpenAI: {proxy_url}")
        else:
            logger.warning("‚ö†Ô∏è AI_PROXY_URL –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω, OpenAI –º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑ –†–§")
        
        # OpenAI (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏ –∏–∑ –†–§)
        openai_key = os.getenv('OPENAI_API_KEY')
        if openai_key:
            self.providers[AIProvider.OPENAI] = OpenAIProvider(openai_key, self.proxy_config)
            logger.info("‚úÖ OpenAI –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        else:
            logger.error("‚ùå OPENAI_API_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω!")
        
        # YandexGPT (–æ—Ç–∫–ª—é—á–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º)
        # yandex_key = os.getenv('YANDEX_API_KEY')
        # yandex_folder = os.getenv('YANDEX_FOLDER_ID')
        # if yandex_key and yandex_folder:
        #     self.providers[AIProvider.YANDEX] = YandexProvider(yandex_key, yandex_folder)
        #     logger.info("‚úÖ YandexGPT –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # GigaChat (–æ—Ç–∫–ª—é—á–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º)
        # gigachat_client_id = os.getenv('GIGACHAT_CLIENT_ID')
        # gigachat_secret = os.getenv('GIGACHAT_CLIENT_SECRET')
        # if gigachat_client_id and gigachat_secret:
        #     self.providers[AIProvider.GIGACHAT] = GigaChatProvider(gigachat_client_id, gigachat_secret)
        #     logger.info("‚úÖ GigaChat –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # Claude (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, —Ç–æ–∂–µ —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏)
        claude_key = os.getenv('CLAUDE_API_KEY')
        if claude_key:
            self.providers[AIProvider.CLAUDE] = ClaudeProvider(claude_key, self.proxy_config)
            logger.info("‚úÖ Claude –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∫–∞–∫ fallback")
        
        # –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞)
        local_url = os.getenv('LOCAL_LLM_URL')
        if local_url:
            self.providers[AIProvider.LOCAL_LLM] = LocalLLMProvider(local_url)
            logger.info("‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è LLM –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∫–∞–∫ fallback")
    
    async def get_completion(self, messages: List[Dict], model: str = None, **kwargs) -> Dict:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback –º–µ–∂–¥—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏
        –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: OpenAI (—á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏) -> Claude -> Local
        """
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ (OpenAI –ø–µ—Ä–≤—ã–º)
        priority_order = [
            AIProvider.OPENAI,    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: OpenAI (—á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏)
            AIProvider.CLAUDE,    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: Claude (—á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏, fallback)
            AIProvider.LOCAL_LLM  # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (fallback)
        ]
        
        last_error = None
        
        for provider_type in priority_order:
            if provider_type not in self.providers:
                continue
                
            provider = self.providers[provider_type]
            
            try:
                logger.info(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å {provider_type.value}")
                
                # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –ø–æ–¥ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞  
                adapted_model = self._adapt_model_for_provider(model, provider_type)
                
                result = await provider.get_completion(
                    messages=messages,
                    model=adapted_model,
                    **kwargs
                )
                
                logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç {provider_type.value}")
                result['provider_used'] = provider_type.value
                return result
                
            except Exception as e:
                last_error = e
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ {provider_type.value}: {str(e)}")
                continue
        
        # –ï—Å–ª–∏ –≤—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã
        raise Exception(f"–í—Å–µ AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_error}")
    
    def _adapt_model_for_provider(self, model: str, provider_type: AIProvider) -> str:
        """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        
        # –ú–∞–ø–ø–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π
        model_mapping = {
            AIProvider.YANDEX: {
                'gpt-4o': 'yandexgpt-lite',
                'gpt-4o-mini': 'yandexgpt'
            },
            AIProvider.GIGACHAT: {
                'gpt-4o': 'GigaChat-Pro',
                'gpt-4o-mini': 'GigaChat'
            },
            AIProvider.CLAUDE: {
                'gpt-4o': 'claude-3-opus-20240229',
                'gpt-4o-mini': 'claude-3-haiku-20240307'
            }
        }
        
        if provider_type in model_mapping and model in model_mapping[provider_type]:
            return model_mapping[provider_type][model]
        
        return model or 'default'


class BaseAIProvider:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    
    def __init__(self, name: str):
        self.name = name
    
    async def get_completion(self, messages: List[Dict], model: str = None, **kwargs) -> Dict:
        raise NotImplementedError


class OpenAIProvider(BaseAIProvider):
    """OpenAI –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∫—Å–∏"""
    
    def __init__(self, api_key: str, proxy_config: Optional[Dict] = None):
        super().__init__("OpenAI")
        self.api_key = api_key
        self.proxy_config = proxy_config
        self.base_url = "https://api.openai.com/v1"
    
    async def get_completion(self, messages: List[Dict], model: str = "gpt-4o-mini", **kwargs) -> Dict:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "temperature": kwargs.get('temperature', 0.7),
            "max_tokens": kwargs.get('max_tokens', 1000)
        }
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞ —Å –ø—Ä–æ–∫—Å–∏
        client_kwargs = {}
        if self.proxy_config:
            client_kwargs['proxies'] = self.proxy_config
            client_kwargs['timeout'] = 60.0
        
        async with httpx.AsyncClient(**client_kwargs) as client:
            response = await client.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload
            )
            response.raise_for_status()
            data = response.json()
            
            return {
                "content": data["choices"][0]["message"]["content"],
                "usage": data.get("usage", {}),
                "model": data.get("model", model)
            }


class YandexProvider(BaseAIProvider):
    """YandexGPT –ø—Ä–æ–≤–∞–π–¥–µ—Ä (—Ä–∞–±–æ—Ç–∞–µ—Ç –∏–∑ –†–§ –±–µ–∑ –ø—Ä–æ–∫—Å–∏)"""
    
    def __init__(self, api_key: str, folder_id: str):
        super().__init__("YandexGPT")
        self.api_key = api_key
        self.folder_id = folder_id
        self.base_url = "https://llm.api.cloud.yandex.net/foundationModels/v1"
    
    async def get_completion(self, messages: List[Dict], model: str = "yandexgpt", **kwargs) -> Dict:
        headers = {
            "Authorization": f"Api-Key {self.api_key}",
            "Content-Type": "application/json"
        }
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç Yandex
        yandex_messages = []
        for msg in messages:
            role = msg["role"]
            if role == "system":
                role = "system"
            elif role == "assistant":
                role = "assistant"
            else:
                role = "user"
            
            yandex_messages.append({
                "role": role,
                "text": msg["content"]
            })
        
        payload = {
            "modelUri": f"gpt://{self.folder_id}/{model}",
            "completionOptions": {
                "stream": False,
                "temperature": kwargs.get('temperature', 0.7),
                "maxTokens": kwargs.get('max_tokens', 1000)
            },
            "messages": yandex_messages
        }
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{self.base_url}/completion",
                headers=headers,
                json=payload
            )
            response.raise_for_status()
            data = response.json()
            
            return {
                "content": data["result"]["alternatives"][0]["message"]["text"],
                "usage": data["result"].get("usage", {}),
                "model": model
            }


class GigaChatProvider(BaseAIProvider):
    """GigaChat –ø—Ä–æ–≤–∞–π–¥–µ—Ä (—Ä–∞–±–æ—Ç–∞–µ—Ç –∏–∑ –†–§ –±–µ–∑ –ø—Ä–æ–∫—Å–∏)"""
    
    def __init__(self, client_id: str, client_secret: str):
        super().__init__("GigaChat")
        self.client_id = client_id
        self.client_secret = client_secret
        self.access_token = None
        self.token_expires_at = None
        self.base_url = "https://gigachat.devices.sberbank.ru/api/v1"
    
    async def _get_access_token(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ access token –¥–ª—è GigaChat"""
        if self.access_token and self.token_expires_at and datetime.now() < self.token_expires_at:
            return self.access_token
        
        import base64
        
        credentials = base64.b64encode(f"{self.client_id}:{self.client_secret}".encode()).decode()
        
        headers = {
            "Authorization": f"Basic {credentials}",
            "RqUID": str(datetime.now().timestamp()),
            "Content-Type": "application/x-www-form-urlencoded"
        }
        
        data = {"scope": "GIGACHAT_API_PERS"}
        
        async with httpx.AsyncClient(verify=False, timeout=60.0) as client:  # GigaChat —Ç—Ä–µ–±—É–µ—Ç verify=False
            response = await client.post(
                "https://ngw.devices.sberbank.ru:9443/api/v2/oauth",
                headers=headers,
                data=data
            )
            response.raise_for_status()
            token_data = response.json()
            
            self.access_token = token_data["access_token"]
            # –¢–æ–∫–µ–Ω –¥–µ–π—Å—Ç–≤—É–µ—Ç 30 –º–∏–Ω—É—Ç
            self.token_expires_at = datetime.now() + timedelta(minutes=25)
            
            return self.access_token
    
    async def get_completion(self, messages: List[Dict], model: str = "GigaChat", **kwargs) -> Dict:
        access_token = await self._get_access_token()
        
        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "temperature": kwargs.get('temperature', 0.7),
            "max_tokens": kwargs.get('max_tokens', 1000)
        }
        
        async with httpx.AsyncClient(verify=False, timeout=60.0) as client:
            response = await client.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload
            )
            response.raise_for_status()
            data = response.json()
            
            return {
                "content": data["choices"][0]["message"]["content"],
                "usage": data.get("usage", {}),
                "model": data.get("model", model)
            }


class ClaudeProvider(BaseAIProvider):
    """Claude –ø—Ä–æ–≤–∞–π–¥–µ—Ä (—á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏)"""
    
    def __init__(self, api_key: str, proxy_config: Optional[Dict] = None):
        super().__init__("Claude")
        self.api_key = api_key
        self.proxy_config = proxy_config
        self.base_url = "https://api.anthropic.com/v1"
    
    async def get_completion(self, messages: List[Dict], model: str = "claude-3-haiku-20240307", **kwargs) -> Dict:
        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        # Claude —Ç—Ä–µ–±—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–π system prompt
        system_prompt = ""
        claude_messages = []
        
        for msg in messages:
            if msg["role"] == "system":
                system_prompt += msg["content"] + "\n"
            else:
                claude_messages.append(msg)
        
        payload = {
            "model": model,
            "max_tokens": kwargs.get('max_tokens', 1000),
            "messages": claude_messages
        }
        
        if system_prompt:
            payload["system"] = system_prompt.strip()
        
        client_kwargs = {}
        if self.proxy_config:
            client_kwargs['proxies'] = self.proxy_config
            client_kwargs['timeout'] = 60.0
        
        async with httpx.AsyncClient(**client_kwargs) as client:
            response = await client.post(
                f"{self.base_url}/messages",
                headers=headers,
                json=payload
            )
            response.raise_for_status()
            data = response.json()
            
            return {
                "content": data["content"][0]["text"],
                "usage": data.get("usage", {}),
                "model": data.get("model", model)
            }


class LocalLLMProvider(BaseAIProvider):
    """–õ–æ–∫–∞–ª—å–Ω–∞—è LLM (Ollama, vLLM –∏ —Ç.–¥.)"""
    
    def __init__(self, base_url: str):
        super().__init__("LocalLLM")
        self.base_url = base_url.rstrip('/')
    
    async def get_completion(self, messages: List[Dict], model: str = "llama2", **kwargs) -> Dict:
        headers = {"Content-Type": "application/json"}
        
        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ª–æ–∫–∞–ª—å–Ω—ã—Ö API
        payload = {
            "model": model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": kwargs.get('temperature', 0.7),
                "num_predict": kwargs.get('max_tokens', 1000)
            }
        }
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                f"{self.base_url}/api/chat",
                headers=headers,
                json=payload
            )
            response.raise_for_status()
            data = response.json()
            
            return {
                "content": data["message"]["content"],
                "usage": {},
                "model": model
            }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–µ–Ω–µ–¥–∂–µ—Ä–∞
ai_providers_manager = AIProvidersManager()


async def get_ai_completion(messages: List[Dict], model: str = None, **kwargs) -> Dict:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç AI
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
    """
    return await ai_providers_manager.get_completion(messages, model, **kwargs)


def get_available_providers() -> List[str]:
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    return [provider.value for provider in ai_providers_manager.providers.keys()]


def get_provider_status() -> Dict:
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    status = {}
    for provider_type, provider in ai_providers_manager.providers.items():
        status[provider_type.value] = {
            "available": True,
            "name": provider.name
        }
    return status