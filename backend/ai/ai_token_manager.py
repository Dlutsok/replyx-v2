from sqlalchemy.orm import Session
from datetime import datetime, timedelta
from database import models
from database.connection import get_db
import random
import time
from typing import Optional, List
import openai
import os
from cache.redis_cache import chatai_cache
import hashlib
import asyncio

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–≤—ã–π AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä
try:
    from ai.ai_providers import get_ai_completion, get_available_providers
    AI_PROVIDERS_AVAILABLE = True
except ImportError:
    AI_PROVIDERS_AVAILABLE = False
    print("‚ö†Ô∏è AI Providers –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ OpenAI")

class AITokenManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –ø—É–ª–∞ AI —Ç–æ–∫–µ–Ω–æ–≤ —Å —É–º–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º + –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    
    def __init__(self):
        # –ù–µ —Å–æ–∑–¥–∞–µ–º –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º get_db() –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        pass
    
    def get_best_token(self, model: str = "gpt-4", user_id: int = None) -> Optional[models.AITokenPool]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –ª—É—á—à–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π —Ç–æ–∫–µ–Ω –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        –ê–ª–≥–æ—Ä–∏—Ç–º:
        1. –§–∏–ª—å—Ç—Ä –ø–æ –º–æ–¥–µ–ª–∏ –∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–æ–≤
        3. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ—Å—Ç–∏
        4. –í—ã–±–æ—Ä –Ω–∞–∏–º–µ–Ω–µ–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ
        """
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à (TTL: 1 –º–∏–Ω—É—Ç–∞)
        cached_token_data = chatai_cache.cache_best_token(model)
        if cached_token_data:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω –∏–∑ –ë–î –ø–æ ID
            from database.connection import SessionLocal
            db = SessionLocal()
            try:
                token = db.query(models.AITokenPool).filter(
                    models.AITokenPool.id == cached_token_data['id']
                ).first()
                if token and token.is_active:
                    print(f"üöÄ CACHE HIT: –õ—É—á—à–∏–π —Ç–æ–∫–µ–Ω –¥–ª—è {model}")
                    return token
            finally:
                db.close()
        
        print(f"üîç CACHE MISS: –ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –¥–ª—è {model}")
        
        self._reset_daily_counters()
        self._reset_monthly_counters()
        
        from database.connection import SessionLocal
        db = SessionLocal()
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–µ –Ω—É–∂–Ω—É—é –º–æ–¥–µ–ª—å
            available_tokens = db.query(models.AITokenPool).filter(
                models.AITokenPool.is_active == True,
                models.AITokenPool.model_access.contains(model)
            ).all()
            
            if not available_tokens:
                print(f"‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ {model}")
                return None
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã –ø–æ –ª–∏–º–∏—Ç–∞–º
            valid_tokens = []
            for token in available_tokens:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–Ω–µ–≤–Ω—ã–µ –ª–∏–º–∏—Ç—ã
                if token.daily_limit and token.current_daily_usage >= token.daily_limit:
                    print(f"‚ö†Ô∏è –¢–æ–∫–µ–Ω {token.name}: –ø—Ä–µ–≤—ã—à–µ–Ω –¥–Ω–µ–≤–Ω–æ–π –ª–∏–º–∏—Ç")
                    continue
                    
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Å—è—á–Ω—ã–µ –ª–∏–º–∏—Ç—ã
                if token.monthly_limit and token.current_monthly_usage >= token.monthly_limit:
                    print(f"‚ö†Ô∏è –¢–æ–∫–µ–Ω {token.name}: –ø—Ä–µ–≤—ã—à–µ–Ω –º–µ—Å—è—á–Ω—ã–π –ª–∏–º–∏—Ç")
                    continue
                    
                valid_tokens.append(token)
            
            if not valid_tokens:
                print(f"‚ùå –í—Å–µ —Ç–æ–∫–µ–Ω—ã –ø—Ä–µ–≤—ã—Å–∏–ª–∏ –ª–∏–º–∏—Ç—ã –¥–ª—è –º–æ–¥–µ–ª–∏ {model}")
                return None
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É (–±–æ–ª—å—à–µ = –ª—É—á—à–µ) –∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ—Å—Ç–∏ (–º–µ–Ω—å—à–µ = –ª—É—á—à–µ)
            valid_tokens.sort(key=lambda t: (
                t.priority,  # –í—ã—à–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                -t.current_daily_usage,  # –ú–µ–Ω—å—à–µ —Ç–µ–∫—É—â–µ–µ –¥–Ω–µ–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
                -t.current_monthly_usage  # –ú–µ–Ω—å—à–µ —Ç–µ–∫—É—â–µ–µ –º–µ—Å—è—á–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
            ), reverse=True)
            
            best_token = valid_tokens[0]
            print(f"üéØ –í—ã–±—Ä–∞–Ω –ª—É—á—à–∏–π —Ç–æ–∫–µ–Ω: {best_token.name} (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {best_token.priority})")
            
            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç (TTL: 1 –º–∏–Ω—É—Ç–∞)
            chatai_cache.set_best_token(
                model=model,
                token_data={'id': best_token.id, 'name': best_token.name},
                ttl=60
            )
            
            return best_token
        finally:
            db.close()
    
    def log_usage(self, token_id: int, user_id: int, assistant_id: int, 
                  model_used: str, prompt_tokens: int, completion_tokens: int,
                  response_time: float, success: bool = True, error_message: str = None,
                  provider_used: str = None):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∞/–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        
        from database.connection import SessionLocal
        db = SessionLocal()
        try:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫–∏ —Ç–æ–∫–µ–Ω–∞ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ token_id –Ω–µ None)
            if token_id:
                token = db.query(models.AITokenPool).filter(models.AITokenPool.id == token_id).first()
                if token:
                    token.current_daily_usage += 1
                    token.current_monthly_usage += 1
                    token.last_used = datetime.utcnow()
                    
                    if success:
                        token.error_count = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫ –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
                    else:
                        token.error_count += 1
                        token.last_error = datetime.utcnow()
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
            usage_log = models.AITokenUsage(
                token_id=token_id,
                user_id=user_id,
                assistant_id=assistant_id,
                model_used=model_used,
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=prompt_tokens + completion_tokens,
                response_time=response_time,
                success=success,
                error_message=error_message
            )
            
            db.add(usage_log)
            db.commit()
        finally:
            db.close()
    
    def _reset_daily_counters(self):
        """–°–±—Ä–æ—Å –¥–Ω–µ–≤–Ω—ã—Ö —Å—á–µ—Ç—á–∏–∫–æ–≤ –≤ –ø–æ–ª–Ω–æ—á—å"""
        now = datetime.utcnow()
        from database.connection import SessionLocal
        db = SessionLocal()
        try:
            tokens_to_reset = db.query(models.AITokenPool).filter(
                models.AITokenPool.last_reset_daily < now.replace(hour=0, minute=0, second=0, microsecond=0)
            ).all()
            
            for token in tokens_to_reset:
                token.current_daily_usage = 0
                token.last_reset_daily = now
            
            if tokens_to_reset:
                db.commit()
        finally:
            db.close()
    
    def _reset_monthly_counters(self):
        """–°–±—Ä–æ—Å –º–µ—Å—è—á–Ω—ã—Ö —Å—á–µ—Ç—á–∏–∫–æ–≤ –≤ –Ω–∞—á–∞–ª–µ –º–µ—Å—è—Ü–∞"""
        now = datetime.utcnow()
        first_day_of_month = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        
        from database.connection import SessionLocal
        db = SessionLocal()
        try:
            tokens_to_reset = db.query(models.AITokenPool).filter(
                models.AITokenPool.last_reset_monthly < first_day_of_month
            ).all()
            
            for token in tokens_to_reset:
                token.current_monthly_usage = 0
                token.last_reset_monthly = now
            
            if tokens_to_reset:
                db.commit()
        finally:
            db.close()
    
    def make_openai_request(self, messages: List[dict], model: str = "gpt-4", 
                           user_id: int = None, assistant_id: int = None,
                           temperature: float = 0.9, max_tokens: int = None,
                           presence_penalty: float = 0.3, frequency_penalty: float = 0.3,
                           is_embedding: bool = False, input_text: str = None):
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∫ AI —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≤—ã–±–æ—Ä–æ–º —Ç–æ–∫–µ–Ω–∞ –∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç fallback –º–µ–∂–¥—É OpenAI —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏ –∏ –¥—Ä—É–≥–∏–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏
        –¢–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é embeddings –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        """
        start_time = time.time()
        
        # –ï—Å–ª–∏ —ç—Ç–æ –∑–∞–ø—Ä–æ—Å –Ω–∞ embedding, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É
        if is_embedding and input_text:
            return self._make_embedding_request(input_text, model, user_id, assistant_id)
        
        # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
        if AI_PROVIDERS_AVAILABLE:
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É —Å –ø—Ä–æ–∫—Å–∏ –∏ fallback
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π event loop –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞ —Å FastAPI
                import asyncio
                loop = None
                try:
                    loop = asyncio.get_running_loop()
                except RuntimeError:
                    pass
                
                if loop is not None:
                    # –ï—Å–ª–∏ event loop —É–∂–µ –∑–∞–ø—É—â–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º thread executor
                    import concurrent.futures
                    import threading
                    
                    def run_async_in_thread():
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        try:
                            return new_loop.run_until_complete(get_ai_completion(
                                messages=messages,
                                model=model,
                                temperature=temperature,
                                max_tokens=max_tokens
                            ))
                        finally:
                            new_loop.close()
                    
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(run_async_in_thread)
                        result = future.result(timeout=30)  # 30 —Å–µ–∫—É–Ω–¥ —Ç–∞–π–º–∞—É—Ç
                else:
                    # –ï—Å–ª–∏ event loop –Ω–µ –∑–∞–ø—É—â–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º asyncio.run
                    result = asyncio.run(get_ai_completion(
                        messages=messages,
                        model=model,
                        temperature=temperature,
                        max_tokens=max_tokens
                    ))
                
                response_time = time.time() - start_time
                
                # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ (–±–µ–∑ token_id, —Ç–∞–∫ –∫–∞–∫ –ø—Ä–æ–≤–∞–π–¥–µ—Ä –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ OpenAI)
                self.log_usage(
                    token_id=None,
                    user_id=user_id,
                    assistant_id=assistant_id,
                    model_used=result.get('model', model),
                    prompt_tokens=result.get('usage', {}).get('prompt_tokens', 0),
                    completion_tokens=result.get('usage', {}).get('completion_tokens', 0),
                    response_time=response_time,
                    success=True,
                    provider_used=result.get('provider_used', 'unknown')
                )
                
                # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –æ—Ç–≤–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                class MockResponse:
                    def __init__(self, content, usage, model):
                        self.choices = [MockChoice(content)]
                        self.usage = MockUsage(usage)
                        self.model = model
                        
                class MockChoice:
                    def __init__(self, content):
                        self.message = MockMessage(content)
                        
                class MockMessage:
                    def __init__(self, content):
                        self.content = content
                        
                class MockUsage:
                    def __init__(self, usage_dict):
                        self.prompt_tokens = usage_dict.get('prompt_tokens', 0)
                        self.completion_tokens = usage_dict.get('completion_tokens', 0)
                        self.total_tokens = usage_dict.get('total_tokens', 
                            self.prompt_tokens + self.completion_tokens)
                
                return MockResponse(
                    content=result['content'],
                    usage=result.get('usage', {}),
                    model=result.get('model', model)
                )
                
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤: {e}")
                # Fallback –Ω–∞ —Å—Ç–∞—Ä—É—é —Å–∏—Å—Ç–µ–º—É —Ç–æ–∫–µ–Ω–æ–≤
        
        # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—É—é —Å–∏—Å—Ç–µ–º—É —Å –ø—É–ª–æ–º —Ç–æ–∫–µ–Ω–æ–≤ OpenAI
        print("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –Ω–∞ —Å–∏—Å—Ç–µ–º—É —Ç–æ–∫–µ–Ω–æ–≤ OpenAI")
        
        # –ü–æ–ª—É—á–∞–µ–º –ª—É—á—à–∏–π —Ç–æ–∫–µ–Ω
        token = self.get_best_token(model, user_id)
        if not token:
            raise Exception("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö AI —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ " + model)
        
        try:
            request_params = {
                "model": model,
                "messages": messages,
                "temperature": temperature
            }
            
            if max_tokens:
                request_params["max_tokens"] = max_tokens
            
            client = openai.OpenAI(api_key=token.token)
            response = client.chat.completions.create(**request_params)
            
            response_time = time.time() - start_time
            
            # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
            self.log_usage(
                token_id=token.id,
                user_id=user_id,
                assistant_id=assistant_id,
                model_used=model,
                prompt_tokens=response.usage.prompt_tokens,
                completion_tokens=response.usage.completion_tokens,
                response_time=response_time,
                success=True,
                provider_used='openai_legacy'
            )
            
            return response
            
        except Exception as e:
            response_time = time.time() - start_time
            
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
            self.log_usage(
                token_id=token.id,
                user_id=user_id,
                assistant_id=assistant_id,
                model_used=model,
                prompt_tokens=0,
                completion_tokens=0,
                response_time=response_time,
                success=False,
                error_message=str(e),
                provider_used='openai_legacy'
            )
            
            raise e
    
    def get_token_stats(self) -> List[dict]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º —Ç–æ–∫–µ–Ω–∞–º"""
        from database.connection import SessionLocal
        db = SessionLocal()
        try:
            tokens = db.query(models.AITokenPool).all()
            stats = []
            
            for token in tokens:
                daily_usage_percent = (token.current_daily_usage / max(token.daily_limit, 1)) * 100
                monthly_usage_percent = (token.current_monthly_usage / max(token.monthly_limit, 1)) * 100
                
                stats.append({
                    "id": token.id,
                    "name": token.name,
                    "is_active": token.is_active,
                    "priority": token.priority,
                    "models": token.model_access.split(','),
                    "daily_usage": token.current_daily_usage,
                    "daily_limit": token.daily_limit,
                    "daily_usage_percent": round(daily_usage_percent, 1),
                    "monthly_usage": token.current_monthly_usage,
                    "monthly_limit": token.monthly_limit,
                    "monthly_usage_percent": round(monthly_usage_percent, 1),
                    "error_count": token.error_count,
                    "last_used": token.last_used.isoformat() if token.last_used else None,
                    "last_error": token.last_error.isoformat() if token.last_error else None
                })
            
            return stats
        finally:
            db.close()
    
    def add_token(self, name: str, token: str, models_str: str = "gpt-4o,gpt-4o-mini",
                  daily_limit: int = 10000, monthly_limit: int = 300000, 
                  priority: int = 1, notes: str = None):
        """–î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω –≤ –ø—É–ª"""
        
        new_token = models.AITokenPool(
            name=name,
            token=token,
            model_access=models_str,
            daily_limit=daily_limit,
            monthly_limit=monthly_limit,
            priority=priority,
            notes=notes
        )
        
        from database.connection import SessionLocal
        db = SessionLocal()
        try:
            db.add(new_token)
            db.commit()
            db.refresh(new_token)
        finally:
            db.close()
        
        return new_token
    



# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–µ–Ω–µ–¥–∂–µ—Ä–∞
ai_token_manager = AITokenManager() 

def get_available_token(db: Session, model: str = "gpt-4o-mini"):
    """–ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞"""
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–∫–µ–Ω –∏–∑ –ø—É–ª–∞
        token = db.query(models.AITokenPool).filter(
            models.AITokenPool.is_active == True,
            models.AITokenPool.model_access.contains(model)
        ).first()
        
        if token:
            return {
                'token': token.token,
                'id': token.id,
                'model': model
            }
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø—É–ª–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
        openai_token = os.getenv('OPENAI_API_KEY')
        if openai_token:
            return {
                'token': openai_token,
                'id': None,
                'model': model
            }
        
        return None
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è AI —Ç–æ–∫–µ–Ω–∞: {e}")
        return None

    def _make_embedding_request(self, text: str, model: str, user_id: int, assistant_id: int = None):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç embedding –¥–ª—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ OpenAI API"""
        start_time = time.time()
        
        # –ü–æ–ª—É—á–∞–µ–º –ª—É—á—à–∏–π —Ç–æ–∫–µ–Ω (embeddings —Ä–∞–±–æ—Ç–∞—é—Ç —Ç–æ–ª—å–∫–æ —Å OpenAI)
        token = self.get_best_token("gpt-4o-mini", user_id)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª—é–±—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞
        if not token:
            raise Exception("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings")
        
        try:
            client = openai.OpenAI(api_key=token.token)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –¥–µ—à–µ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è embeddings
            embedding_model = model if model.startswith('text-embedding') else 'text-embedding-3-small'
            
            response = client.embeddings.create(
                model=embedding_model,
                input=text,
                encoding_format="float"
            )
            
            response_time = time.time() - start_time
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ embedding
            self.log_usage(
                token_id=token.id,
                user_id=user_id,
                assistant_id=assistant_id,
                model_used=embedding_model,
                prompt_tokens=response.usage.prompt_tokens,
                completion_tokens=0,  # –£ embeddings –Ω–µ—Ç completion tokens
                response_time=response_time,
                success=True,
                provider_used="openai_embedding"
            )
            
            return response
            
        except Exception as e:
            response_time = time.time() - start_time
            
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
            self.log_usage(
                token_id=token.id if token else None,
                user_id=user_id,
                assistant_id=assistant_id,
                model_used=model,
                prompt_tokens=0,
                completion_tokens=0,
                response_time=response_time,
                success=False,
                provider_used="openai_embedding",
                error_message=str(e)
            )
            
            raise Exception(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embedding: {e}")