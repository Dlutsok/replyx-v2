from sqlalchemy.orm import Session
from datetime import datetime, timedelta
from database import models
from database.connection import get_db
import random
import time
from typing import Optional, List
import openai
import os
from cache.redis_cache import chatai_cache
import hashlib
import asyncio

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–≤—ã–π AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä
try:
    from ai.ai_providers import get_ai_completion, get_available_providers
    AI_PROVIDERS_AVAILABLE = True
except ImportError:
    AI_PROVIDERS_AVAILABLE = False
    print("‚ö†Ô∏è AI Providers –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ OpenAI")

class AITokenManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –ø—É–ª–∞ AI —Ç–æ–∫–µ–Ω–æ–≤ —Å —É–º–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º + –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    
    def __init__(self):
        # –ù–µ —Å–æ–∑–¥–∞–µ–º –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º get_db() –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        pass
    
    def get_best_token(self, model: str = "gpt-4", user_id: int = None) -> Optional[models.AITokenPool]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –ª—É—á—à–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π —Ç–æ–∫–µ–Ω –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        –ê–ª–≥–æ—Ä–∏—Ç–º:
        1. –§–∏–ª—å—Ç—Ä –ø–æ –º–æ–¥–µ–ª–∏ –∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–æ–≤
        3. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ—Å—Ç–∏
        4. –í—ã–±–æ—Ä –Ω–∞–∏–º–µ–Ω–µ–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ
        """
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à (TTL: 1 –º–∏–Ω—É—Ç–∞)
        cached_token_data = chatai_cache.cache_best_token(model)
        if cached_token_data:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω –∏–∑ –ë–î –ø–æ ID
            from database.connection import SessionLocal
            db = SessionLocal()
            try:
                token = db.query(models.AITokenPool).filter(
                    models.AITokenPool.id == cached_token_data['id']
                ).first()
                if token and token.is_active:
                    print(f"üöÄ CACHE HIT: –õ—É—á—à–∏–π —Ç–æ–∫–µ–Ω –¥–ª—è {model}")
                    return token
            finally:
                db.close()
        
        print(f"üîç CACHE MISS: –ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –¥–ª—è {model}")
        
        self._reset_daily_counters()
        self._reset_monthly_counters()
        
        from database.connection import SessionLocal
        db = SessionLocal()
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–µ –Ω—É–∂–Ω—É—é –º–æ–¥–µ–ª—å
            available_tokens = db.query(models.AITokenPool).filter(
                models.AITokenPool.is_active == True,
                models.AITokenPool.model_access.contains(model)
            ).all()
            
            if not available_tokens:
                print(f"‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ {model}")
                return None
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã –ø–æ –ª–∏–º–∏—Ç–∞–º
            valid_tokens = []
            for token in available_tokens:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–Ω–µ–≤–Ω—ã–µ –ª–∏–º–∏—Ç—ã
                if token.daily_limit and token.current_daily_usage >= token.daily_limit:
                    print(f"‚ö†Ô∏è –¢–æ–∫–µ–Ω {token.name}: –ø—Ä–µ–≤—ã—à–µ–Ω –¥–Ω–µ–≤–Ω–æ–π –ª–∏–º–∏—Ç")
                    continue
                    
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Å—è—á–Ω—ã–µ –ª–∏–º–∏—Ç—ã
                if token.monthly_limit and token.current_monthly_usage >= token.monthly_limit:
                    print(f"‚ö†Ô∏è –¢–æ–∫–µ–Ω {token.name}: –ø—Ä–µ–≤—ã—à–µ–Ω –º–µ—Å—è—á–Ω—ã–π –ª–∏–º–∏—Ç")
                    continue
                    
                valid_tokens.append(token)
            
            if not valid_tokens:
                print(f"‚ùå –í—Å–µ —Ç–æ–∫–µ–Ω—ã –ø—Ä–µ–≤—ã—Å–∏–ª–∏ –ª–∏–º–∏—Ç—ã –¥–ª—è –º–æ–¥–µ–ª–∏ {model}")
                return None
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É (–±–æ–ª—å—à–µ = –ª—É—á—à–µ) –∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ—Å—Ç–∏ (–º–µ–Ω—å—à–µ = –ª—É—á—à–µ)
            valid_tokens.sort(key=lambda t: (
                t.priority,  # –í—ã—à–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                -t.current_daily_usage,  # –ú–µ–Ω—å—à–µ —Ç–µ–∫—É—â–µ–µ –¥–Ω–µ–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
                -t.current_monthly_usage  # –ú–µ–Ω—å—à–µ —Ç–µ–∫—É—â–µ–µ –º–µ—Å—è—á–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
            ), reverse=True)
            
            best_token = valid_tokens[0]
            print(f"üéØ –í—ã–±—Ä–∞–Ω –ª—É—á—à–∏–π —Ç–æ–∫–µ–Ω: {best_token.name} (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {best_token.priority})")
            
            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç (TTL: 1 –º–∏–Ω—É—Ç–∞)
            chatai_cache.set_best_token(
                model=model,
                token_data={'id': best_token.id, 'name': best_token.name},
                ttl=60
            )
            
            return best_token
        finally:
            db.close()
    
    def log_usage(self, token_id: int, user_id: int, assistant_id: int, 
                  model_used: str, prompt_tokens: int, completion_tokens: int,
                  response_time: float, success: bool = True, error_message: str = None,
                  provider_used: str = None):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∞/–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        
        from database.connection import SessionLocal
        db = SessionLocal()
        try:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫–∏ —Ç–æ–∫–µ–Ω–∞ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ token_id –Ω–µ None)
            if token_id:
                token = db.query(models.AITokenPool).filter(models.AITokenPool.id == token_id).first()
                if token:
                    token.current_daily_usage += 1
                    token.current_monthly_usage += 1
                    token.last_used = datetime.utcnow()
                    
                    if success:
                        token.error_count = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫ –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
                    else:
                        token.error_count += 1
                        token.last_error = datetime.utcnow()
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
            usage_log = models.AITokenUsage(
                token_id=token_id,
                user_id=user_id,
                assistant_id=assistant_id,
                model_used=model_used,
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=prompt_tokens + completion_tokens,
                response_time=response_time,
                success=success,
                error_message=error_message
            )
            
            db.add(usage_log)
            db.commit()
        finally:
            db.close()
    
    def _reset_daily_counters(self):
        """–°–±—Ä–æ—Å –¥–Ω–µ–≤–Ω—ã—Ö —Å—á–µ—Ç—á–∏–∫–æ–≤ –≤ –ø–æ–ª–Ω–æ—á—å"""
        now = datetime.utcnow()
        from database.connection import SessionLocal
        db = SessionLocal()
        try:
            tokens_to_reset = db.query(models.AITokenPool).filter(
                models.AITokenPool.last_reset_daily < now.replace(hour=0, minute=0, second=0, microsecond=0)
            ).all()
            
            for token in tokens_to_reset:
                token.current_daily_usage = 0
                token.last_reset_daily = now
            
            if tokens_to_reset:
                db.commit()
        finally:
            db.close()
    
    def _reset_monthly_counters(self):
        """–°–±—Ä–æ—Å –º–µ—Å—è—á–Ω—ã—Ö —Å—á–µ—Ç—á–∏–∫–æ–≤ –≤ –Ω–∞—á–∞–ª–µ –º–µ—Å—è—Ü–∞"""
        now = datetime.utcnow()
        first_day_of_month = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        
        from database.connection import SessionLocal
        db = SessionLocal()
        try:
            tokens_to_reset = db.query(models.AITokenPool).filter(
                models.AITokenPool.last_reset_monthly < first_day_of_month
            ).all()
            
            for token in tokens_to_reset:
                token.current_monthly_usage = 0
                token.last_reset_monthly = now
            
            if tokens_to_reset:
                db.commit()
        finally:
            db.close()
    
    def _make_embedding_request(self, text: str, model: str, user_id: int, assistant_id: int = None):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç embedding –¥–ª—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ OpenAI API —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∫—Å–∏"""
        start_time = time.time()
        
        # –ü–æ–ª—É—á–∞–µ–º –ª—É—á—à–∏–π —Ç–æ–∫–µ–Ω (embeddings —Ä–∞–±–æ—Ç–∞—é—Ç —Ç–æ–ª—å–∫–æ —Å OpenAI)
        token = self.get_best_token("gpt-4o-mini", user_id)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª—é–±—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞
        if not token:
            raise Exception("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings")
        
        api_key = token.token
        embedding_model = model if model.startswith('text-embedding') else 'text-embedding-3-small'
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–∫—Å–∏ —Å–∏—Å—Ç–µ–º—É –¥–ª—è embeddings
        from .proxy_manager import get_proxy_manager
        proxy_manager = get_proxy_manager()
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": embedding_model,
            "input": text,
            "encoding_format": "float"
        }
        
        max_attempts = 3
        last_error = None
        
        for attempt in range(max_attempts):
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–∫—Å–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç)
            proxy_url, client_kwargs = proxy_manager.get_proxy_for_request(is_stream=False, is_async=False)
            
            if not proxy_url and attempt == 0:
                # –í—Å–µ –ø—Ä–æ–∫—Å–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –ø—Ä–æ–±—É–µ–º –±–µ–∑ –ø—Ä–æ–∫—Å–∏
                client_kwargs = {"timeout": 30.0, "trust_env": False}
                print("‚ö†Ô∏è [EMBEDDINGS] –í—Å–µ –ø—Ä–æ–∫—Å–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –ø–æ–ø—ã—Ç–∫–∞ –±–µ–∑ –ø—Ä–æ–∫—Å–∏")
            
            # –ù–∞—Ö–æ–¥–∏–º —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–∫—Å–∏ –¥–ª—è –º–µ—Ç—Ä–∏–∫
            current_proxy = None
            if proxy_url:
                for proxy in proxy_manager.proxies:
                    if proxy.url == proxy_url:
                        current_proxy = proxy
                        break
                
                masked_url = proxy_manager._mask_proxy_url(proxy_url)
                print(f"üîó [EMBEDDINGS] –ü–æ–ø—ã—Ç–∫–∞ {attempt+1}/{max_attempts} —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏: {masked_url}")
            
            try:
                import httpx
                
                with httpx.Client(**client_kwargs) as client:
                    response = client.post(
                        "https://api.openai.com/v1/embeddings",
                        headers=headers,
                        json=payload
                    )
                    
                    response_time = time.time() - start_time
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∫–æ–¥
                    if response.status_code in [500, 502, 503, 504]:
                        # Upstream –æ—à–∏–±–∫–∞, –Ω–µ –ø—Ä–æ–±–ª–µ–º–∞ —Å –ø—Ä–æ–∫—Å–∏
                        if current_proxy:
                            error_type = proxy_manager.record_proxy_failure(
                                current_proxy, Exception(f"HTTP {response.status_code}"), 
                                response.status_code
                            )
                            if not proxy_manager.should_switch_proxy(error_type):
                                raise Exception(f"OpenAI API error: HTTP {response.status_code}")
                    
                    response.raise_for_status()
                    data = response.json()
                    
                    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —É—Å–ø–µ—à–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∫—Å–∏
                    if current_proxy:
                        proxy_manager.record_proxy_success(current_proxy, response_time)
                        print(f"‚úÖ [EMBEDDINGS] –£—Å–ø–µ—à–Ω—ã–π –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ '{current_proxy.name}' –∑–∞ {response_time:.2f}s")
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ embedding
                    usage_data = data.get('usage', {})
                    self.log_usage(
                        token_id=(token.id if token else None),
                        user_id=user_id,
                        assistant_id=assistant_id,
                        model_used=embedding_model,
                        prompt_tokens=usage_data.get('prompt_tokens', 0),
                        completion_tokens=0,  # –£ embeddings –Ω–µ—Ç completion tokens
                        response_time=response_time,
                        success=True,
                        provider_used=f"openai_embedding_via_proxy_{current_proxy.name if current_proxy else 'direct'}"
                    )
                    
                    # –°–æ–∑–¥–∞–µ–º mock –æ–±—ä–µ–∫—Ç response –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI
                    class MockEmbeddingResponse:
                        def __init__(self, data):
                            self.data = [MockEmbeddingData(item) for item in data.get('data', [])]
                            self.usage = MockUsage(data.get('usage', {}))
                    
                    class MockEmbeddingData:
                        def __init__(self, item):
                            self.embedding = item.get('embedding', [])
                    
                    class MockUsage:
                        def __init__(self, usage_dict):
                            self.prompt_tokens = usage_dict.get('prompt_tokens', 0)
                            self.total_tokens = usage_dict.get('total_tokens', 0)
                    
                    return MockEmbeddingResponse(data)
                    
            except httpx.HTTPStatusError as e:
                response_time = time.time() - start_time
                status = e.response.status_code if e.response else None
                body = e.response.text[:500] if e.response else ''
                
                last_error = Exception(f"HTTP {status}: {body}")
                
                if current_proxy:
                    error_type = proxy_manager.record_proxy_failure(
                        current_proxy, e, status
                    )
                    
                    if not proxy_manager.should_switch_proxy(error_type):
                        # –ù–µ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º –ø—Ä–æ–∫—Å–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ 5xx)
                        break
                
                print(f"‚ö†Ô∏è [EMBEDDINGS] HTTP –æ—à–∏–±–∫–∞ —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏ (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {status}")
                continue
                
            except (httpx.ProxyError, httpx.ConnectError, httpx.ReadTimeout, 
                   httpx.RemoteProtocolError, httpx.ConnectTimeout) as e:
                
                response_time = time.time() - start_time
                last_error = e
                
                if current_proxy:
                    error_type = proxy_manager.record_proxy_failure(current_proxy, e)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–µ–Ω –ª–∏ retry —Å —Ç–µ–º –∂–µ –ø—Ä–æ–∫—Å–∏
                    if (proxy_manager.should_retry_with_same_proxy(error_type) and 
                        attempt < max_attempts - 1):
                        
                        print(f"‚ö†Ô∏è [EMBEDDINGS] Retry —Å —Ç–µ–º –∂–µ –ø—Ä–æ–∫—Å–∏ '{current_proxy.name}': {error_type.value}")
                        time.sleep(0.5)
                        continue
                
                print(f"‚ö†Ô∏è [EMBEDDINGS] –°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–æ–∫—Å–∏ (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {repr(e)}")
                continue
                
            except Exception as e:
                response_time = time.time() - start_time
                last_error = e
                
                if current_proxy:
                    proxy_manager.record_proxy_failure(current_proxy, e)
                
                print(f"‚ö†Ô∏è [EMBEDDINGS] –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {repr(e)}")
                continue
        
        # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ—É–¥–∞—á–Ω—ã
        response_time = time.time() - start_time
        error_msg = f"Embeddings –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –ø–æ—Å–ª–µ {max_attempts} –ø–æ–ø—ã—Ç–æ–∫. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_error}"
        print(f"‚ùå [EMBEDDINGS] {error_msg}")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
        self.log_usage(
            token_id=(token.id if token else None),
            user_id=user_id,
            assistant_id=assistant_id,
            model_used=embedding_model,
            prompt_tokens=0,
            completion_tokens=0,
            response_time=response_time,
            success=False,
            provider_used="openai_embedding_failed",
            error_message=str(last_error)
        )
        
        raise Exception(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embedding: {last_error}")
    
    def make_openai_request(self, messages: List[dict], model: str = "gpt-4", 
                           user_id: int = None, assistant_id: int = None,
                           temperature: float = 0.9, max_tokens: int = None,
                           presence_penalty: float = 0.3, frequency_penalty: float = 0.3,
                           is_embedding: bool = False, input_text: str = None):
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∫ AI —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≤—ã–±–æ—Ä–æ–º —Ç–æ–∫–µ–Ω–∞ –∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç fallback –º–µ–∂–¥—É OpenAI —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏ –∏ –¥—Ä—É–≥–∏–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏
        –¢–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é embeddings –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        """
        start_time = time.time()
        
        # –ï—Å–ª–∏ —ç—Ç–æ –∑–∞–ø—Ä–æ—Å –Ω–∞ embedding, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É
        if is_embedding and input_text:
            return self._make_embedding_request(input_text, model, user_id, assistant_id)
        
        # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
        if AI_PROVIDERS_AVAILABLE:
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É —Å –ø—Ä–æ–∫—Å–∏ –∏ fallback
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π event loop –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞ —Å FastAPI
                import asyncio
                loop = None
                try:
                    loop = asyncio.get_running_loop()
                except RuntimeError:
                    pass
                
                if loop is not None:
                    # –ï—Å–ª–∏ event loop —É–∂–µ –∑–∞–ø—É—â–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º thread executor
                    import concurrent.futures
                    import threading
                    
                    def run_async_in_thread():
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        try:
                            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤–Ω—É—Ç—Ä–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ scope
                            from ai.ai_providers import get_ai_completion as ai_completion
                            return new_loop.run_until_complete(ai_completion(
                                messages=messages,
                                model=model,
                                temperature=temperature,
                                max_tokens=max_tokens
                            ))
                        finally:
                            new_loop.close()
                    
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(run_async_in_thread)
                        result = future.result(timeout=30)  # 30 —Å–µ–∫—É–Ω–¥ —Ç–∞–π–º–∞—É—Ç
                else:
                    # –ï—Å–ª–∏ event loop –Ω–µ –∑–∞–ø—É—â–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º asyncio.run
                    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ scope
                    from ai.ai_providers import get_ai_completion as ai_completion
                    result = asyncio.run(ai_completion(
                        messages=messages,
                        model=model,
                        temperature=temperature,
                        max_tokens=max_tokens
                    ))
                
                response_time = time.time() - start_time
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
                if not result or not isinstance(result, dict):
                    raise ValueError("Provider returned empty or invalid result")
                
                usage_dict = result.get('usage') or {}
                model_used = result.get('model', model)
                provider_used = result.get('provider_used', 'unknown')
                content_value = result.get('content') or result.get('text') or ""
                
                # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ (–±–µ–∑ token_id, —Ç–∞–∫ –∫–∞–∫ –ø—Ä–æ–≤–∞–π–¥–µ—Ä –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ OpenAI)
                self.log_usage(
                    token_id=None,
                    user_id=user_id,
                    assistant_id=assistant_id,
                    model_used=model_used,
                    prompt_tokens=usage_dict.get('prompt_tokens', 0),
                    completion_tokens=usage_dict.get('completion_tokens', 0),
                    response_time=response_time,
                    success=True,
                    provider_used=provider_used
                )
                
                # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –æ—Ç–≤–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                class MockResponse:
                    def __init__(self, content, usage, model):
                        self.choices = [MockChoice(content)]
                        self.usage = MockUsage(usage)
                        self.model = model
                        
                class MockChoice:
                    def __init__(self, content):
                        self.message = MockMessage(content)
                        
                class MockMessage:
                    def __init__(self, content):
                        self.content = content
                        
                class MockUsage:
                    def __init__(self, usage_dict):
                        self.prompt_tokens = (usage_dict or {}).get('prompt_tokens', 0)
                        self.completion_tokens = (usage_dict or {}).get('completion_tokens', 0)
                        self.total_tokens = (usage_dict or {}).get('total_tokens', 
                            self.prompt_tokens + self.completion_tokens)
                
                return MockResponse(
                    content=content_value,
                    usage=usage_dict,
                    model=model_used
                )
                
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤: {e}")
                # Fallback –Ω–∞ —Å—Ç–∞—Ä—É—é —Å–∏—Å—Ç–µ–º—É —Ç–æ–∫–µ–Ω–æ–≤
        
        # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—É—é —Å–∏—Å—Ç–µ–º—É —Å –ø—É–ª–æ–º —Ç–æ–∫–µ–Ω–æ–≤ OpenAI
        print("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –Ω–∞ —Å–∏—Å—Ç–µ–º—É —Ç–æ–∫–µ–Ω–æ–≤ OpenAI")
        
        # –ü–æ–ª—É—á–∞–µ–º –ª—É—á—à–∏–π —Ç–æ–∫–µ–Ω
        token = self.get_best_token(model, user_id)
        if not token:
            raise Exception("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö AI —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ " + model)
        
        # –≠—Ç–æ—Ç fallback –Ω–µ –¥–æ–ª–∂–µ–Ω –≤—ã–∑—ã–≤–∞—Ç—å—Å—è - –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –¥–æ–ª–∂–Ω–∞ —Ä–∞–±–æ—Ç–∞—Ç—å
        response_time = time.time() - start_time
        
        # –õ–æ–≥–∏—Ä—É–µ–º —á—Ç–æ fallback –±—ã–ª –≤—ã–∑–≤–∞–Ω (—ç—Ç–æ –æ—à–∏–±–∫–∞)
        self.log_usage(
            token_id=token.id,
            user_id=user_id,
            assistant_id=assistant_id,
            model_used=model,
            prompt_tokens=0,
            completion_tokens=0,
            response_time=response_time,
            success=False,
            error_message=f"Legacy fallback called for model {model} - this should not happen",
            provider_used='openai_legacy_fallback_error'
        )
        
        raise Exception(f"Legacy fallback disabled. New provider system should handle model {model}. Check AI_PROVIDERS_AVAILABLE import.")
    
    def get_token_stats(self) -> List[dict]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º —Ç–æ–∫–µ–Ω–∞–º"""
        from database.connection import SessionLocal
        db = SessionLocal()
        try:
            tokens = db.query(models.AITokenPool).all()
            stats = []
            
            for token in tokens:
                daily_usage_percent = (token.current_daily_usage / max(token.daily_limit, 1)) * 100
                monthly_usage_percent = (token.current_monthly_usage / max(token.monthly_limit, 1)) * 100
                
                stats.append({
                    "id": token.id,
                    "name": token.name,
                    "is_active": token.is_active,
                    "priority": token.priority,
                    "models": token.model_access.split(','),
                    "daily_usage": token.current_daily_usage,
                    "daily_limit": token.daily_limit,
                    "daily_usage_percent": round(daily_usage_percent, 1),
                    "monthly_usage": token.current_monthly_usage,
                    "monthly_limit": token.monthly_limit,
                    "monthly_usage_percent": round(monthly_usage_percent, 1),
                    "error_count": token.error_count,
                    "last_used": token.last_used.isoformat() if token.last_used else None,
                    "last_error": token.last_error.isoformat() if token.last_error else None
                })
            
            return stats
        finally:
            db.close()
    
    def add_token(self, name: str, token: str, models_str: str = "gpt-4o,gpt-4o-mini",
                  daily_limit: int = 10000, monthly_limit: int = 300000, 
                  priority: int = 1, notes: str = None):
        """–î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω –≤ –ø—É–ª"""
        
        new_token = models.AITokenPool(
            name=name,
            token=token,
            model_access=models_str,
            daily_limit=daily_limit,
            monthly_limit=monthly_limit,
            priority=priority,
            notes=notes
        )
        
        from database.connection import SessionLocal
        db = SessionLocal()
        try:
            db.add(new_token)
            db.commit()
            db.refresh(new_token)
        finally:
            db.close()
        
        return new_token
    



# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–µ–Ω–µ–¥–∂–µ—Ä–∞
ai_token_manager = AITokenManager() 

def get_available_token(db: Session, model: str = "gpt-4o-mini"):
    """–ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞"""
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–∫–µ–Ω –∏–∑ –ø—É–ª–∞
        token = db.query(models.AITokenPool).filter(
            models.AITokenPool.is_active == True,
            models.AITokenPool.model_access.contains(model)
        ).first()
        
        if token:
            return {
                'token': token.token,
                'id': token.id,
                'model': model
            }
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø—É–ª–µ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None
        return None
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è AI —Ç–æ–∫–µ–Ω–∞: {e}")
        return None


def count_tokens(text: str) -> int:
    """
    –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
    –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç: 1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ, 6 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
    """
    if not text or not isinstance(text, str):
        return 0
    
    # –ë–∞–∑–æ–≤—ã–π –ø–æ–¥—Å—á–µ—Ç: –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å–ª–æ–≤–∞ + —Ä—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞ + —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
    import re
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —Å–ª–æ–≤–∞ –∏ —Å–∏–º–≤–æ–ª—ã
    words = re.findall(r'\w+', text, re.UNICODE)
    spaces_and_punctuation = len(re.findall(r'[^\w]', text, re.UNICODE))
    
    # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤
    token_count = 0
    for word in words:
        if re.match(r'^[a-zA-Z]+$', word):  # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å–ª–æ–≤–∞
            token_count += max(1, len(word) // 4)
        else:  # –†—É—Å—Å–∫–∏–µ –∏ –¥—Ä—É–≥–∏–µ —Å–ª–æ–≤–∞
            token_count += max(1, len(word) // 3)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω—ã –¥–ª—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ –ø—Ä–æ–±–µ–ª–æ–≤
    token_count += max(1, spaces_and_punctuation // 2)
    
    return max(1, token_count)  # –ú–∏–Ω–∏–º—É–º 1 —Ç–æ–∫–µ–Ω