from fastapi import APIRouter, Depends, HTTPException, Query, Body, UploadFile, File, Form, BackgroundTasks
from sqlalchemy.orm import Session, joinedload
from sqlalchemy import func
from typing import List, Optional
from datetime import datetime
import json
import os
import logging
from pydantic import BaseModel

from database import models, schemas, crud, auth
from database.connection import get_db
from validators.rate_limiter import rate_limit_api
from services.s3_storage_service import get_s3_service

logger = logging.getLogger(__name__)

router = APIRouter()
def _background_index_document(doc_id: int, user_id: int, assistant_id: Optional[int], text: str, doc_type: str, importance: int = 10):
    """–§–æ–Ω–æ–≤–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞/–∑–Ω–∞–Ω–∏—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å–µ—Å—Å–∏–∏."""
    try:
        from database.connection import SessionLocal
        from services.embeddings_service import embeddings_service
        db = SessionLocal()
        try:
            embeddings_service.index_document(
                doc_id=doc_id,
                user_id=user_id,
                assistant_id=assistant_id,
                text=text,
                doc_type=doc_type,
                importance=importance,
                db=db,
            )
        finally:
            db.close()
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.warning(f"[BG_INDEX] failed: {e}")

def _background_generate_summary(doc_id: int, user_id: int):
    """–§–æ–Ω–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—ã–∂–∏–º–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ UserKnowledge(type='summary')."""
    try:
        from database.connection import SessionLocal
        db = SessionLocal()
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è analyze_document_internal
            current_user = db.query(models.User).filter(models.User.id == user_id).first()
            if not current_user:
                return
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –≤—ã–∂–∏–º–∫–∏
            existing = db.query(models.UserKnowledge).filter(
                models.UserKnowledge.doc_id == doc_id,
                models.UserKnowledge.user_id == user_id,
                models.UserKnowledge.type == 'summary'
            ).first()
            if existing:
                return
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—ã–∂–∏–º–∫—É
            result = analyze_document_internal(doc_id, current_user, db)
            summaries = result.get("summaries", [])
            doc_type = result.get("doc_type")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            knowledge = models.UserKnowledge(
                user_id=user_id,
                assistant_id=None,
                doc_id=doc_id,
                content=json.dumps(summaries),
                type='summary',
                doc_type=doc_type,
                importance=10
            )
            db.add(knowledge)
            db.commit()
        finally:
            db.close()
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.warning(f"[BG_SUMMARY] failed: {e}")


def _background_generate_summary_and_index(doc_id: int, user_id: int, assistant_id: Optional[int], text: str, doc_type: str):
    """–§–æ–Ω–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—ã–∂–∏–º–∫–∏ –∏ –ü–û–°–õ–ï–î–£–Æ–©–ê–Ø –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
    from database.connection import SessionLocal
    db = SessionLocal()
    try:
        logger.info(f"üîÑ Starting background summary generation for doc_id={doc_id}")
        
        # 1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—ã–∂–∏–º–∫—É
        _background_generate_summary(doc_id, user_id)
        logger.info(f"‚úÖ Summary generated for doc_id={doc_id}")
        
        # 2. –ü–æ–ª—É—á–∞–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤—ã–∂–∏–º–∫—É
        summary_record = db.query(models.UserKnowledge).filter(
            models.UserKnowledge.doc_id == doc_id,
            models.UserKnowledge.user_id == user_id,
            models.UserKnowledge.type == 'summary'
        ).first()
        
        text_for_embeddings = text  # Fallback to original
        
        if summary_record:
            try:
                summaries_data = json.loads(summary_record.content)
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –≤—ã–∂–∏–º–∫–∏ –≤ –µ–¥–∏–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è embeddings
                summary_parts = []
                for summary_item in summaries_data:
                    if isinstance(summary_item, dict) and 'summary' in summary_item:
                        summary_parts.append(summary_item['summary'])
                text_for_embeddings = "\n\n".join(summary_parts)
                logger.info(f"üìÑ Using cleaned summary text ({len(text_for_embeddings)} chars) for embeddings")
            except Exception as e:
                logger.warning(f"Failed to parse summary JSON, using original text: {e}")
        else:
            logger.warning("No summary found after generation, using original text")
        
        # 3. –¢–û–õ–¨–ö–û –ü–û–°–õ–ï –≤—ã–∂–∏–º–∫–∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        if assistant_id is not None:
            from services.embeddings_service import embeddings_service
            indexed_chunks = embeddings_service.index_document(
                doc_id=doc_id,
                user_id=user_id,
                assistant_id=assistant_id,
                text=text_for_embeddings,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –û–ß–ò–©–ï–ù–ù–´–ô —Ç–µ–∫—Å—Ç
                doc_type=doc_type,
                importance=10,
                db=db
            )
            logger.info(f"‚úÖ Document {doc_id} indexed with {indexed_chunks} chunks using cleaned summary for assistant {assistant_id}")
        
    except Exception as e:
        logger.warning(f"[BG_SUMMARY_INDEX] failed for doc_id={doc_id}: {e}")
    finally:
        db.close()


def _generate_summary_and_index_sync(doc_id: int, user_id: int, assistant_id: Optional[int], text: str, doc_type: str, db: Session):
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—ã–∂–∏–º–∫–∏ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è (–¥–ª—è fallback)."""
    try:
        logger.info(f"üîÑ Synchronous summary generation and indexing for doc_id={doc_id}")
        
        # 1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—ã–∂–∏–º–∫—É —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        current_user = db.query(models.User).filter(models.User.id == user_id).first()
        if not current_user:
            return
            
        result = analyze_document_internal(doc_id, current_user, db)
        summaries = result.get("summaries", [])
        doc_type_result = result.get("doc_type")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—ã–∂–∏–º–∫—É
        knowledge = models.UserKnowledge(
            user_id=user_id,
            assistant_id=None,
            doc_id=doc_id,
            content=json.dumps(summaries),
            type='summary',
            doc_type=doc_type_result,
            importance=10
        )
        db.add(knowledge)
        db.commit()
        
        # 2. –ü–æ–ª—É—á–∞–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        text_for_embeddings = text
        try:
            summary_parts = []
            for summary_item in summaries:
                if isinstance(summary_item, dict) and 'summary' in summary_item:
                    summary_parts.append(summary_item['summary'])
            if summary_parts:
                text_for_embeddings = "\n\n".join(summary_parts)
                logger.info(f"üìÑ Using cleaned summary text for embeddings")
        except Exception as e:
            logger.warning(f"Failed to process summaries, using original text: {e}")
        
        # 3. –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        if assistant_id is not None:
            from services.embeddings_service import embeddings_service
            indexed_chunks = embeddings_service.index_document(
                doc_id=doc_id,
                user_id=user_id,
                assistant_id=assistant_id,
                text=text_for_embeddings,
                doc_type=doc_type,
                importance=10,
                db=db
            )
            logger.info(f"‚úÖ Document {doc_id} indexed with {indexed_chunks} chunks using cleaned summary")
            
    except Exception as e:
        logger.warning(f"[SYNC_SUMMARY_INDEX] failed for doc_id={doc_id}: {e}")


def analyze_document_internal(doc_id: int, user: models.User, db: Session):
    """–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–≤—ã–Ω–µ—Å–µ–Ω–∞ –¥–ª—è background task)"""
    doc = db.query(models.Document).filter(models.Document.id == doc_id).first()
    if not doc:
        return None

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    from services.balance_service import BalanceService
    balance_service = BalanceService(db)

    if not balance_service.can_use_document_analysis(user.id):
        return {"error": "–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"}

    # –ü—Ä–æ–≤–æ–¥–∏–º –∞–Ω–∞–ª–∏–∑ (—Ç—è–∂–µ–ª–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è)
    try:
        file_path = f"./storage/documents/{doc.filename}"

        if doc.filename.endswith('.pdf'):
            import PyPDF2
            with open(file_path, 'rb') as pdf_file:
                pdf_reader = PyPDF2.PdfReader(pdf_file)
                text = ""
                for page in pdf_reader.pages:
                    text += page.extract_text()
        else:
            with open(file_path, 'r', encoding='utf-8') as file:
                text = file.read()

        # AI –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞
        from ai.ai_token_manager import ai_token_manager
        analysis = ai_token_manager.make_openai_request(
            messages=[{
                "role": "user",
                "content": f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –≤—ã–¥–µ–ª–∏ –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:\n\n{text[:8000]}"
            }],
            user_id=user.id
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        doc.analysis_result = analysis.choices[0].message.content if analysis.choices else "–ê–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è"
        db.commit()

        # –°–ø–∏—Å—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã
        balance_service.consume_document_analysis(user.id)

        return {"status": "success", "analysis": doc.analysis_result}

    except Exception as e:
        logger.error(f"Analysis failed for doc {doc_id}: {e}")
        return {"error": str(e)}


def _background_analyze_document(doc_id: int, user_id: int):
    """Background task –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ event loop"""
    from database.connection import SessionLocal

    db = SessionLocal()
    try:
        logger.info(f"üßµ Starting background analysis for doc_id={doc_id}")

        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –¥–æ–∫—É–º–µ–Ω—Ç
        user = db.query(models.User).filter(models.User.id == user_id).first()
        doc = db.query(models.Document).filter(models.Document.id == doc_id).first()

        if not user or not doc:
            logger.error(f"User {user_id} or document {doc_id} not found")
            return

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ (–≤—ã–Ω–µ—Å–ª–∏ —Ç—è–∂–µ–ª—É—é —Ä–∞–±–æ—Ç—É –∏–∑ event loop)
        result = analyze_document_internal(doc_id, user, db)

        logger.info(f"‚úÖ Background analysis completed for doc_id={doc_id}")
        return result

    except Exception as e:
        logger.error(f"[BG_ANALYZE] failed for doc_id={doc_id}: {e}")
    finally:
        db.close()


# get_db –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –∏–∑ database.connection

# Helper functions to avoid circular imports
def invalidate_knowledge_cache(user_id: int, assistant_id: int):
    """Wrapper –¥–ª—è –∏–Ω–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫—ç—à–∞ –∑–Ω–∞–Ω–∏–π"""
    try:
        from cache.redis_cache import chatai_cache
        chatai_cache.invalidate_knowledge_cache(user_id, assistant_id)
    except Exception:
        pass

def hot_reload_assistant_bots(assistant_id: int, db: Session):
    """–ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ"""
    try:
        from main import hot_reload_assistant_bots as _hot_reload
        return _hot_reload(assistant_id, db)
    except ImportError:
        pass

def reload_assistant_bots(assistant_id: int, db: Session):
    """–ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ"""
    try:
        from main import reload_assistant_bots as _reload
        return _reload(assistant_id, db)
    except ImportError:
        pass



def extract_document_text(doc_id: int, current_user: models.User, filename: str, file_content: bytes = None) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –µ–≥–æ —Ç–∏–ø–∞"""
    file_extension = os.path.splitext(filename)[1].lower()
    text = ""

    try:
        # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        if file_content is None:
            s3_service = get_s3_service()
            if s3_service:
                # –í—Å–µ —Ñ–∞–π–ª—ã (–≤–∫–ª—é—á–∞—è –∏–º–ø–æ—Ä—Ç —Å–∞–π—Ç–æ–≤) —Ç–µ–ø–µ—Ä—å —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ –ø–∞–ø–∫–µ documents
                file_type = "documents"

                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ S3
                object_key = s3_service.get_user_object_key(current_user.id, filename, file_type)
                file_content = s3_service.download_file(object_key)
                if file_content is None:
                    logger.error(f"Failed to download file from S3: {object_key}")
                    return ""
            else:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ (fallback)
                from validators.file_validator import file_validator
                file_path = file_validator.get_safe_upload_path(current_user.id, filename)
                if not os.path.exists(file_path):
                    logger.error(f"File not found: {file_path}")
                    return ""
                with open(file_path, 'rb') as f:
                    file_content = f.read()

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
        if file_extension == '.pdf':
            import PyPDF2
            from io import BytesIO
            pdf_file = BytesIO(file_content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            for page_num in range(len(pdf_reader.pages)):
                text += pdf_reader.pages[page_num].extract_text() + "\n"

        elif file_extension == '.docx':
            from docx import Document as DocxDoc
            from io import BytesIO
            docx_file = BytesIO(file_content)
            docx = DocxDoc(docx_file)
            text = '\n'.join([p.text for p in docx.paragraphs])

        elif file_extension == '.doc':
            # –î–ª—è .doc —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ—Å—Ç–æ–µ —á—Ç–µ–Ω–∏–µ –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞
            text = file_content.decode("utf-8", errors="ignore")

        else:
            # –î–ª—è .txt –∏ –¥—Ä—É–≥–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
            text = file_content.decode("utf-8", errors="ignore")

        return text.strip()

    except Exception as e:
        logger.error(f"Error extracting text from {filename}: {e}")
        return ""

def determine_document_type(filename: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –¥–ª—è –ª—É—á—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    filename_lower = filename.lower()
    
    if any(word in filename_lower for word in ['–∏–Ω—Å—Ç—Ä—É–∫—Ü', 'instruction', 'manual', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤']):
        return 'instruction'
    elif any(word in filename_lower for word in ['—Ä–µ–≥–ª–∞–º–µ–Ω—Ç', 'policy', 'procedure']):
        return 'regulation'
    elif any(word in filename_lower for word in ['faq', '–≤–æ–ø—Ä–æ—Å', '–æ—Ç–≤–µ—Ç', 'question']):
        return 'faq'
    elif any(word in filename_lower for word in ['–ø—Ä–∞–π—Å', 'price', '—Ü–µ–Ω', '—Ç–∞—Ä–∏—Ñ', 'cost']):
        return 'pricing'
    elif any(word in filename_lower for word in ['–∫–æ–Ω—Ç–∞–∫—Ç', 'contact', '—Ç–µ–ª–µ—Ñ–æ–Ω', 'email']):
        return 'contacts'
    elif any(word in filename_lower for word in ['–∫–æ–º–ø–∞–Ω', 'company', 'about', '–æ_–Ω–∞—Å']):
        return 'company_info'
    else:
        return 'document'

# --- User Knowledge Endpoint ---

@router.get("/user-knowledge/{user_id}")
def get_user_knowledge(user_id: int, assistant_id: int = Query(None), db: Session = Depends(get_db)):
    """
    üéØ –ò–°–ü–†–ê–í–õ–ï–ù–ê –ö–û–†–ù–ï–í–ê–Ø –ü–†–û–ë–õ–ï–ú–ê –ö–≠–®–ò–†–û–í–ê–ù–ò–Ø!
    –¢–µ–ø–µ—Ä—å —Ñ—É–Ω–∫—Ü–∏—è —É—á–∏—Ç—ã–≤–∞–µ—Ç assistant_id –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –∑–Ω–∞–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ
    """
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω—É–∂–Ω–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    target_assistant = None
    if assistant_id:
        target_assistant = db.query(models.Assistant).filter(
            models.Assistant.id == assistant_id,
            models.Assistant.user_id == user_id
        ).first()
    else:
        # –ï—Å–ª–∏ assistant_id –Ω–µ —É–∫–∞–∑–∞–Ω, –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–≥–æ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ
        target_assistant = db.query(models.Assistant).filter(
            models.Assistant.user_id == user_id,
            models.Assistant.is_active == True
        ).first()
    
    # üî• –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–æ–ª—É—á–∞–µ–º –¢–û–õ–¨–ö–û –∑–Ω–∞–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    if target_assistant:
        print(f"[GET_USER_KNOWLEDGE] –ó–∞–≥—Ä—É–∂–∞–µ–º –∑–Ω–∞–Ω–∏—è –¢–û–õ–¨–ö–û –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ {target_assistant.id} ({target_assistant.name}) –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
        # –ü–æ–ª—É—á–∞–µ–º –¢–û–õ–¨–ö–û –∑–Ω–∞–Ω–∏—è –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–µ –∫ —ç—Ç–æ–º—É –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É
        # –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è - –∫–∞–∂–¥—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º
        knowledge_with_docs = db.query(
            models.UserKnowledge,
            models.Document
        ).join(
            models.Document, 
            models.UserKnowledge.doc_id == models.Document.id
        ).filter(
            models.UserKnowledge.user_id == user_id,
            models.UserKnowledge.assistant_id == target_assistant.id  # –¢–û–õ–¨–ö–û —ç—Ç–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        ).all()
    else:
        print(f"[GET_USER_KNOWLEDGE] –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞–Ω–∏—è")
        # –ï—Å–ª–∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
        knowledge_with_docs = []
    
    print(f"[GET_USER_KNOWLEDGE] –ù–∞–π–¥–µ–Ω–æ {len(knowledge_with_docs)} –∑–∞–ø–∏—Å–µ–π –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞")
    
    documents = []
    for entry, doc in knowledge_with_docs:
        doc_type = entry.doc_type or "–î–æ–∫—É–º–µ–Ω—Ç"
        
        # –î–ª—è Quick Fix –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        if doc.filename.startswith('quick_fix_'):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å—Ç—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –±–µ–∑ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∞–π–ª
            documents.append(entry.content)
        else:
            # –î–ª—è –æ–±—ã—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–µ—Ñ–∏–∫—Å
            prefix = f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Ç–∏–ø–∞ '{doc_type}':\n"
            documents.append(prefix + entry.content)
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –Ω—É–∂–Ω–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
    system_prompt = "–¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç."
    if target_assistant:
        system_prompt = target_assistant.system_prompt or system_prompt
        print(f"[GET_USER_KNOWLEDGE] –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –æ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ {target_assistant.id}: '{system_prompt[:100]}...'")
    
    return {
        "system_prompt": system_prompt,
        "documents": documents
    }

# --- Documents CRUD ---

@router.get("/documents")
def get_documents(
    page: int = Query(1, ge=1, description="–ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã"),
    limit: int = Query(50, ge=1, le=100, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"),
    assistant_id: Optional[int] = Query(None, description="ID –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"),
    current_user: models.User = Depends(auth.get_current_user), 
    db: Session = Depends(get_db)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É"""
    
    if assistant_id is not None:
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É —á–µ—Ä–µ–∑ —Ç–∞–±–ª–∏—Ü—É UserKnowledge
        query = db.query(models.Document).join(
            models.UserKnowledge, 
            models.Document.id == models.UserKnowledge.doc_id
        ).filter(
            models.Document.user_id == current_user.id,
            models.UserKnowledge.assistant_id == assistant_id
        )
        
        total = query.count()
        offset = (page - 1) * limit
        documents = query.order_by(models.Document.upload_date.desc()).offset(offset).limit(limit).all()
    else:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        total = db.query(models.Document).filter(models.Document.user_id == current_user.id).count()
        
        offset = (page - 1) * limit
        documents = db.query(models.Document).filter(
            models.Document.user_id == current_user.id
        ).order_by(models.Document.upload_date.desc()).offset(offset).limit(limit).all()
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å—é –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
    result = {
        "documents": documents,  # –§—Ä–æ–Ω—Ç–µ–Ω–¥ –æ–∂–∏–¥–∞–µ—Ç "documents"
        "items": documents,      # –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å 
        "total": total,
        "page": page,
        "pages": (total + limit - 1) // limit,
        "limit": limit
    }
    
    return result

@router.post("/documents", response_model=schemas.DocumentRead)
@rate_limit_api(limit=10, window=300)  # 10 —Ñ–∞–π–ª–æ–≤ –∑–∞ 5 –º–∏–Ω—É—Ç
async def upload_document(
    file: UploadFile = File(...),
    assistant_id: Optional[int] = Form(None),
    current_user: models.User = Depends(auth.get_current_user),
    db: Session = Depends(get_db),
    background_tasks: BackgroundTasks = None,
):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
    from validators.file_validator import file_validator
    from services.balance_service import BalanceService
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞
    balance_service = BalanceService(db)
    if not balance_service.check_sufficient_balance(current_user.id, "document_upload"):
        raise HTTPException(
            status_code=402, 
            detail="–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ä–µ–¥—Å—Ç–≤ –Ω–∞ –±–∞–ª–∞–Ω—Å–µ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"
        )
    
    # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
    content = await file.read()
    await file.seek(0)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —É–∫–∞–∑–∞—Ç–µ–ª—å –≤ –Ω–∞—á–∞–ª–æ
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞ —Å –Ω–æ–≤—ã–º –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–æ–º
    try:
        mime_type, safe_filename = await file_validator.validate_file_content(file, content)
    except HTTPException as e:
        logger.warning(f"File upload rejected for user {current_user.id}: {e.detail}")
        raise e
    
    # –ü–æ–ª—É—á–∞–µ–º S3 —Å–µ—Ä–≤–∏—Å
    s3_service = get_s3_service()

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
    if s3_service:
        # –î–ª—è S3 –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ —Å timestamp
        import hashlib
        from datetime import datetime
        content_hash = hashlib.sha256(content).hexdigest()[:16]
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        secure_filename = f"{current_user.id}_{timestamp}_{content_hash}.docx"
    else:
        secure_filename = file_validator.generate_secure_filename(
            user_id=current_user.id,
            original_filename=safe_filename,
            content=content
        )

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
    file_path = None
    try:
        if s3_service:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ S3 (–¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –ø–∞–ø–∫—É documents)
            object_key = f"users/{current_user.id}/documents/{secure_filename}"

            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–∏—Å—ã –≤–º–µ—Å—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–π –¥–ª—è Timeweb Cloud)
            metadata = {
                'user-id': str(current_user.id),
                'original-filename': safe_filename,
                'upload-time': datetime.now().isoformat()
            }

            upload_result = s3_service.upload_file(
                file_content=content,
                object_key=object_key,
                content_type=mime_type,
                metadata=metadata
            )

            if not upload_result.get('success'):
                raise Exception(f"S3 upload failed: {upload_result.get('error')}")

            logger.info(f"File uploaded to S3: {object_key} by user {current_user.id}")

        else:
            # Fallback: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ
            file_path = file_validator.get_safe_upload_path(current_user.id, secure_filename)
            with open(file_path, "wb") as f:
                f.write(content)
            logger.info(f"File uploaded locally: {file_path} by user {current_user.id}")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –≤ –ë–î —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º –∏–º–µ–Ω–µ–º —Ñ–∞–π–ª–∞
        doc = crud.create_document(db, current_user.id, secure_filename, len(content))
        
        # –°–ø–∏—Å—ã–≤–∞–µ–º —Å—Ä–µ–¥—Å—Ç–≤–∞ –∑–∞ –∑–∞–≥—Ä—É–∑–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
        try:
            balance_service.charge_for_service(current_user.id, "document_upload")
            logger.info(f"Charged user {current_user.id} for document upload")
        except Exception as e:
            logger.error(f"Failed to charge user {current_user.id} for document upload: {e}")
            # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å, –µ—Å–ª–∏ —Å–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å
        
        # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        from cache.redis_cache import cache
        cache.delete_pattern(f"user_documents:{current_user.id}:*")
        
        # üöÄ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –ò–ù–î–ï–ö–°–ê–¶–ò–Ø –ß–ï–†–ï–ó EMBEDDINGS (–ë–ï–ó HOT-RELOAD)
        # –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥: –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¢–û–õ–¨–ö–û –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞, –µ—Å–ª–∏ –æ–Ω —É–∫–∞–∑–∞–Ω
        try:
            logger.info(f"Starting automatic embedding indexing for doc_id={doc.id}, user_id={current_user.id}")

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø–µ—Ä–µ–¥–∞–µ–º —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç)
            text = extract_document_text(doc.id, current_user, secure_filename, content)
            # –°—á–∏—Ç–∞–µ–º doc_hash –¥–ª—è –≥—Ä—É–±–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π
            try:
                import hashlib
                doc_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
                doc.doc_hash = doc_hash
                db.commit()
            except Exception:
                pass
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ª—É—á—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            doc_type = determine_document_type(secure_filename)
            
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å embeddings
            from services.embeddings_service import embeddings_service

            target_assistant_id: Optional[int] = None
            if assistant_id is not None:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
                assistant = db.query(models.Assistant).filter(
                    models.Assistant.id == int(assistant_id),
                    models.Assistant.user_id == current_user.id
                ).first()
                if not assistant:
                    raise HTTPException(status_code=404, detail="Assistant not found")
                target_assistant_id = assistant.id

            if target_assistant_id is not None:
                # –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å—å –∑–Ω–∞–Ω–∏–π, —á—Ç–æ–±—ã –¥–æ–∫—É–º–µ–Ω—Ç –±—ã–ª –ø—Ä–∏–≤—è–∑–∞–Ω –∫ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É –∏ –æ—Ç–æ–±—Ä–∞–∂–∞–ª—Å—è –≤ UI
                try:
                    existing_knowledge = db.query(models.UserKnowledge).filter(
                        models.UserKnowledge.user_id == current_user.id,
                        models.UserKnowledge.assistant_id == target_assistant_id,
                        models.UserKnowledge.doc_id == doc.id
                    ).first()
                    if not existing_knowledge:
                        knowledge = models.UserKnowledge(
                            user_id=current_user.id,
                            assistant_id=target_assistant_id,
                            doc_id=doc.id,
                            content=text,  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –∫–∞–∫ –∑–Ω–∞–Ω–∏–µ 'original'
                            type='original',
                            doc_type=doc_type,
                            importance=10
                        )
                        db.add(knowledge)
                        db.commit()
                except Exception as e:
                    logger.warning(f"[UPLOAD_DOCUMENT] failed to create UserKnowledge link: {e}")
                # –ó–∞–ø—É—Å–∫–∞–µ–º –§–û–ù–û–í–£–Æ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤—ã–∂–∏–º–∫–∏ –∏ –ø–æ—Å–ª–µ–¥—É—é—â—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
                if background_tasks is not None:
                    logger.info(f"üßµ Scheduling background summary generation and indexing for doc_id={doc.id}")
                    background_tasks.add_task(_background_generate_summary_and_index, doc.id, current_user.id, target_assistant_id, text, doc_type)
                else:
                    # –ï—Å–ª–∏ background_tasks –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –¥–µ–ª–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ—Å—Ç–æ–≤)
                    logger.info(f"üîÑ Generating summary and indexing synchronously for doc_id={doc.id}")
                    _generate_summary_and_index_sync(doc.id, current_user.id, target_assistant_id, text, doc_type, db)
            else:
                # –ë–µ–∑ assistant_id –±–æ–ª—å—à–µ –ù–ï –≤—ã–ø–æ–ª–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –¥–ª—è –≤—Å–µ—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ (–∏–∑–±–µ–≥–∞–µ–º "—Ä–∞–∑–º–∞–∑—ã–≤–∞–Ω–∏—è")
                logger.info("‚ÑπÔ∏è Document uploaded without assistant_id - skipping assistant-specific embedding indexing")

        except Exception as e:
            logger.error(f"Failed to index document embeddings {doc.id}: {e}")
            # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≥—Ä—É–∑–∫–∏, –µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å
            # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–º–æ–∂–µ—Ç –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∑–∂–µ
        
        return doc
        
    except Exception as e:
        logger.error(f"Error saving file: {e}")
        # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª –µ—Å–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –≤ –ë–î –Ω–µ —É–¥–∞–ª–æ—Å—å
        if file_path and os.path.exists(file_path):
            os.remove(file_path)
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞")

@router.post("/documents/import-website", response_model=schemas.DocumentRead)
def import_website(
    data: dict,
    current_user: models.User = Depends(auth.get_current_user),
    db: Session = Depends(get_db)
):
    """–ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å–∞–π—Ç–∞ –ø–æ URL –∫–∞–∫ –¥–æ–∫—É–º–µ–Ω—Ç –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –ø–æ–¥ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
    from services.embeddings_service import embeddings_service
    from services.balance_service import BalanceService
    import re
    import time
    import requests
    try:
        from bs4 import BeautifulSoup  # type: ignore
    except Exception:
        raise HTTPException(status_code=500, detail="–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å beautifulsoup4 –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ")

    url = (data or {}).get('url')
    assistant_id = (data or {}).get('assistant_id')
    if not url:
        raise HTTPException(status_code=400, detail="url –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∫–∞–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    balance_service = BalanceService(db)
    if not balance_service.check_sufficient_balance(current_user.id, "document_upload"):
        raise HTTPException(status_code=402, detail="–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ä–µ–¥—Å—Ç–≤ –Ω–∞ –±–∞–ª–∞–Ω—Å–µ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ —Å–∞–π—Ç–∞")

    # –í—ã—Ç—è–≥–∏–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    try:
        headers = {"User-Agent": "ChatAI-Importer/1.0"}
        resp = requests.get(url, headers=headers, timeout=20)
        resp.raise_for_status()
        html = resp.text
    except Exception as e:
        logger.error(f"Website fetch failed: {e}")
        raise HTTPException(status_code=400, detail="–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É URL")

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
    soup = BeautifulSoup(html, 'html.parser')
    # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã/—Å—Ç–∏–ª–∏
    for tag in soup(["script", "style", "noscript"]):
        tag.extract()
    text = soup.get_text("\n")
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r"\n{2,}", "\n\n", text)
    text = text.strip()
    if not text:
        raise HTTPException(status_code=400, detail="–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã")

    # –ì–æ—Ç–æ–≤–∏–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        host = (parsed.netloc or 'site').replace(':', '_').replace('.', '_')
    except Exception:
        host = 'site'

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫—Ä–∞—Å–∏–≤–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –∫–∞–∫ —É –æ–±—ã—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    from datetime import datetime
    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    import hashlib
    content_hash = hashlib.sha256(text.encode('utf-8')).hexdigest()[:6]
    safe_name = f"–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å —Å–∞–π—Ç–∞ ‚Äî {host} {timestamp}_{content_hash}.md"

    # –ü–æ–ª—É—á–∞–µ–º S3 —Å–µ—Ä–≤–∏—Å
    s3_service = get_s3_service()
    content_bytes = text.encode('utf-8')
    size = len(content_bytes)

    try:
        if s3_service:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ S3 (–∫–∞–∫ –æ–±—ã—á–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã)
            object_key = f"users/{current_user.id}/documents/{safe_name}"

            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                'user-id': str(current_user.id),
                'original-url': url,
                'upload-time': datetime.now().isoformat(),
                'source': 'website-import'
            }

            upload_result = s3_service.upload_file(
                file_content=content_bytes,
                object_key=object_key,
                content_type='text/markdown',
                metadata=metadata
            )

            if not upload_result.get('success'):
                raise Exception(f"S3 upload failed: {upload_result.get('error')}")

            logger.info(f"Website content uploaded to S3: {object_key} by user {current_user.id}")

        else:
            # Fallback: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ
            upload_dir = os.path.join("uploads", str(current_user.id))
            os.makedirs(upload_dir, exist_ok=True)
            file_path = os.path.join(upload_dir, safe_name)
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(text)
            logger.info(f"Website content uploaded locally: {file_path} by user {current_user.id}")

        doc = crud.create_document(db, current_user.id, safe_name, size)

        # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        target_assistant_id = None
        if assistant_id is not None:
            assistant = db.query(models.Assistant).filter(
                models.Assistant.id == int(assistant_id),
                models.Assistant.user_id == current_user.id
            ).first()
            if not assistant:
                raise HTTPException(status_code=404, detail="Assistant not found")
            target_assistant_id = assistant.id

        if target_assistant_id is not None:
            try:
                embeddings_service.index_document(
                    doc_id=doc.id,
                    user_id=current_user.id,
                    assistant_id=target_assistant_id,
                    text=text,
                    doc_type='website',
                    importance=10,
                    db=db
                )
            except Exception as e:
                logger.warning(f"Embedding index failed for website doc_id={doc.id}: {e}")

        # –°–ø–∏—Å–∞–Ω–∏–µ —Å—Ä–µ–¥—Å—Ç–≤
        try:
            balance_service.charge_for_service(current_user.id, "document_upload", f"–ò–º–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ {host}")
        except Exception as e:
            logger.error(f"Charge failed after website import: {e}")

        return doc
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Website import failed: {e}")
        raise HTTPException(status_code=500, detail="–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ —Å–∞–π—Ç–∞")

@router.delete("/documents/{doc_id}")
def delete_document(doc_id: int, current_user: models.User = Depends(auth.get_current_user), db: Session = Depends(get_db)):
    """
    –£–ª—É—á—à–µ–Ω–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø–æ–ª–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π –≤—Å–µ—Ö —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    from utils.bot_cleanup import enhanced_document_deletion
    
    try:
        success = enhanced_document_deletion(doc_id, current_user.id, db)
        
        if success:
            logger.info(f"[DELETE_DOCUMENT] ‚úÖ Document {doc_id} completely deleted with full cleanup")
            return {"ok": True}
        else:
            raise HTTPException(status_code=404, detail="Document not found or deletion failed")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[DELETE_DOCUMENT] Error: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")

@router.get("/documents/{doc_id}/text")
def get_document_text(doc_id: int, current_user: models.User = Depends(auth.get_current_user), db: Session = Depends(get_db)):
    doc = db.query(models.Document).filter(models.Document.id == doc_id, models.Document.user_id == current_user.id).first()
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")

    try:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Ñ—É–Ω–∫—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–µ—Ä–µ—Ç S3 –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ)
        text = extract_document_text(doc.id, current_user, doc.filename)
        if not text:
            raise HTTPException(status_code=404, detail="File not found or empty")
        return {"text": text}
    except Exception as e:
        logger.error(f"Error getting document text for doc_id={doc_id}: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")


@router.get("/documents/{doc_id}/summary")
def get_document_summary(doc_id: int, current_user: models.User = Depends(auth.get_current_user), db: Session = Depends(get_db)):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—ã–∂–∏–º–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –ë–î; –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ ‚Äî –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç."""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–ª–∞–¥–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–º
    doc = db.query(models.Document).filter(
        models.Document.id == doc_id,
        models.Document.user_id == current_user.id
    ).first()
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")

    # –ò—â–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –≤—ã–∂–∏–º–∫—É
    existing = db.query(models.UserKnowledge).filter(
        models.UserKnowledge.doc_id == doc_id,
        models.UserKnowledge.user_id == current_user.id,
        models.UserKnowledge.type == 'summary'
    ).first()

    if existing:
        try:
            content = json.loads(existing.content)
        except Exception:
            content = existing.content
        return {"summaries": content, "doc_type": existing.doc_type}

    # –ï—Å–ª–∏ –Ω–µ—Ç ‚Äî –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    try:
        result = analyze_document_internal(doc_id, current_user, db)
        summaries = result.get("summaries", [])
        doc_type = result.get("doc_type")

        try:
            knowledge = models.UserKnowledge(
                user_id=current_user.id,
                assistant_id=None,  # –≥–ª–æ–±–∞–ª—å–Ω–∞—è –≤—ã–∂–∏–º–∫–∞, –Ω–µ –ø—Ä–∏–≤—è–∑–∞–Ω–∞ –∫ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É
                doc_id=doc_id,
                content=json.dumps(summaries),
                type='summary',
                doc_type=doc_type,
                importance=10
            )
            db.add(knowledge)
            db.commit()
        except Exception:
            db.rollback()
        return {"summaries": summaries, "doc_type": doc_type}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Summary generation failed: {e}")

def analyze_document_internal(doc_id: int, current_user: models.User, db: Session):
    """–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    print(f"[analyze_document_internal] Analyzing doc_id={doc_id} for user {current_user.id}")
    doc = db.query(models.Document).filter(models.Document.id == doc_id, models.Document.user_id == current_user.id).first()
    if not doc:
        print(f"[analyze_document_internal] Document not found in DB for user_id={current_user.id}, doc_id={doc_id}")
        raise Exception("Document not found")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –ø—É–ª —Ç–æ–∫–µ–Ω–æ–≤ –≤–º–µ—Å—Ç–æ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
    from ai.ai_token_manager import ai_token_manager
    
    # --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ ---
    try:
        text = extract_document_text(doc_id, current_user, doc.filename)
        if not text:
            raise Exception("Failed to extract text from document")
    except Exception as e:
        print(f"[analyze_document_internal] –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
        raise Exception(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è gpt-4o (16384 —Ç–æ–∫–µ–Ω–∞)
    # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ 1 —Ç–æ–∫–µ–Ω = 4 —Å–∏–º–≤–æ–ª–∞, –æ—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ –∏ –æ—Ç–≤–µ—Ç–∞
    total_length = len(text)
    if total_length < 40000:
        chunk_size = total_length  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ü–µ–ª–∏–∫–æ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–æ 40KB –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
    elif total_length < 120000:
        chunk_size = 30000  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –¥–µ—Ç–∞–ª–µ–π
    else:
        chunk_size = 20000   # –î–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ç–æ–∂–µ –±–æ–ª—å—à–∏–µ —á–∞–Ω–∫–∏
    
    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
    chunks = []
    current_pos = 0
    while current_pos < len(text):
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–ª–∏ –∞–±–∑–∞—Ü–∞ –¥–ª—è –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        end_pos = min(current_pos + chunk_size, len(text))
        if end_pos < len(text):
            # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–ª–∏ –∞–±–∑–∞—Ü–∞
            for separator in ['\n\n', '\n', '. ', '! ', '? ']:
                last_separator = text.rfind(separator, current_pos, end_pos)
                if last_separator > current_pos + chunk_size // 2:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –Ω–µ –≤ –Ω–∞—á–∞–ª–µ —á–∞–Ω–∫–∞
                    end_pos = last_separator + len(separator)
                    break
        
        chunks.append(text[current_pos:end_pos])
        current_pos = end_pos
    
    summaries = []
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    doc_type_prompt = f"–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è, —Ä–µ–≥–ª–∞–º–µ–Ω—Ç, —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫, —Å—Ç–∞—Ç—å—è –∏ —Ç.–¥.) –∏ –µ–≥–æ –æ—Å–Ω–æ–≤–Ω—É—é —Ç–µ–º—É. –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å: {text[:500]}..."
    try:
        doc_type_response = ai_token_manager.make_openai_request(
            messages=[
                {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –∏ —Ç–µ–º—É –¥–æ–∫—É–º–µ–Ω—Ç–∞."},
                {"role": "user", "content": doc_type_prompt}
            ],
            model="gpt-4o",
            user_id=current_user.id,
            assistant_id=None
        )
        doc_type = doc_type_response.choices[0].message.content.strip()
    except Exception as e:
        print(f"[analyze_document_internal] –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
        doc_type = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞"
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    for idx, chunk in enumerate(chunks):
        prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Ç–∏–ø–∞ "{doc_type}" –∏ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –≤—ã–¥–µ–ª–∏ –í–°–ï –≤–∞–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏:

1. –ö–õ–Æ–ß–ï–í–´–ï –§–ê–ö–¢–´ (—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –í–°–ï–• –¥–µ—Ç–∞–ª–µ–π):
   - –ù–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π, –ø—Ä–æ–¥—É–∫—Ç–æ–≤
   - –î–∞—Ç—ã, —Ü–∏—Ñ—Ä—ã, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
   - –ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (email, —Ç–µ–ª–µ—Ñ–æ–Ω—ã, –∞–¥—Ä–µ—Å–∞)
   - –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã, —á–∞—Å—ã, —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è

2. –£–°–õ–£–ì–ò –ò –ü–†–û–î–£–ö–¢–´ (—Å –ü–û–õ–ù–´–ú–ò –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏):
   - –¢–æ—á–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫—É—Ä—Å–æ–≤, –ø—Ä–æ–≥—Ä–∞–º–º, —É—Å–ª—É–≥
   - –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
   - –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

3. FAQ –ò –í–û–ü–†–û–°–´ (—Å–æ—Ö—Ä–∞–Ω–∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ):
   - –í–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã –ø–æ–ª–Ω–æ—Å—Ç—å—é
   - –ü—Ä–∞–≤–∏–ª–∞ –∏ —É—Å–ª–æ–≤–∏—è
   - –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º

4. –§–£–ù–ö–¶–ò–ò –ò –ó–ê–î–ê–ß–ò (–µ—Å–ª–∏ –µ—Å—Ç—å):
   - –ß—Ç–æ —É–º–µ–µ—Ç –¥–µ–ª–∞—Ç—å —Å–∏—Å—Ç–µ–º–∞/–±–æ—Ç
   - –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

6. üí∞ –¶–ï–ù–´ –ò –¢–ê–†–ò–§–´ (–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –∏–∑–≤–ª–µ–∫–∏ –í–°–ï —Ü–µ–Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ):
   - –°—Ç–æ–∏–º–æ—Å—Ç—å —É—Å–ª—É–≥ (–≤–∫–ª—é—á–∞—è —Ü–µ–Ω—ã —Å –ø–æ–º–µ—Ç–∫–∞–º–∏ "???", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ", "~", "–ø—Ä–∏–º–µ—Ä–Ω–æ")
   - –ü–ª–∞–Ω—ã –∏ –ø–∞–∫–µ—Ç—ã
   - –°–∫–∏–¥–∫–∏ –∏ –∞–∫—Ü–∏–∏
   - –û–°–û–ë–û –í–ê–ñ–ù–û: –µ—Å–ª–∏ —Ü–µ–Ω–∞ —É–∫–∞–∑–∞–Ω–∞ –∫–∞–∫ "???", "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ", "~", "–ø—Ä–∏–º–µ—Ä–Ω–æ" - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —Å–æ—Ö—Ä–∞–Ω–∏ —ç—Ç—É –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å

7. üö® –ü–†–û–ë–õ–ï–ú–´ –ò –ñ–ê–õ–û–ë–´ –ö–õ–ò–ï–ù–¢–û–í (–ù–û–í–´–ô –†–ê–ó–î–ï–õ - –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–π):
   - –ñ–∞–ª–æ–±—ã –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ —Å–µ—Ä–≤–∏—Å–∞ ("–Ω–µ –ø–µ—Ä–µ–∑–≤–æ–Ω–∏–ª–∏", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–ø—Ä–æ–±–ª–µ–º—ã")
   - –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–∑—ã–≤—ã –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
   - –£–ø–æ–º–∏–Ω–∞–Ω–∏—è –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å –º–µ–Ω–µ–¥–∂–µ—Ä–∞–º–∏, –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
   - –õ—é–±—ã–µ —Ñ—Ä–∞–∑—ã —Ç–∏–ø–∞: "–æ–±–µ—â–∞–ª–∏, –Ω–æ...", "—Å–∫–∞–∑–∞–ª–∏, —á—Ç–æ –±—É–¥–µ—Ç, –Ω–æ...", "–¥–æ —Å–∏—Ö –ø–æ—Ä –Ω–µ—Ç"
   - –°–∞—Ä–∫–∞–∑–º –∏ –∏—Ä–æ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ

8. üìù –û–°–û–ë–ï–ù–ù–û–°–¢–ò –î–û–ö–£–ú–ï–ù–¢–ê:
   - –°—Ç–∏–ª—å –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π, —é–º–æ—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π, —Å–∞—Ç–∏—Ä–∞)
   - –ü–æ–ª–Ω–æ—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (–µ—Å—Ç—å –ª–∏ –ø—Ä–æ–ø—É—Å–∫–∏, –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–∏)
   - –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
   - –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

9. üîç –ù–ï–û–ü–†–ï–î–ï–õ–ï–ù–ù–û–°–¢–ò –ò –°–ü–ï–¶–°–ò–ú–í–û–õ–´ (–ù–û–í–´–ô –†–ê–ó–î–ï–õ):
   - –í—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è "???", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ", "–•–ó", "~", "–ø—Ä–∏–º–µ—Ä–Ω–æ"
   - –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (—Ä–∞–∑–Ω—ã–µ –¥–∞—Ç—ã, —Ü–∏—Ñ—Ä—ã)
   - –ù–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–µ—Ç–∫–∞–º–∏
   - –≠–º–æ–¥–∑–∏ —Å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º (ü§∑‚Äç‚ôÇÔ∏è, ü§î)

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
- –ù–ï –æ–±–æ–±—â–∞–π –∏ –ù–ï –ø—Ä–æ–ø—É—Å–∫–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ—Ç–∞–ª–∏!
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï —É–ø–æ–º—è–Ω—É—Ç—ã–µ email –∞–¥—Ä–µ—Å–∞ —Ç–æ—á–Ω–æ –∫–∞–∫ –Ω–∞–ø–∏—Å–∞–Ω–æ, –æ—Ç–º–µ—Ç—å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–µ –Ω–æ–º–µ—Ä–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é, –≤–∫–ª—é—á–∞—è –Ω–µ–ø–æ–ª–Ω—ã–µ
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï –Ω–∞–∑–≤–∞–Ω–∏—è –∫—É—Ä—Å–æ–≤ –∏ —É—Å–ª—É–≥ –¥–æ—Å–ª–æ–≤–Ω–æ
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï FAQ –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã —Ü–µ–ª–∏–∫–æ–º
- –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –∏–∑–≤–ª–µ–∫–∞–π –∂–∞–ª–æ–±—ã –∏ –ø—Ä–æ–±–ª–µ–º—ã –∫–ª–∏–µ–Ω—Ç–æ–≤
- –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —Å–æ—Ö—Ä–∞–Ω—è–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ü–µ–Ω–∞—Ö –∏ –¥–∞–Ω–Ω—ã—Ö
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–ø–æ–ª–Ω–∞—è, —é–º–æ—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–æ–±–ª–µ–º—ã - —á–µ—Ç–∫–æ —É–∫–∞–∂–∏ —ç—Ç–æ

–¢–µ–∫—Å—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞:
{chunk}

–°–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤—ã–∂–∏–º–∫—É –¥–ª—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –í–°–ï –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –ø—Ä–æ–±–ª–µ–º—ã –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏.
"""
        try:
            response = ai_token_manager.make_openai_request(
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –∑–Ω–∞–Ω–∏–π. –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: —Å–æ—Ö—Ä–∞–Ω—è–π –í–°–ï –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ - email –∞–¥—Ä–µ—Å–∞, —Ç–µ–ª–µ—Ñ–æ–Ω—ã, –Ω–∞–∑–≤–∞–Ω–∏—è, —Ü–µ–Ω—ã, –¥–∞—Ç—ã. –ù–ï –æ–±–æ–±—â–∞–π –∏ –ù–ï –ø—Ä–æ–ø—É—Å–∫–∞–π –¥–µ—Ç–∞–ª–∏! –û–°–û–ë–û –í–ê–ñ–ù–û: –∏–∑–≤–ª–µ–∫–∞–π –∂–∞–ª–æ–±—ã –∫–ª–∏–µ–Ω—Ç–æ–≤, –ø—Ä–æ–±–ª–µ–º—ã —Å–µ—Ä–≤–∏—Å–∞, –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ü–µ–Ω–∞—Ö (???, ~, –ø—Ä–∏–º–µ—Ä–Ω–æ). –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Å–æ–∑–¥–∞—Ç—å –ü–û–õ–ù–£–Æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤—ã–∂–∏–º–∫—É –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –≤–∫–ª—é—á–∞—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –ø—Ä–æ–±–ª–µ–º—ã, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º –¥–ª—è —á–µ—Å—Ç–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º."},
                    {"role": "user", "content": prompt}
                ],
                model="gpt-4o",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å —Å –±–æ–ª—å—à–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                user_id=current_user.id,
                assistant_id=None,
                temperature=0.05  # –ï—â–µ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            )
            summary = response.choices[0].message.content.strip()
        except Exception as e:
            print(f"[analyze_document_internal] –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —á–∞–Ω–∫–∞ {idx+1}: {e}")
            summary = f"[–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}]"
        
        summaries.append({"chunk": idx+1, "summary": summary})
    
    # –°–æ–∑–¥–∞–µ–º –æ–±—â–µ–µ —Ä–µ–∑—é–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    if len(chunks) > 1:
        try:
            all_summaries = "\n\n".join([s["summary"] for s in summaries])
            final_prompt = f"""
–ù–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –≤—ã–∂–∏–º–æ–∫ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Ç–∏–ø–∞ "{doc_type}", —Å–æ–∑–¥–∞–π –µ–¥–∏–Ω–æ–µ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞:

{all_summaries}

–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ, –∫–æ—Ç–æ—Ä–æ–µ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
"""
            final_response = ai_token_manager.make_openai_request(
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –°–æ–∑–¥–∞–π –µ–¥–∏–Ω–æ–µ —Ä–µ–∑—é–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã–∂–∏–º–æ–∫ –∏–∑ –µ–≥–æ —á–∞—Å—Ç–µ–π."},
                    {"role": "user", "content": final_prompt}
                ],
                model="gpt-4o",
                user_id=current_user.id,
                assistant_id=None
            )
            document_summary = final_response.choices[0].message.content.strip()
            summaries.append({"chunk": 0, "summary": document_summary, "is_overall": True})
        except Exception as e:
            print(f"[analyze_document_internal] –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—â–µ–≥–æ —Ä–µ–∑—é–º–µ: {e}")
    
    print(f"[analyze_document_internal] Analysis complete for doc_id={doc_id}, user_id={current_user.id}")
    
    return {"summaries": summaries, "doc_type": doc_type}

@router.get("/documents/{doc_id}/summary-status")
def get_document_summary_status(doc_id: int, current_user: models.User = Depends(auth.get_current_user), db: Session = Depends(get_db)):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—É—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã–∂–∏–º–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–ª–∞–¥–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–º
    doc = db.query(models.Document).filter(
        models.Document.id == doc_id,
        models.Document.user_id == current_user.id
    ).first()
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –≤—ã–∂–∏–º–∫–∞
    existing_summary = db.query(models.UserKnowledge).filter(
        models.UserKnowledge.doc_id == doc_id,
        models.UserKnowledge.user_id == current_user.id,
        models.UserKnowledge.type == 'summary'
    ).first()

    if existing_summary:
        return {
            "status": "completed",
            "has_summary": True,
            "created_at": existing_summary.created_at
        }
    else:
        return {
            "status": "processing",
            "has_summary": False,
            "created_at": None
        }

@router.post("/analyze-document/{doc_id}")
async def analyze_document(
    doc_id: int,
    background_tasks: BackgroundTasks,
    current_user: models.User = Depends(auth.get_current_user),
    db: Session = Depends(get_db)
):
    print(f"[analyze_document] User {current_user.id} ({current_user.email}) requests analysis for doc_id={doc_id}")
    doc = db.query(models.Document).filter(models.Document.id == doc_id, models.Document.user_id == current_user.id).first()
    if not doc:
        print(f"[analyze_document] Document not found in DB for user_id={current_user.id}, doc_id={doc_id}")
        raise HTTPException(status_code=404, detail="Document not found")
    
# –§–∞–π–ª –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ S3 –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –≤ —Ñ—É–Ω–∫—Ü–∏–∏ extract_document_text
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –ø—É–ª —Ç–æ–∫–µ–Ω–æ–≤ –≤–º–µ—Å—Ç–æ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
    from ai.ai_token_manager import ai_token_manager
    
    # --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ ---
    try:
        text = extract_document_text(doc_id, current_user, doc.filename)
        if not text:
            raise HTTPException(status_code=404, detail="Failed to extract text from document")
    except Exception as e:
        print(f"[analyze_document] –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è gpt-4o (16384 —Ç–æ–∫–µ–Ω–∞)
    # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ 1 —Ç–æ–∫–µ–Ω = 4 —Å–∏–º–≤–æ–ª–∞, –æ—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ –∏ –æ—Ç–≤–µ—Ç–∞
    total_length = len(text)
    if total_length < 40000:
        chunk_size = total_length  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ü–µ–ª–∏–∫–æ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–æ 40KB –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
    elif total_length < 120000:
        chunk_size = 30000  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –¥–µ—Ç–∞–ª–µ–π
    else:
        chunk_size = 20000   # –î–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ç–æ–∂–µ –±–æ–ª—å—à–∏–µ —á–∞–Ω–∫–∏
    
    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
    chunks = []
    current_pos = 0
    while current_pos < len(text):
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–ª–∏ –∞–±–∑–∞—Ü–∞ –¥–ª—è –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        end_pos = min(current_pos + chunk_size, len(text))
        if end_pos < len(text):
            # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–ª–∏ –∞–±–∑–∞—Ü–∞
            for separator in ['\n\n', '\n', '. ', '! ', '? ']:
                last_separator = text.rfind(separator, current_pos, end_pos)
                if last_separator > current_pos + chunk_size // 2:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –Ω–µ –≤ –Ω–∞—á–∞–ª–µ —á–∞–Ω–∫–∞
                    end_pos = last_separator + len(separator)
                    break
        
        chunks.append(text[current_pos:end_pos])
        current_pos = end_pos
    
    summaries = []
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    doc_type_prompt = f"–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è, —Ä–µ–≥–ª–∞–º–µ–Ω—Ç, —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫, —Å—Ç–∞—Ç—å—è –∏ —Ç.–¥.) –∏ –µ–≥–æ –æ—Å–Ω–æ–≤–Ω—É—é —Ç–µ–º—É. –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å: {text[:500]}..."
    try:
        doc_type_response = ai_token_manager.make_openai_request(
            messages=[
                {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –∏ —Ç–µ–º—É –¥–æ–∫—É–º–µ–Ω—Ç–∞."},
                {"role": "user", "content": doc_type_prompt}
            ],
            model="gpt-4o",
            user_id=current_user.id,
            assistant_id=None
        )
        doc_type = doc_type_response.choices[0].message.content.strip()
    except Exception as e:
        print(f"[analyze_document] –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
        doc_type = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞"
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    for idx, chunk in enumerate(chunks):
        prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Ç–∏–ø–∞ "{doc_type}" –∏ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –≤—ã–¥–µ–ª–∏ –í–°–ï –≤–∞–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏:

1. –ö–õ–Æ–ß–ï–í–´–ï –§–ê–ö–¢–´ (—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –í–°–ï–• –¥–µ—Ç–∞–ª–µ–π):
   - –ù–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π, –ø—Ä–æ–¥—É–∫—Ç–æ–≤
   - –î–∞—Ç—ã, —Ü–∏—Ñ—Ä—ã, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
   - –ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (email, —Ç–µ–ª–µ—Ñ–æ–Ω—ã, –∞–¥—Ä–µ—Å–∞)
   - –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã, —á–∞—Å—ã, —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è

2. –£–°–õ–£–ì–ò –ò –ü–†–û–î–£–ö–¢–´ (—Å –ü–û–õ–ù–´–ú–ò –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏):
   - –¢–æ—á–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫—É—Ä—Å–æ–≤, –ø—Ä–æ–≥—Ä–∞–º–º, —É—Å–ª—É–≥
   - –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
   - –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

3. FAQ –ò –í–û–ü–†–û–°–´ (—Å–æ—Ö—Ä–∞–Ω–∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ):
   - –í–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã –ø–æ–ª–Ω–æ—Å—Ç—å—é
   - –ü—Ä–∞–≤–∏–ª–∞ –∏ —É—Å–ª–æ–≤–∏—è
   - –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º

4. –§–£–ù–ö–¶–ò–ò –ò –ó–ê–î–ê–ß–ò (–µ—Å–ª–∏ –µ—Å—Ç—å):
   - –ß—Ç–æ —É–º–µ–µ—Ç –¥–µ–ª–∞—Ç—å —Å–∏—Å—Ç–µ–º–∞/–±–æ—Ç
   - –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

6. üí∞ –¶–ï–ù–´ –ò –¢–ê–†–ò–§–´ (–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –∏–∑–≤–ª–µ–∫–∏ –í–°–ï —Ü–µ–Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ):
   - –°—Ç–æ–∏–º–æ—Å—Ç—å —É—Å–ª—É–≥ (–≤–∫–ª—é—á–∞—è —Ü–µ–Ω—ã —Å –ø–æ–º–µ—Ç–∫–∞–º–∏ "???", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ", "~", "–ø—Ä–∏–º–µ—Ä–Ω–æ")
   - –ü–ª–∞–Ω—ã –∏ –ø–∞–∫–µ—Ç—ã
   - –°–∫–∏–¥–∫–∏ –∏ –∞–∫—Ü–∏–∏
   - –û–°–û–ë–û –í–ê–ñ–ù–û: –µ—Å–ª–∏ —Ü–µ–Ω–∞ —É–∫–∞–∑–∞–Ω–∞ –∫–∞–∫ "???", "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ", "~", "–ø—Ä–∏–º–µ—Ä–Ω–æ" - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —Å–æ—Ö—Ä–∞–Ω–∏ —ç—Ç—É –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å

7. üö® –ü–†–û–ë–õ–ï–ú–´ –ò –ñ–ê–õ–û–ë–´ –ö–õ–ò–ï–ù–¢–û–í (–ù–û–í–´–ô –†–ê–ó–î–ï–õ - –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–π):
   - –ñ–∞–ª–æ–±—ã –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ —Å–µ—Ä–≤–∏—Å–∞ ("–Ω–µ –ø–µ—Ä–µ–∑–≤–æ–Ω–∏–ª–∏", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–ø—Ä–æ–±–ª–µ–º—ã")
   - –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–∑—ã–≤—ã –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
   - –£–ø–æ–º–∏–Ω–∞–Ω–∏—è –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å –º–µ–Ω–µ–¥–∂–µ—Ä–∞–º–∏, –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
   - –õ—é–±—ã–µ —Ñ—Ä–∞–∑—ã —Ç–∏–ø–∞: "–æ–±–µ—â–∞–ª–∏, –Ω–æ...", "—Å–∫–∞–∑–∞–ª–∏, —á—Ç–æ –±—É–¥–µ—Ç, –Ω–æ...", "–¥–æ —Å–∏—Ö –ø–æ—Ä –Ω–µ—Ç"
   - –°–∞—Ä–∫–∞–∑–º –∏ –∏—Ä–æ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ

8. üìù –û–°–û–ë–ï–ù–ù–û–°–¢–ò –î–û–ö–£–ú–ï–ù–¢–ê:
   - –°—Ç–∏–ª—å –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π, —é–º–æ—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π, —Å–∞—Ç–∏—Ä–∞)
   - –ü–æ–ª–Ω–æ—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (–µ—Å—Ç—å –ª–∏ –ø—Ä–æ–ø—É—Å–∫–∏, –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–∏)
   - –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
   - –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

9. üîç –ù–ï–û–ü–†–ï–î–ï–õ–ï–ù–ù–û–°–¢–ò –ò –°–ü–ï–¶–°–ò–ú–í–û–õ–´ (–ù–û–í–´–ô –†–ê–ó–î–ï–õ):
   - –í—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è "???", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ", "–•–ó", "~", "–ø—Ä–∏–º–µ—Ä–Ω–æ"
   - –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (—Ä–∞–∑–Ω—ã–µ –¥–∞—Ç—ã, —Ü–∏—Ñ—Ä—ã)
   - –ù–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–µ—Ç–∫–∞–º–∏
   - –≠–º–æ–¥–∑–∏ —Å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º (ü§∑‚Äç‚ôÇÔ∏è, ü§î)

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
- –ù–ï –æ–±–æ–±—â–∞–π –∏ –ù–ï –ø—Ä–æ–ø—É—Å–∫–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ—Ç–∞–ª–∏!
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï —É–ø–æ–º—è–Ω—É—Ç—ã–µ email –∞–¥—Ä–µ—Å–∞ —Ç–æ—á–Ω–æ –∫–∞–∫ –Ω–∞–ø–∏—Å–∞–Ω–æ, –æ—Ç–º–µ—Ç—å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–µ –Ω–æ–º–µ—Ä–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é, –≤–∫–ª—é—á–∞—è –Ω–µ–ø–æ–ª–Ω—ã–µ
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï –Ω–∞–∑–≤–∞–Ω–∏—è –∫—É—Ä—Å–æ–≤ –∏ —É—Å–ª—É–≥ –¥–æ—Å–ª–æ–≤–Ω–æ
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï FAQ –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã —Ü–µ–ª–∏–∫–æ–º
- –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –∏–∑–≤–ª–µ–∫–∞–π –∂–∞–ª–æ–±—ã –∏ –ø—Ä–æ–±–ª–µ–º—ã –∫–ª–∏–µ–Ω—Ç–æ–≤
- –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —Å–æ—Ö—Ä–∞–Ω—è–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ü–µ–Ω–∞—Ö –∏ –¥–∞–Ω–Ω—ã—Ö
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–ø–æ–ª–Ω–∞—è, —é–º–æ—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–æ–±–ª–µ–º—ã - —á–µ—Ç–∫–æ —É–∫–∞–∂–∏ —ç—Ç–æ

–¢–µ–∫—Å—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞:
{chunk}

–°–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤—ã–∂–∏–º–∫—É –¥–ª—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –í–°–ï –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –ø—Ä–æ–±–ª–µ–º—ã –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏.
"""
        try:
            response = ai_token_manager.make_openai_request(
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –∑–Ω–∞–Ω–∏–π. –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: —Å–æ—Ö—Ä–∞–Ω—è–π –í–°–ï –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ - email –∞–¥—Ä–µ—Å–∞, —Ç–µ–ª–µ—Ñ–æ–Ω—ã, –Ω–∞–∑–≤–∞–Ω–∏—è, —Ü–µ–Ω—ã, –¥–∞—Ç—ã. –ù–ï –æ–±–æ–±—â–∞–π –∏ –ù–ï –ø—Ä–æ–ø—É—Å–∫–∞–π –¥–µ—Ç–∞–ª–∏! –û–°–û–ë–û –í–ê–ñ–ù–û: –∏–∑–≤–ª–µ–∫–∞–π –∂–∞–ª–æ–±—ã –∫–ª–∏–µ–Ω—Ç–æ–≤, –ø—Ä–æ–±–ª–µ–º—ã —Å–µ—Ä–≤–∏—Å–∞, –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ü–µ–Ω–∞—Ö (???, ~, –ø—Ä–∏–º–µ—Ä–Ω–æ). –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Å–æ–∑–¥–∞—Ç—å –ü–û–õ–ù–£–Æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤—ã–∂–∏–º–∫—É –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –≤–∫–ª—é—á–∞—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –ø—Ä–æ–±–ª–µ–º—ã, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º –¥–ª—è —á–µ—Å—Ç–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º."},
                    {"role": "user", "content": prompt}
                ],
                model="gpt-4o",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å —Å –±–æ–ª—å—à–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                user_id=current_user.id,
                assistant_id=None,
                temperature=0.05  # –ï—â–µ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            )
            summary = response.choices[0].message.content.strip()
        except Exception as e:
            print(f"[analyze_document] –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —á–∞–Ω–∫–∞ {idx+1}: {e}")
            summary = f"[–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}]"
        
        summaries.append({"chunk": idx+1, "summary": summary})
    
    # –°–æ–∑–¥–∞–µ–º –æ–±—â–µ–µ —Ä–µ–∑—é–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    if len(chunks) > 1:
        try:
            all_summaries = "\n\n".join([s["summary"] for s in summaries])
            final_prompt = f"""
–ù–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –≤—ã–∂–∏–º–æ–∫ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Ç–∏–ø–∞ "{doc_type}", —Å–æ–∑–¥–∞–π –µ–¥–∏–Ω–æ–µ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞:

{all_summaries}

–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ, –∫–æ—Ç–æ—Ä–æ–µ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
"""
            final_response = ai_token_manager.make_openai_request(
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –°–æ–∑–¥–∞–π –µ–¥–∏–Ω–æ–µ —Ä–µ–∑—é–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã–∂–∏–º–æ–∫ –∏–∑ –µ–≥–æ —á–∞—Å—Ç–µ–π."},
                    {"role": "user", "content": final_prompt}
                ],
                model="gpt-4o",
                user_id=current_user.id,
                assistant_id=None
            )
            document_summary = final_response.choices[0].message.content.strip()
            summaries.append({"chunk": 0, "summary": document_summary, "is_overall": True})
        except Exception as e:
            print(f"[analyze_document] –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—â–µ–≥–æ —Ä–µ–∑—é–º–µ: {e}")
    
    print(f"[analyze_document] Analysis complete for doc_id={doc_id}, user_id={current_user.id}")
    
    # üî• –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –≥–æ—Ä—è—á–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –±–æ—Ç–æ–≤ –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    # (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –µ—Å—Ç—å –Ω–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –±–æ—Ç–æ–≤)
    assistants = db.query(models.Assistant).filter(models.Assistant.user_id == current_user.id).all()
    print(f"[analyze_document] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ {len(assistants)} –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    for assistant in assistants:
        hot_reload_assistant_bots(assistant.id, db)
    
    return {"summaries": summaries, "doc_type": doc_type}

# --- Knowledge Management ---

class KnowledgeIn(BaseModel):
    doc_id: int
    content: str
    type: str  # 'summary' –∏–ª–∏ 'original'
    doc_type: Optional[str] = None
    importance: Optional[int] = 10
    assistant_id: Optional[int] = None  # –î–æ–±–∞–≤–ª—è–µ–º assistant_id

@router.post("/knowledge/confirm")
def confirm_knowledge(data: KnowledgeIn, current_user: models.User = Depends(auth.get_current_user), db: Session = Depends(get_db), background_tasks: BackgroundTasks = None):
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –¥–æ–∫—É–º–µ–Ω—Ç –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    doc = db.query(models.Document).filter(models.Document.id == data.doc_id, models.Document.user_id == current_user.id).first()
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")
    # –î–æ–±–∞–≤–∏—Ç—å –∑–Ω–∞–Ω–∏–µ
    knowledge = models.UserKnowledge(
        user_id=current_user.id,
        assistant_id=data.assistant_id,  # –ü—Ä–∏–≤—è–∑—ã–≤–∞–µ–º –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É
        doc_id=data.doc_id,
        content=data.content,
        type=data.type,
        doc_type=data.doc_type,
        importance=data.importance or 10
    )
    db.add(knowledge)
    db.commit()
    db.refresh(knowledge)
    
    # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à –∑–Ω–∞–Ω–∏–π
    invalidate_knowledge_cache(current_user.id, data.assistant_id)

    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω–æ–µ –∑–Ω–∞–Ω–∏–µ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ (incremental)
    try:
        if background_tasks is not None:
            background_tasks.add_task(
                _background_index_document,
                knowledge.doc_id,
                current_user.id,
                data.assistant_id,
                knowledge.content,
                data.doc_type or 'confirmed_knowledge',
                data.importance or 10,
            )
        else:
            from services.embeddings_service import embeddings_service
            embeddings_service.index_document(
                doc_id=knowledge.doc_id,
                user_id=current_user.id,
                assistant_id=data.assistant_id,
                text=knowledge.content,
                doc_type=data.doc_type or 'confirmed_knowledge',
                importance=data.importance or 10,
                db=db,
            )
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.warning(f"[CONFIRM_KNOWLEDGE] indexing failed: {e}")

    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Ä—Å–∏—é –∑–Ω–∞–Ω–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞(–æ–≤), —á—Ç–æ–±—ã –∫—ç—à –æ—Ç–≤–µ—Ç–æ–≤ –ø–µ—Ä–µ—Å—Ç–∞–ª —Å–æ–≤–ø–∞–¥–∞—Ç—å
    try:
        from services.embeddings_service import embeddings_service
        if data.assistant_id:
            embeddings_service.increment_knowledge_version(data.assistant_id, db)
        else:
            assistants = db.query(models.Assistant).filter(models.Assistant.user_id == current_user.id).all()
            for assistant in assistants:
                embeddings_service.increment_knowledge_version(assistant.id, db)
    except Exception as _e:
        logger = logging.getLogger(__name__)
        logger.warning(f"[CONFIRM_KNOWLEDGE] Failed to bump knowledge version: {_e}")
    
    # üî• –ì–û–†–Ø–ß–ê–Ø –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –∏–ª–∏ –≤—Å–µ—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤
    print(f"[CONFIRM_KNOWLEDGE] –î–æ–±–∞–≤–ª–µ–Ω–æ –∑–Ω–∞–Ω–∏–µ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {current_user.id}, –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç {data.assistant_id}")
    if data.assistant_id:
        print(f"[CONFIRM_KNOWLEDGE] –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –±–æ—Ç–æ–≤ –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ {data.assistant_id}")
        hot_reload_assistant_bots(data.assistant_id, db)
    else:
        # –ï—Å–ª–∏ –∑–Ω–∞–Ω–∏–µ –æ–±—â–µ–µ - –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        assistants = db.query(models.Assistant).filter(models.Assistant.user_id == current_user.id).all()
        print(f"[CONFIRM_KNOWLEDGE] –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –±–æ—Ç–æ–≤ –¥–ª—è {len(assistants)} –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
        for assistant in assistants:
            hot_reload_assistant_bots(assistant.id, db)
    
    return {"ok": True, "id": knowledge.id}

@router.get("/knowledge/confirmed")
def get_confirmed_knowledge(
    assistant_id: int = Query(None),
    page: int = Query(1, ge=1, description="–ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã"),
    limit: int = Query(50, ge=1, le=100, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"),
    current_user: models.User = Depends(auth.get_current_user), 
    db: Session = Depends(get_db)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
    
    # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å EAGER LOADING –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è N+1
    query = db.query(models.UserKnowledge).options(
        joinedload(models.UserKnowledge.document)
    ).filter(models.UserKnowledge.user_id == current_user.id)
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É
    if assistant_id:
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
        assistant = db.query(models.Assistant).filter(
            models.Assistant.id == assistant_id,
            models.Assistant.user_id == current_user.id
        ).first()
        if not assistant:
            raise HTTPException(status_code=404, detail="Assistant not found")
        
        query = query.filter(models.UserKnowledge.assistant_id == assistant_id)
    
    # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
    total = query.count()
    
    # –ü–∞–≥–∏–Ω–∞—Ü–∏—è
    offset = (page - 1) * limit
    entries = query.order_by(models.UserKnowledge.id.desc()).offset(offset).limit(limit).all()
    
    items = []
    for entry in entries:
        try:
            content = json.loads(entry.content) if entry.type == 'summary' else entry.content
        except Exception:
            content = entry.content
        items.append({
            'id': entry.id,  # –î–æ–±–∞–≤–ª—è–µ–º ID –∏–∑ —Ç–∞–±–ª–∏—Ü—ã UserKnowledge
            'doc_id': entry.doc_id,
            'type': entry.type,
            'content': content,
            'doc_type': entry.doc_type,
            'importance': entry.importance,
            'last_used': entry.last_used.isoformat() if entry.last_used else None,
            'usage_count': entry.usage_count,
            'assistant_id': entry.assistant_id  # –î–æ–±–∞–≤–ª—è–µ–º assistant_id
        })
    
    return {
        "items": items,
        "total": total,
        "page": page,
        "pages": (total + limit - 1) // limit,
        "limit": limit
    }

@router.put("/knowledge/{knowledge_id}")
def update_knowledge(knowledge_id: int, data: KnowledgeIn, current_user: models.User = Depends(auth.get_current_user), db: Session = Depends(get_db), background_tasks: BackgroundTasks = None):
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–Ω–∞–Ω–∏–µ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    knowledge = db.query(models.UserKnowledge).filter(
        models.UserKnowledge.id == knowledge_id,
        models.UserKnowledge.user_id == current_user.id
    ).first()
    
    if not knowledge:
        raise HTTPException(status_code=404, detail="Knowledge not found")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∑–Ω–∞–Ω–∏–µ
    knowledge.content = data.content
    knowledge.type = data.type
    knowledge.doc_type = data.doc_type or knowledge.doc_type
    knowledge.importance = data.importance or knowledge.importance
    knowledge.assistant_id = data.assistant_id
    
    db.commit()
    
    # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à –∑–Ω–∞–Ω–∏–π
    invalidate_knowledge_cache(current_user.id, knowledge.assistant_id)

    # –§–æ–Ω–æ–≤–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ –∑–Ω–∞–Ω–∏—è
    try:
        if background_tasks is not None:
            background_tasks.add_task(
                _background_index_document,
                knowledge.doc_id,
                current_user.id,
                knowledge.assistant_id,
                knowledge.content,
                knowledge.doc_type or 'confirmed_knowledge',
                knowledge.importance or 10,
            )
        else:
            from services.embeddings_service import embeddings_service
            # –ü–µ—Ä–µ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π —É–¥–∞–ª–∏–º —Å—Ç–∞—Ä—ã–µ confirmed_knowledge embeddings –ø–æ —ç—Ç–æ–º—É doc_id
            try:
                db.query(models.KnowledgeEmbedding).filter(
                    models.KnowledgeEmbedding.doc_id == knowledge.doc_id,
                    models.KnowledgeEmbedding.source == 'confirmed_knowledge'
                ).delete(synchronize_session=False)
                db.commit()
            except Exception:
                db.rollback()
            embeddings_service.index_document(
                doc_id=knowledge.doc_id,
                user_id=current_user.id,
                assistant_id=knowledge.assistant_id,
                text=knowledge.content,
                doc_type=knowledge.doc_type or 'confirmed_knowledge',
                importance=knowledge.importance or 10,
                db=db,
            )
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.warning(f"[UPDATE_KNOWLEDGE] indexing failed: {e}")

    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Ä—Å–∏—é –∑–Ω–∞–Ω–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞(–æ–≤)
    try:
        from services.embeddings_service import embeddings_service
        if knowledge.assistant_id:
            embeddings_service.increment_knowledge_version(knowledge.assistant_id, db)
        else:
            assistants = db.query(models.Assistant).filter(models.Assistant.user_id == current_user.id).all()
            for assistant in assistants:
                embeddings_service.increment_knowledge_version(assistant.id, db)
    except Exception as _e:
        logger = logging.getLogger(__name__)
        logger.warning(f"[UPDATE_KNOWLEDGE] Failed to bump knowledge version: {_e}")
    
    # üî• –ì–û–†–Ø–ß–ê–Ø –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –∏–ª–∏ –≤—Å–µ—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤
    if knowledge.assistant_id:
        hot_reload_assistant_bots(knowledge.assistant_id, db)
    else:
        # –ï—Å–ª–∏ –∑–Ω–∞–Ω–∏–µ –æ–±—â–µ–µ - –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        assistants = db.query(models.Assistant).filter(models.Assistant.user_id == current_user.id).all()
        for assistant in assistants:
            hot_reload_assistant_bots(assistant.id, db)
    
    return {"ok": True}

@router.delete("/knowledge/{knowledge_id}")
def delete_knowledge(knowledge_id: int, current_user: models.User = Depends(auth.get_current_user), db: Session = Depends(get_db)):
    """
    –£–ª—É—á—à–µ–Ω–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –∑–Ω–∞–Ω–∏–π —Å –ø–æ–ª–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π –≤—Å–µ—Ö —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    from utils.bot_cleanup import enhanced_knowledge_deletion
    
    try:
        success = enhanced_knowledge_deletion(knowledge_id, current_user.id, db)
        
        if success:
            logger.info(f"[DELETE_KNOWLEDGE] ‚úÖ Knowledge {knowledge_id} completely deleted with full cleanup")
            # –ë–∞–º–ø –≤–µ—Ä—Å–∏–∏ –∑–Ω–∞–Ω–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞(–æ–≤) –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è
            try:
                from services.embeddings_service import embeddings_service
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ —ç—Ç–æ–π –∑–∞–ø–∏—Å–∏ (–æ–Ω–∞ —É–∂–µ —É–¥–∞–ª–µ–Ω–∞, –ø–æ—ç—Ç–æ–º—É –±–µ—Ä—ë–º –∏–∑ –ª–æ–≥–∏–∫–∏ enhanced_knowledge_deletion –Ω–µ –ø–æ–ª—É—á–∏—Ç—Å—è)
                # –í –∫–∞—á–µ—Å—Ç–≤–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞ ‚Äî –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç –¥–ª—è –≤—Å–µ—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                assistants = db.query(models.Assistant).filter(models.Assistant.user_id == current_user.id).all()
                for assistant in assistants:
                    embeddings_service.increment_knowledge_version(assistant.id, db)
            except Exception as _e:
                logger.warning(f"[DELETE_KNOWLEDGE] Failed to bump knowledge version: {_e}")

            # –û—á–∏—Å—Ç–∫–∞ embeddings –∏—Å—Ç–æ—á–Ω–∏–∫–∞ confirmed_knowledge –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –≥–¥–µ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π
            try:
                # –ù–∞—Ö–æ–¥–∏–º doc_id –±–µ–∑ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π
                subq = db.query(models.UserKnowledge.doc_id).filter(
                    models.UserKnowledge.user_id == current_user.id
                ).subquery()
                deleted = db.query(models.KnowledgeEmbedding).filter(
                    ~models.KnowledgeEmbedding.doc_id.in_(subq),
                    models.KnowledgeEmbedding.source == 'confirmed_knowledge'
                ).delete(synchronize_session=False)
                db.commit()
                logger.info(f"[DELETE_KNOWLEDGE] cleaned {deleted} orphan confirmed_knowledge embeddings")
            except Exception as _e:
                db.rollback()
                logger.warning(f"[DELETE_KNOWLEDGE] embeddings cleanup failed: {_e}")
            return {"ok": True}
        else:
            raise HTTPException(status_code=404, detail="Knowledge not found or deletion failed")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[DELETE_KNOWLEDGE] Error: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –∑–Ω–∞–Ω–∏–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")

@router.get("/knowledge/stats")
def get_knowledge_stats(current_user: models.User = Depends(auth.get_current_user), db: Session = Depends(get_db)):
    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    knowledge_count = db.query(models.UserKnowledge).filter(
        models.UserKnowledge.user_id == current_user.id
    ).count()
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    most_used = db.query(
        models.UserKnowledge,
        models.Document.filename
    ).join(
        models.Document, 
        models.UserKnowledge.doc_id == models.Document.id
    ).filter(
        models.UserKnowledge.user_id == current_user.id
    ).order_by(
        models.UserKnowledge.usage_count.desc()
    ).limit(5).all()
    
    top_docs = []
    for k, filename in most_used:
        top_docs.append({
            "id": k.id,
            "doc_id": k.doc_id,
            "filename": filename,
            "doc_type": k.doc_type,
            "usage_count": k.usage_count,
            "last_used": k.last_used.isoformat() if k.last_used else None,
            "importance": k.importance
        })
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –∏—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    doc_types = db.query(
        models.UserKnowledge.doc_type,
        func.count(models.UserKnowledge.id).label('count')
    ).filter(
        models.UserKnowledge.user_id == current_user.id
    ).group_by(
        models.UserKnowledge.doc_type
    ).all()
    
    types_count = {}
    for doc_type, count in doc_types:
        types_count[doc_type or '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π'] = count
    
    return {
        "total_documents": knowledge_count,
        "most_used": top_docs,
        "document_types": types_count,
        "total_content_length": sum(len(k.content or '') for k, _ in most_used),
        "avg_importance": sum(k.importance or 0 for k, _ in most_used) / len(most_used) if most_used else 0
    }

@router.post("/knowledge/track-usage")
def track_document_usage(
    used_docs: List[str] = Body(..., description="–°–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"),
    current_user: models.User = Depends(auth.get_current_user), 
    db: Session = Depends(get_db)
):
    """–û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –æ—Ç–≤–µ—Ç–∞—Ö AI"""
    if not used_docs:
        return {"message": "–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è"}
    
    updated_count = 0
    
    for doc_content in used_docs:
        # –ò—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É (—á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)
        knowledge_entries = db.query(models.UserKnowledge).filter(
            models.UserKnowledge.user_id == current_user.id
        ).all()
        
        for entry in knowledge_entries:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
            # –ò–ª–∏ –Ω–∞–æ–±–æ—Ä–æ—Ç - —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            if (entry.content in doc_content or doc_content in entry.content):
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                entry.usage_count = (entry.usage_count or 0) + 1
                entry.last_used = datetime.utcnow()
                updated_count += 1
                break
    
    db.commit()
    
    return {
        "message": f"–û–±–Ω–æ–≤–ª–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è {updated_count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
        "updated_count": updated_count
    }