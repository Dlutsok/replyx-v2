from fastapi import APIRouter, Depends, HTTPException, Query, Body, UploadFile, File
from sqlalchemy.orm import Session, joinedload
from sqlalchemy import func
from typing import List, Optional
from datetime import datetime
import json
import os
import logging
from pydantic import BaseModel

from database import models, schemas, crud, auth
from database.connection import get_db
from validators.rate_limiter import rate_limit_api

logger = logging.getLogger(__name__)

router = APIRouter()

# get_db –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –∏–∑ database.connection

# Helper functions to avoid circular imports
def invalidate_knowledge_cache(user_id: int, assistant_id: int):
    """Wrapper –¥–ª—è –∏–Ω–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫—ç—à–∞ –∑–Ω–∞–Ω–∏–π"""
    try:
        import chatai_cache
        chatai_cache.invalidate_knowledge_cache(user_id, assistant_id)
    except ImportError:
        pass

def hot_reload_assistant_bots(assistant_id: int, db: Session):
    """–ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ"""
    try:
        from main import hot_reload_assistant_bots as _hot_reload
        return _hot_reload(assistant_id, db)
    except ImportError:
        pass

def reload_assistant_bots(assistant_id: int, db: Session):
    """–ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ"""
    try:
        from main import reload_assistant_bots as _reload
        return _reload(assistant_id, db)
    except ImportError:
        pass

def extract_document_text(doc_id: int, current_user: models.User, file_path: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –µ–≥–æ —Ç–∏–ø–∞"""
    file_extension = os.path.splitext(file_path)[1].lower()
    text = ""
    
    try:
        if file_extension == '.pdf':
            import PyPDF2
            with open(file_path, 'rb') as pdf_file:
                pdf_reader = PyPDF2.PdfReader(pdf_file)
                for page_num in range(len(pdf_reader.pages)):
                    text += pdf_reader.pages[page_num].extract_text() + "\n"
        
        elif file_extension == '.docx':
            from docx import Document as DocxDoc
            docx = DocxDoc(file_path)
            text = '\n'.join([p.text for p in docx.paragraphs])
        
        elif file_extension == '.doc':
            # –î–ª—è .doc —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ—Å—Ç–æ–µ —á—Ç–µ–Ω–∏–µ –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                text = f.read()
        
        else:
            # –î–ª—è .txt –∏ –¥—Ä—É–≥–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                text = f.read()
        
        return text.strip()
        
    except Exception as e:
        logger.error(f"Error extracting text from {file_path}: {e}")
        return ""

def determine_document_type(filename: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –¥–ª—è –ª—É—á—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    filename_lower = filename.lower()
    
    if any(word in filename_lower for word in ['–∏–Ω—Å—Ç—Ä—É–∫—Ü', 'instruction', 'manual', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤']):
        return 'instruction'
    elif any(word in filename_lower for word in ['—Ä–µ–≥–ª–∞–º–µ–Ω—Ç', 'policy', 'procedure']):
        return 'regulation'
    elif any(word in filename_lower for word in ['faq', '–≤–æ–ø—Ä–æ—Å', '–æ—Ç–≤–µ—Ç', 'question']):
        return 'faq'
    elif any(word in filename_lower for word in ['–ø—Ä–∞–π—Å', 'price', '—Ü–µ–Ω', '—Ç–∞—Ä–∏—Ñ', 'cost']):
        return 'pricing'
    elif any(word in filename_lower for word in ['–∫–æ–Ω—Ç–∞–∫—Ç', 'contact', '—Ç–µ–ª–µ—Ñ–æ–Ω', 'email']):
        return 'contacts'
    elif any(word in filename_lower for word in ['–∫–æ–º–ø–∞–Ω', 'company', 'about', '–æ_–Ω–∞—Å']):
        return 'company_info'
    else:
        return 'document'

# --- User Knowledge Endpoint ---

@router.get("/user-knowledge/{user_id}")
def get_user_knowledge(user_id: int, assistant_id: int = Query(None), db: Session = Depends(get_db)):
    """
    üéØ –ò–°–ü–†–ê–í–õ–ï–ù–ê –ö–û–†–ù–ï–í–ê–Ø –ü–†–û–ë–õ–ï–ú–ê –ö–≠–®–ò–†–û–í–ê–ù–ò–Ø!
    –¢–µ–ø–µ—Ä—å —Ñ—É–Ω–∫—Ü–∏—è —É—á–∏—Ç—ã–≤–∞–µ—Ç assistant_id –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –∑–Ω–∞–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ
    """
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω—É–∂–Ω–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    target_assistant = None
    if assistant_id:
        target_assistant = db.query(models.Assistant).filter(
            models.Assistant.id == assistant_id,
            models.Assistant.user_id == user_id
        ).first()
    else:
        # –ï—Å–ª–∏ assistant_id –Ω–µ —É–∫–∞–∑–∞–Ω, –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–≥–æ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ
        target_assistant = db.query(models.Assistant).filter(
            models.Assistant.user_id == user_id,
            models.Assistant.is_active == True
        ).first()
    
    # üî• –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–æ–ª—É—á–∞–µ–º –¢–û–õ–¨–ö–û –∑–Ω–∞–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    if target_assistant:
        print(f"[GET_USER_KNOWLEDGE] –ó–∞–≥—Ä—É–∂–∞–µ–º –∑–Ω–∞–Ω–∏—è –¢–û–õ–¨–ö–û –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ {target_assistant.id} ({target_assistant.name}) –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
        # –ü–æ–ª—É—á–∞–µ–º –¢–û–õ–¨–ö–û –∑–Ω–∞–Ω–∏—è –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–µ –∫ —ç—Ç–æ–º—É –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É
        # –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è - –∫–∞–∂–¥—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º
        knowledge_with_docs = db.query(
            models.UserKnowledge,
            models.Document
        ).join(
            models.Document, 
            models.UserKnowledge.doc_id == models.Document.id
        ).filter(
            models.UserKnowledge.user_id == user_id,
            models.UserKnowledge.assistant_id == target_assistant.id  # –¢–û–õ–¨–ö–û —ç—Ç–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        ).all()
    else:
        print(f"[GET_USER_KNOWLEDGE] –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞–Ω–∏—è")
        # –ï—Å–ª–∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
        knowledge_with_docs = []
    
    print(f"[GET_USER_KNOWLEDGE] –ù–∞–π–¥–µ–Ω–æ {len(knowledge_with_docs)} –∑–∞–ø–∏—Å–µ–π –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞")
    
    documents = []
    for entry, doc in knowledge_with_docs:
        doc_type = entry.doc_type or "–î–æ–∫—É–º–µ–Ω—Ç"
        
        # –î–ª—è Quick Fix –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        if doc.filename.startswith('quick_fix_'):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å—Ç—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –±–µ–∑ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∞–π–ª
            documents.append(entry.content)
        else:
            # –î–ª—è –æ–±—ã—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–µ—Ñ–∏–∫—Å
            prefix = f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Ç–∏–ø–∞ '{doc_type}':\n"
            documents.append(prefix + entry.content)
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –Ω—É–∂–Ω–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
    system_prompt = "–¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç."
    if target_assistant:
        system_prompt = target_assistant.system_prompt or system_prompt
        print(f"[GET_USER_KNOWLEDGE] –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –æ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ {target_assistant.id}: '{system_prompt[:100]}...'")
    
    return {
        "system_prompt": system_prompt,
        "documents": documents
    }

# --- Documents CRUD ---

@router.get("/documents")
def get_documents(
    page: int = Query(1, ge=1, description="–ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã"),
    limit: int = Query(50, ge=1, le=100, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"),
    current_user: models.User = Depends(auth.get_current_user), 
    db: Session = Depends(get_db)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
    total = db.query(models.Document).filter(models.Document.user_id == current_user.id).count()
    
    # –ü–∞–≥–∏–Ω–∞—Ü–∏—è —Å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π
    offset = (page - 1) * limit
    documents = db.query(models.Document).filter(
        models.Document.user_id == current_user.id
    ).order_by(models.Document.upload_date.desc()).offset(offset).limit(limit).all()
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    result = {
        "items": documents,
        "total": total,
        "page": page,
        "pages": (total + limit - 1) // limit,
        "limit": limit
    }
    
    return result

@router.post("/documents", response_model=schemas.DocumentRead)
@rate_limit_api(limit=10, window=300)  # 10 —Ñ–∞–π–ª–æ–≤ –∑–∞ 5 –º–∏–Ω—É—Ç
async def upload_document(file: UploadFile = File(...), current_user: models.User = Depends(auth.get_current_user), db: Session = Depends(get_db)):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
    from validators.file_validator import file_validator
    from services.balance_service import BalanceService
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞
    balance_service = BalanceService(db)
    if not balance_service.check_sufficient_balance(current_user.id, "document_upload"):
        raise HTTPException(
            status_code=402, 
            detail="–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ä–µ–¥—Å—Ç–≤ –Ω–∞ –±–∞–ª–∞–Ω—Å–µ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"
        )
    
    # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
    content = await file.read()
    await file.seek(0)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —É–∫–∞–∑–∞—Ç–µ–ª—å –≤ –Ω–∞—á–∞–ª–æ
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞ —Å –Ω–æ–≤—ã–º –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–æ–º
    try:
        mime_type, safe_filename = await file_validator.validate_file_content(file, content)
    except HTTPException as e:
        logger.warning(f"File upload rejected for user {current_user.id}: {e.detail}")
        raise e
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
    secure_filename = file_validator.generate_secure_filename(
        user_id=current_user.id,
        original_filename=safe_filename,
        content=content
    )
    
    # –ü–æ–ª—É—á–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    file_path = file_validator.get_safe_upload_path(current_user.id, secure_filename)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
    try:
        with open(file_path, "wb") as f:
            f.write(content)
        
        logger.info(f"File uploaded successfully: {file_path} by user {current_user.id}")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –≤ –ë–î —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º –∏–º–µ–Ω–µ–º —Ñ–∞–π–ª–∞
        doc = crud.create_document(db, current_user.id, secure_filename, len(content))
        
        # –°–ø–∏—Å—ã–≤–∞–µ–º —Å—Ä–µ–¥—Å—Ç–≤–∞ –∑–∞ –∑–∞–≥—Ä—É–∑–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
        try:
            balance_service.charge_for_service(current_user.id, "document_upload")
            logger.info(f"Charged user {current_user.id} for document upload")
        except Exception as e:
            logger.error(f"Failed to charge user {current_user.id} for document upload: {e}")
            # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å, –µ—Å–ª–∏ —Å–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å
        
        # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        from cache.redis_cache import cache
        cache.delete_pattern(f"user_documents:{current_user.id}:*")
        
        # üöÄ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –ò–ù–î–ï–ö–°–ê–¶–ò–Ø –ß–ï–†–ï–ó EMBEDDINGS (–ë–ï–ó HOT-RELOAD)
        # –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥: —Å–æ–∑–¥–∞–µ–º embeddings –¥–ª—è lazy-retrieval –±–µ–∑ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ –±–æ—Ç–æ–≤
        try:
            logger.info(f"Starting automatic embedding indexing for doc_id={doc.id}, user_id={current_user.id}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            text = extract_document_text(doc.id, current_user, file_path)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ª—É—á—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            doc_type = determine_document_type(secure_filename)
            
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å embeddings
            from services.embeddings_service import embeddings_service
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            assistants = db.query(models.Assistant).filter(models.Assistant.user_id == current_user.id).all()
            
            if assistants:
                # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ (–∏–ª–∏ –º–æ–∂–Ω–æ –±–µ–∑ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É)
                for assistant in assistants:
                    indexed_chunks = embeddings_service.index_document(
                        doc_id=doc.id,
                        user_id=current_user.id,
                        assistant_id=assistant.id,
                        text=text,
                        doc_type=doc_type,
                        importance=10,
                        db=db
                    )
                    
                    logger.info(f"Indexed {indexed_chunks} chunks for assistant {assistant.id}")
                
                logger.info(f"‚úÖ Document indexed with embeddings for {len(assistants)} assistants - NO BOT RELOAD NEEDED!")
                
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤, –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –±–µ–∑ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É
                indexed_chunks = embeddings_service.index_document(
                    doc_id=doc.id,
                    user_id=current_user.id,
                    assistant_id=None,  # –û–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤
                    text=text,
                    doc_type=doc_type,
                    importance=10,
                    db=db
                )
                
                logger.info(f"‚úÖ Document indexed with {indexed_chunks} chunks as general knowledge")
                
        except Exception as e:
            logger.error(f"Failed to index document embeddings {doc.id}: {e}")
            # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≥—Ä—É–∑–∫–∏, –µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å
            # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–º–æ–∂–µ—Ç –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∑–∂–µ
        
        return doc
        
    except Exception as e:
        logger.error(f"Error saving file {file_path}: {e}")
        # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª –µ—Å–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –≤ –ë–î –Ω–µ —É–¥–∞–ª–æ—Å—å
        if os.path.exists(file_path):
            os.remove(file_path)
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞")

@router.delete("/documents/{doc_id}")
def delete_document(doc_id: int, current_user: models.User = Depends(auth.get_current_user), db: Session = Depends(get_db)):
    """
    –£–ª—É—á—à–µ–Ω–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø–æ–ª–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π –≤—Å–µ—Ö —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    from utils.bot_cleanup import enhanced_document_deletion
    
    try:
        success = enhanced_document_deletion(doc_id, current_user.id, db)
        
        if success:
            logger.info(f"[DELETE_DOCUMENT] ‚úÖ Document {doc_id} completely deleted with full cleanup")
            return {"ok": True}
        else:
            raise HTTPException(status_code=404, detail="Document not found or deletion failed")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[DELETE_DOCUMENT] Error: {e}")
        raise HTTPException(status_code=500, detail="Internal server error during document deletion")

@router.get("/documents/{doc_id}/text")
def get_document_text(doc_id: int, current_user: models.User = Depends(auth.get_current_user), db: Session = Depends(get_db)):
    doc = db.query(models.Document).filter(models.Document.id == doc_id, models.Document.user_id == current_user.id).first()
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º user_id –¥–ª—è –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É, –∫–∞–∫ –≤ upload_document
    file_path = os.path.join("uploads", str(current_user.id), doc.filename)
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="File not found")
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞
    text = ""
    file_extension = os.path.splitext(file_path)[1].lower()
    
    if file_extension == '.pdf':
        try:
            import PyPDF2
            with open(file_path, 'rb') as pdf_file:
                pdf_reader = PyPDF2.PdfReader(pdf_file)
                for page_num in range(len(pdf_reader.pages)):
                    text += pdf_reader.pages[page_num].extract_text() + "\n"
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}")
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}")
    elif file_extension == '.docx':
        try:
            from docx import Document as DocxDoc
            docx = DocxDoc(file_path)
            text = '\n'.join([p.text for p in docx.paragraphs])
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è .docx: {e}")
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è .docx: {e}")
    else:
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                text = f.read()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
    
    return {"text": text}

def analyze_document_internal(doc_id: int, current_user: models.User, db: Session):
    """–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    print(f"[analyze_document_internal] Analyzing doc_id={doc_id} for user {current_user.id}")
    doc = db.query(models.Document).filter(models.Document.id == doc_id, models.Document.user_id == current_user.id).first()
    if not doc:
        print(f"[analyze_document_internal] Document not found in DB for user_id={current_user.id}, doc_id={doc_id}")
        raise Exception("Document not found")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º user_id –¥–ª—è –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É, –∫–∞–∫ –≤ upload_document
    file_path = os.path.join("uploads", str(current_user.id), doc.filename)
    if not os.path.exists(file_path):
        print(f"[analyze_document_internal] File not found: {file_path}")
        raise Exception("File not found")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –ø—É–ª —Ç–æ–∫–µ–Ω–æ–≤ –≤–º–µ—Å—Ç–æ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
    from ai.ai_token_manager import ai_token_manager
    
    # --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ ---
    text = ""
    file_extension = os.path.splitext(file_path)[1].lower()
    
    if file_extension == '.pdf':
        try:
            import PyPDF2
            with open(file_path, 'rb') as pdf_file:
                pdf_reader = PyPDF2.PdfReader(pdf_file)
                for page_num in range(len(pdf_reader.pages)):
                    text += pdf_reader.pages[page_num].extract_text() + "\n"
        except Exception as e:
            print(f"[analyze_document_internal] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}")
            raise Exception(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}")
    elif file_extension == '.docx':
        try:
            from docx import Document as DocxDoc
            docx = DocxDoc(file_path)
            text = '\n'.join([p.text for p in docx.paragraphs])
        except Exception as e:
            print(f"[analyze_document_internal] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è .docx: {e}")
            raise Exception(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è .docx: {e}")
    elif file_extension == '.doc':
        try:
            # –î–ª—è .doc —Ñ–∞–π–ª–æ–≤ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å textract –∏–ª–∏ –¥—Ä—É–≥–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
            # –ó–¥–µ—Å—å –ø—Ä–æ—Å—Ç–æ–µ —á—Ç–µ–Ω–∏–µ –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                text = f.read()
        except Exception as e:
            print(f"[analyze_document_internal] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è .doc: {e}")
            raise Exception(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è .doc: {e}")
    else:
        # –î–ª—è .txt –∏ –¥—Ä—É–≥–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                text = f.read()
        except Exception as e:
            print(f"[analyze_document_internal] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
            raise Exception(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è gpt-4o (16384 —Ç–æ–∫–µ–Ω–∞)
    # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ 1 —Ç–æ–∫–µ–Ω = 4 —Å–∏–º–≤–æ–ª–∞, –æ—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ –∏ –æ—Ç–≤–µ—Ç–∞
    total_length = len(text)
    if total_length < 40000:
        chunk_size = total_length  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ü–µ–ª–∏–∫–æ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–æ 40KB –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
    elif total_length < 120000:
        chunk_size = 30000  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –¥–µ—Ç–∞–ª–µ–π
    else:
        chunk_size = 20000   # –î–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ç–æ–∂–µ –±–æ–ª—å—à–∏–µ —á–∞–Ω–∫–∏
    
    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
    chunks = []
    current_pos = 0
    while current_pos < len(text):
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–ª–∏ –∞–±–∑–∞—Ü–∞ –¥–ª—è –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        end_pos = min(current_pos + chunk_size, len(text))
        if end_pos < len(text):
            # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–ª–∏ –∞–±–∑–∞—Ü–∞
            for separator in ['\n\n', '\n', '. ', '! ', '? ']:
                last_separator = text.rfind(separator, current_pos, end_pos)
                if last_separator > current_pos + chunk_size // 2:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –Ω–µ –≤ –Ω–∞—á–∞–ª–µ —á–∞–Ω–∫–∞
                    end_pos = last_separator + len(separator)
                    break
        
        chunks.append(text[current_pos:end_pos])
        current_pos = end_pos
    
    summaries = []
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    doc_type_prompt = f"–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è, —Ä–µ–≥–ª–∞–º–µ–Ω—Ç, —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫, —Å—Ç–∞—Ç—å—è –∏ —Ç.–¥.) –∏ –µ–≥–æ –æ—Å–Ω–æ–≤–Ω—É—é —Ç–µ–º—É. –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å: {text[:500]}..."
    try:
        doc_type_response = ai_token_manager.make_openai_request(
            messages=[
                {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –∏ —Ç–µ–º—É –¥–æ–∫—É–º–µ–Ω—Ç–∞."},
                {"role": "user", "content": doc_type_prompt}
            ],
            model="gpt-4o",
            user_id=current_user.id,
            assistant_id=None
        )
        doc_type = doc_type_response.choices[0].message.content.strip()
    except Exception as e:
        print(f"[analyze_document_internal] –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
        doc_type = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞"
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    for idx, chunk in enumerate(chunks):
        prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Ç–∏–ø–∞ "{doc_type}" –∏ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –≤—ã–¥–µ–ª–∏ –í–°–ï –≤–∞–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏:

1. –ö–õ–Æ–ß–ï–í–´–ï –§–ê–ö–¢–´ (—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –í–°–ï–• –¥–µ—Ç–∞–ª–µ–π):
   - –ù–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π, –ø—Ä–æ–¥—É–∫—Ç–æ–≤
   - –î–∞—Ç—ã, —Ü–∏—Ñ—Ä—ã, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
   - –ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (email, —Ç–µ–ª–µ—Ñ–æ–Ω—ã, –∞–¥—Ä–µ—Å–∞)
   - –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã, —á–∞—Å—ã, —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è

2. –£–°–õ–£–ì–ò –ò –ü–†–û–î–£–ö–¢–´ (—Å –ü–û–õ–ù–´–ú–ò –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏):
   - –¢–æ—á–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫—É—Ä—Å–æ–≤, –ø—Ä–æ–≥—Ä–∞–º–º, —É—Å–ª—É–≥
   - –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
   - –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

3. FAQ –ò –í–û–ü–†–û–°–´ (—Å–æ—Ö—Ä–∞–Ω–∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ):
   - –í–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã –ø–æ–ª–Ω–æ—Å—Ç—å—é
   - –ü—Ä–∞–≤–∏–ª–∞ –∏ —É—Å–ª–æ–≤–∏—è
   - –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º

4. –§–£–ù–ö–¶–ò–ò –ò –ó–ê–î–ê–ß–ò (–µ—Å–ª–∏ –µ—Å—Ç—å):
   - –ß—Ç–æ —É–º–µ–µ—Ç –¥–µ–ª–∞—Ç—å —Å–∏—Å—Ç–µ–º–∞/–±–æ—Ç
   - –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

6. üí∞ –¶–ï–ù–´ –ò –¢–ê–†–ò–§–´ (–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –∏–∑–≤–ª–µ–∫–∏ –í–°–ï —Ü–µ–Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ):
   - –°—Ç–æ–∏–º–æ—Å—Ç—å —É—Å–ª—É–≥ (–≤–∫–ª—é—á–∞—è —Ü–µ–Ω—ã —Å –ø–æ–º–µ—Ç–∫–∞–º–∏ "???", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ", "~", "–ø—Ä–∏–º–µ—Ä–Ω–æ")
   - –ü–ª–∞–Ω—ã –∏ –ø–∞–∫–µ—Ç—ã
   - –°–∫–∏–¥–∫–∏ –∏ –∞–∫—Ü–∏–∏
   - –û–°–û–ë–û –í–ê–ñ–ù–û: –µ—Å–ª–∏ —Ü–µ–Ω–∞ —É–∫–∞–∑–∞–Ω–∞ –∫–∞–∫ "???", "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ", "~", "–ø—Ä–∏–º–µ—Ä–Ω–æ" - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —Å–æ—Ö—Ä–∞–Ω–∏ —ç—Ç—É –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å

7. üö® –ü–†–û–ë–õ–ï–ú–´ –ò –ñ–ê–õ–û–ë–´ –ö–õ–ò–ï–ù–¢–û–í (–ù–û–í–´–ô –†–ê–ó–î–ï–õ - –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–π):
   - –ñ–∞–ª–æ–±—ã –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ —Å–µ—Ä–≤–∏—Å–∞ ("–Ω–µ –ø–µ—Ä–µ–∑–≤–æ–Ω–∏–ª–∏", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–ø—Ä–æ–±–ª–µ–º—ã")
   - –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–∑—ã–≤—ã –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
   - –£–ø–æ–º–∏–Ω–∞–Ω–∏—è –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å –º–µ–Ω–µ–¥–∂–µ—Ä–∞–º–∏, –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
   - –õ—é–±—ã–µ —Ñ—Ä–∞–∑—ã —Ç–∏–ø–∞: "–æ–±–µ—â–∞–ª–∏, –Ω–æ...", "—Å–∫–∞–∑–∞–ª–∏, —á—Ç–æ –±—É–¥–µ—Ç, –Ω–æ...", "–¥–æ —Å–∏—Ö –ø–æ—Ä –Ω–µ—Ç"
   - –°–∞—Ä–∫–∞–∑–º –∏ –∏—Ä–æ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ

8. üìù –û–°–û–ë–ï–ù–ù–û–°–¢–ò –î–û–ö–£–ú–ï–ù–¢–ê:
   - –°—Ç–∏–ª—å –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π, —é–º–æ—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π, —Å–∞—Ç–∏—Ä–∞)
   - –ü–æ–ª–Ω–æ—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (–µ—Å—Ç—å –ª–∏ –ø—Ä–æ–ø—É—Å–∫–∏, –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–∏)
   - –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
   - –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

9. üîç –ù–ï–û–ü–†–ï–î–ï–õ–ï–ù–ù–û–°–¢–ò –ò –°–ü–ï–¶–°–ò–ú–í–û–õ–´ (–ù–û–í–´–ô –†–ê–ó–î–ï–õ):
   - –í—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è "???", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ", "–•–ó", "~", "–ø—Ä–∏–º–µ—Ä–Ω–æ"
   - –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (—Ä–∞–∑–Ω—ã–µ –¥–∞—Ç—ã, —Ü–∏—Ñ—Ä—ã)
   - –ù–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–µ—Ç–∫–∞–º–∏
   - –≠–º–æ–¥–∑–∏ —Å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º (ü§∑‚Äç‚ôÇÔ∏è, ü§î)

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
- –ù–ï –æ–±–æ–±—â–∞–π –∏ –ù–ï –ø—Ä–æ–ø—É—Å–∫–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ—Ç–∞–ª–∏!
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï —É–ø–æ–º—è–Ω—É—Ç—ã–µ email –∞–¥—Ä–µ—Å–∞ —Ç–æ—á–Ω–æ –∫–∞–∫ –Ω–∞–ø–∏—Å–∞–Ω–æ, –æ—Ç–º–µ—Ç—å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–µ –Ω–æ–º–µ—Ä–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é, –≤–∫–ª—é—á–∞—è –Ω–µ–ø–æ–ª–Ω—ã–µ
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï –Ω–∞–∑–≤–∞–Ω–∏—è –∫—É—Ä—Å–æ–≤ –∏ —É—Å–ª—É–≥ –¥–æ—Å–ª–æ–≤–Ω–æ
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï FAQ –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã —Ü–µ–ª–∏–∫–æ–º
- –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –∏–∑–≤–ª–µ–∫–∞–π –∂–∞–ª–æ–±—ã –∏ –ø—Ä–æ–±–ª–µ–º—ã –∫–ª–∏–µ–Ω—Ç–æ–≤
- –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —Å–æ—Ö—Ä–∞–Ω—è–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ü–µ–Ω–∞—Ö –∏ –¥–∞–Ω–Ω—ã—Ö
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–ø–æ–ª–Ω–∞—è, —é–º–æ—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–æ–±–ª–µ–º—ã - —á–µ—Ç–∫–æ —É–∫–∞–∂–∏ —ç—Ç–æ

–¢–µ–∫—Å—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞:
{chunk}

–°–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤—ã–∂–∏–º–∫—É –¥–ª—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –í–°–ï –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –ø—Ä–æ–±–ª–µ–º—ã –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏.
"""
        try:
            response = ai_token_manager.make_openai_request(
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –∑–Ω–∞–Ω–∏–π. –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: —Å–æ—Ö—Ä–∞–Ω—è–π –í–°–ï –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ - email –∞–¥—Ä–µ—Å–∞, —Ç–µ–ª–µ—Ñ–æ–Ω—ã, –Ω–∞–∑–≤–∞–Ω–∏—è, —Ü–µ–Ω—ã, –¥–∞—Ç—ã. –ù–ï –æ–±–æ–±—â–∞–π –∏ –ù–ï –ø—Ä–æ–ø—É—Å–∫–∞–π –¥–µ—Ç–∞–ª–∏! –û–°–û–ë–û –í–ê–ñ–ù–û: –∏–∑–≤–ª–µ–∫–∞–π –∂–∞–ª–æ–±—ã –∫–ª–∏–µ–Ω—Ç–æ–≤, –ø—Ä–æ–±–ª–µ–º—ã —Å–µ—Ä–≤–∏—Å–∞, –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ü–µ–Ω–∞—Ö (???, ~, –ø—Ä–∏–º–µ—Ä–Ω–æ). –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Å–æ–∑–¥–∞—Ç—å –ü–û–õ–ù–£–Æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤—ã–∂–∏–º–∫—É –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –≤–∫–ª—é—á–∞—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –ø—Ä–æ–±–ª–µ–º—ã, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º –¥–ª—è —á–µ—Å—Ç–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º."},
                    {"role": "user", "content": prompt}
                ],
                model="gpt-4o",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å —Å –±–æ–ª—å—à–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                user_id=current_user.id,
                assistant_id=None,
                temperature=0.05  # –ï—â–µ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            )
            summary = response.choices[0].message.content.strip()
        except Exception as e:
            print(f"[analyze_document_internal] –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —á–∞–Ω–∫–∞ {idx+1}: {e}")
            summary = f"[–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}]"
        
        summaries.append({"chunk": idx+1, "summary": summary})
    
    # –°–æ–∑–¥–∞–µ–º –æ–±—â–µ–µ —Ä–µ–∑—é–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    if len(chunks) > 1:
        try:
            all_summaries = "\n\n".join([s["summary"] for s in summaries])
            final_prompt = f"""
–ù–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –≤—ã–∂–∏–º–æ–∫ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Ç–∏–ø–∞ "{doc_type}", —Å–æ–∑–¥–∞–π –µ–¥–∏–Ω–æ–µ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞:

{all_summaries}

–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ, –∫–æ—Ç–æ—Ä–æ–µ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
"""
            final_response = ai_token_manager.make_openai_request(
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –°–æ–∑–¥–∞–π –µ–¥–∏–Ω–æ–µ —Ä–µ–∑—é–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã–∂–∏–º–æ–∫ –∏–∑ –µ–≥–æ —á–∞—Å—Ç–µ–π."},
                    {"role": "user", "content": final_prompt}
                ],
                model="gpt-4o",
                user_id=current_user.id,
                assistant_id=None
            )
            document_summary = final_response.choices[0].message.content.strip()
            summaries.append({"chunk": 0, "summary": document_summary, "is_overall": True})
        except Exception as e:
            print(f"[analyze_document_internal] –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—â–µ–≥–æ —Ä–µ–∑—é–º–µ: {e}")
    
    print(f"[analyze_document_internal] Analysis complete for doc_id={doc_id}, user_id={current_user.id}")
    
    return {"summaries": summaries, "doc_type": doc_type}

@router.post("/analyze-document/{doc_id}")
def analyze_document(doc_id: int, current_user: models.User = Depends(auth.get_current_user), db: Session = Depends(get_db)):
    print(f"[analyze_document] User {current_user.id} ({current_user.email}) requests analysis for doc_id={doc_id}")
    doc = db.query(models.Document).filter(models.Document.id == doc_id, models.Document.user_id == current_user.id).first()
    if not doc:
        print(f"[analyze_document] Document not found in DB for user_id={current_user.id}, doc_id={doc_id}")
        raise HTTPException(status_code=404, detail="Document not found")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º user_id –¥–ª—è –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É, –∫–∞–∫ –≤ upload_document
    file_path = os.path.join("uploads", str(current_user.id), doc.filename)
    if not os.path.exists(file_path):
        print(f"[analyze_document] File not found: {file_path}")
        print(f"[analyze_document] Critical error in initialization: 404: File not found")
        raise HTTPException(status_code=404, detail="File not found")
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –ø—É–ª —Ç–æ–∫–µ–Ω–æ–≤ –≤–º–µ—Å—Ç–æ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
    from ai.ai_token_manager import ai_token_manager
    
    # --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ ---
    text = ""
    file_extension = os.path.splitext(file_path)[1].lower()
    
    if file_extension == '.pdf':
        try:
            import PyPDF2
            with open(file_path, 'rb') as pdf_file:
                pdf_reader = PyPDF2.PdfReader(pdf_file)
                for page_num in range(len(pdf_reader.pages)):
                    text += pdf_reader.pages[page_num].extract_text() + "\n"
        except Exception as e:
            print(f"[analyze_document] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}")
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}")
    elif file_extension == '.docx':
        try:
            from docx import Document as DocxDoc
            docx = DocxDoc(file_path)
            text = '\n'.join([p.text for p in docx.paragraphs])
        except Exception as e:
            print(f"[analyze_document] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è .docx: {e}")
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è .docx: {e}")
    elif file_extension == '.doc':
        try:
            # –î–ª—è .doc —Ñ–∞–π–ª–æ–≤ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å textract –∏–ª–∏ –¥—Ä—É–≥–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
            # –ó–¥–µ—Å—å –ø—Ä–æ—Å—Ç–æ–µ —á—Ç–µ–Ω–∏–µ –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                text = f.read()
        except Exception as e:
            print(f"[analyze_document] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è .doc: {e}")
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è .doc: {e}")
    else:
        # –î–ª—è .txt –∏ –¥—Ä—É–≥–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                text = f.read()
        except Exception as e:
            print(f"[analyze_document] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è gpt-4o (16384 —Ç–æ–∫–µ–Ω–∞)
    # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ 1 —Ç–æ–∫–µ–Ω = 4 —Å–∏–º–≤–æ–ª–∞, –æ—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ –∏ –æ—Ç–≤–µ—Ç–∞
    total_length = len(text)
    if total_length < 40000:
        chunk_size = total_length  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ü–µ–ª–∏–∫–æ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–æ 40KB –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
    elif total_length < 120000:
        chunk_size = 30000  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –¥–µ—Ç–∞–ª–µ–π
    else:
        chunk_size = 20000   # –î–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ç–æ–∂–µ –±–æ–ª—å—à–∏–µ —á–∞–Ω–∫–∏
    
    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
    chunks = []
    current_pos = 0
    while current_pos < len(text):
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–ª–∏ –∞–±–∑–∞—Ü–∞ –¥–ª—è –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        end_pos = min(current_pos + chunk_size, len(text))
        if end_pos < len(text):
            # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–ª–∏ –∞–±–∑–∞—Ü–∞
            for separator in ['\n\n', '\n', '. ', '! ', '? ']:
                last_separator = text.rfind(separator, current_pos, end_pos)
                if last_separator > current_pos + chunk_size // 2:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –Ω–µ –≤ –Ω–∞—á–∞–ª–µ —á–∞–Ω–∫–∞
                    end_pos = last_separator + len(separator)
                    break
        
        chunks.append(text[current_pos:end_pos])
        current_pos = end_pos
    
    summaries = []
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    doc_type_prompt = f"–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è, —Ä–µ–≥–ª–∞–º–µ–Ω—Ç, —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫, —Å—Ç–∞—Ç—å—è –∏ —Ç.–¥.) –∏ –µ–≥–æ –æ—Å–Ω–æ–≤–Ω—É—é —Ç–µ–º—É. –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å: {text[:500]}..."
    try:
        doc_type_response = ai_token_manager.make_openai_request(
            messages=[
                {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –∏ —Ç–µ–º—É –¥–æ–∫—É–º–µ–Ω—Ç–∞."},
                {"role": "user", "content": doc_type_prompt}
            ],
            model="gpt-4o",
            user_id=current_user.id,
            assistant_id=None
        )
        doc_type = doc_type_response.choices[0].message.content.strip()
    except Exception as e:
        print(f"[analyze_document] –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
        doc_type = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞"
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    for idx, chunk in enumerate(chunks):
        prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Ç–∏–ø–∞ "{doc_type}" –∏ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –≤—ã–¥–µ–ª–∏ –í–°–ï –≤–∞–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏:

1. –ö–õ–Æ–ß–ï–í–´–ï –§–ê–ö–¢–´ (—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –í–°–ï–• –¥–µ—Ç–∞–ª–µ–π):
   - –ù–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π, –ø—Ä–æ–¥—É–∫—Ç–æ–≤
   - –î–∞—Ç—ã, —Ü–∏—Ñ—Ä—ã, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
   - –ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (email, —Ç–µ–ª–µ—Ñ–æ–Ω—ã, –∞–¥—Ä–µ—Å–∞)
   - –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã, —á–∞—Å—ã, —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è

2. –£–°–õ–£–ì–ò –ò –ü–†–û–î–£–ö–¢–´ (—Å –ü–û–õ–ù–´–ú–ò –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏):
   - –¢–æ—á–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫—É—Ä—Å–æ–≤, –ø—Ä–æ–≥—Ä–∞–º–º, —É—Å–ª—É–≥
   - –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
   - –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

3. FAQ –ò –í–û–ü–†–û–°–´ (—Å–æ—Ö—Ä–∞–Ω–∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ):
   - –í–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã –ø–æ–ª–Ω–æ—Å—Ç—å—é
   - –ü—Ä–∞–≤–∏–ª–∞ –∏ —É—Å–ª–æ–≤–∏—è
   - –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º

4. –§–£–ù–ö–¶–ò–ò –ò –ó–ê–î–ê–ß–ò (–µ—Å–ª–∏ –µ—Å—Ç—å):
   - –ß—Ç–æ —É–º–µ–µ—Ç –¥–µ–ª–∞—Ç—å —Å–∏—Å—Ç–µ–º–∞/–±–æ—Ç
   - –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

6. üí∞ –¶–ï–ù–´ –ò –¢–ê–†–ò–§–´ (–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –∏–∑–≤–ª–µ–∫–∏ –í–°–ï —Ü–µ–Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ):
   - –°—Ç–æ–∏–º–æ—Å—Ç—å —É—Å–ª—É–≥ (–≤–∫–ª—é—á–∞—è —Ü–µ–Ω—ã —Å –ø–æ–º–µ—Ç–∫–∞–º–∏ "???", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ", "~", "–ø—Ä–∏–º–µ—Ä–Ω–æ")
   - –ü–ª–∞–Ω—ã –∏ –ø–∞–∫–µ—Ç—ã
   - –°–∫–∏–¥–∫–∏ –∏ –∞–∫—Ü–∏–∏
   - –û–°–û–ë–û –í–ê–ñ–ù–û: –µ—Å–ª–∏ —Ü–µ–Ω–∞ —É–∫–∞–∑–∞–Ω–∞ –∫–∞–∫ "???", "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ", "~", "–ø—Ä–∏–º–µ—Ä–Ω–æ" - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —Å–æ—Ö—Ä–∞–Ω–∏ —ç—Ç—É –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å

7. üö® –ü–†–û–ë–õ–ï–ú–´ –ò –ñ–ê–õ–û–ë–´ –ö–õ–ò–ï–ù–¢–û–í (–ù–û–í–´–ô –†–ê–ó–î–ï–õ - –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–π):
   - –ñ–∞–ª–æ–±—ã –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ —Å–µ—Ä–≤–∏—Å–∞ ("–Ω–µ –ø–µ—Ä–µ–∑–≤–æ–Ω–∏–ª–∏", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–ø—Ä–æ–±–ª–µ–º—ã")
   - –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–∑—ã–≤—ã –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
   - –£–ø–æ–º–∏–Ω–∞–Ω–∏—è –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å –º–µ–Ω–µ–¥–∂–µ—Ä–∞–º–∏, –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
   - –õ—é–±—ã–µ —Ñ—Ä–∞–∑—ã —Ç–∏–ø–∞: "–æ–±–µ—â–∞–ª–∏, –Ω–æ...", "—Å–∫–∞–∑–∞–ª–∏, —á—Ç–æ –±—É–¥–µ—Ç, –Ω–æ...", "–¥–æ —Å–∏—Ö –ø–æ—Ä –Ω–µ—Ç"
   - –°–∞—Ä–∫–∞–∑–º –∏ –∏—Ä–æ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ

8. üìù –û–°–û–ë–ï–ù–ù–û–°–¢–ò –î–û–ö–£–ú–ï–ù–¢–ê:
   - –°—Ç–∏–ª—å –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π, —é–º–æ—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π, —Å–∞—Ç–∏—Ä–∞)
   - –ü–æ–ª–Ω–æ—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (–µ—Å—Ç—å –ª–∏ –ø—Ä–æ–ø—É—Å–∫–∏, –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–∏)
   - –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
   - –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

9. üîç –ù–ï–û–ü–†–ï–î–ï–õ–ï–ù–ù–û–°–¢–ò –ò –°–ü–ï–¶–°–ò–ú–í–û–õ–´ (–ù–û–í–´–ô –†–ê–ó–î–ï–õ):
   - –í—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è "???", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ", "–•–ó", "~", "–ø—Ä–∏–º–µ—Ä–Ω–æ"
   - –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (—Ä–∞–∑–Ω—ã–µ –¥–∞—Ç—ã, —Ü–∏—Ñ—Ä—ã)
   - –ù–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–µ—Ç–∫–∞–º–∏
   - –≠–º–æ–¥–∑–∏ —Å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º (ü§∑‚Äç‚ôÇÔ∏è, ü§î)

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
- –ù–ï –æ–±–æ–±—â–∞–π –∏ –ù–ï –ø—Ä–æ–ø—É—Å–∫–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ—Ç–∞–ª–∏!
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï —É–ø–æ–º—è–Ω—É—Ç—ã–µ email –∞–¥—Ä–µ—Å–∞ —Ç–æ—á–Ω–æ –∫–∞–∫ –Ω–∞–ø–∏—Å–∞–Ω–æ, –æ—Ç–º–µ—Ç—å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–µ –Ω–æ–º–µ—Ä–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é, –≤–∫–ª—é—á–∞—è –Ω–µ–ø–æ–ª–Ω—ã–µ
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï –Ω–∞–∑–≤–∞–Ω–∏—è –∫—É—Ä—Å–æ–≤ –∏ —É—Å–ª—É–≥ –¥–æ—Å–ª–æ–≤–Ω–æ
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï FAQ –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã —Ü–µ–ª–∏–∫–æ–º
- –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –∏–∑–≤–ª–µ–∫–∞–π –∂–∞–ª–æ–±—ã –∏ –ø—Ä–æ–±–ª–µ–º—ã –∫–ª–∏–µ–Ω—Ç–æ–≤
- –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —Å–æ—Ö—Ä–∞–Ω—è–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ü–µ–Ω–∞—Ö –∏ –¥–∞–Ω–Ω—ã—Ö
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–ø–æ–ª–Ω–∞—è, —é–º–æ—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–æ–±–ª–µ–º—ã - —á–µ—Ç–∫–æ —É–∫–∞–∂–∏ —ç—Ç–æ

–¢–µ–∫—Å—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞:
{chunk}

–°–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤—ã–∂–∏–º–∫—É –¥–ª—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –í–°–ï –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –ø—Ä–æ–±–ª–µ–º—ã –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏.
"""
        try:
            response = ai_token_manager.make_openai_request(
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –∑–Ω–∞–Ω–∏–π. –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: —Å–æ—Ö—Ä–∞–Ω—è–π –í–°–ï –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ - email –∞–¥—Ä–µ—Å–∞, —Ç–µ–ª–µ—Ñ–æ–Ω—ã, –Ω–∞–∑–≤–∞–Ω–∏—è, —Ü–µ–Ω—ã, –¥–∞—Ç—ã. –ù–ï –æ–±–æ–±—â–∞–π –∏ –ù–ï –ø—Ä–æ–ø—É—Å–∫–∞–π –¥–µ—Ç–∞–ª–∏! –û–°–û–ë–û –í–ê–ñ–ù–û: –∏–∑–≤–ª–µ–∫–∞–π –∂–∞–ª–æ–±—ã –∫–ª–∏–µ–Ω—Ç–æ–≤, –ø—Ä–æ–±–ª–µ–º—ã —Å–µ—Ä–≤–∏—Å–∞, –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ü–µ–Ω–∞—Ö (???, ~, –ø—Ä–∏–º–µ—Ä–Ω–æ). –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Å–æ–∑–¥–∞—Ç—å –ü–û–õ–ù–£–Æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤—ã–∂–∏–º–∫—É –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –≤–∫–ª—é—á–∞—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –ø—Ä–æ–±–ª–µ–º—ã, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º –¥–ª—è —á–µ—Å—Ç–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º."},
                    {"role": "user", "content": prompt}
                ],
                model="gpt-4o",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å —Å –±–æ–ª—å—à–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                user_id=current_user.id,
                assistant_id=None,
                temperature=0.05  # –ï—â–µ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            )
            summary = response.choices[0].message.content.strip()
        except Exception as e:
            print(f"[analyze_document] –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —á–∞–Ω–∫–∞ {idx+1}: {e}")
            summary = f"[–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}]"
        
        summaries.append({"chunk": idx+1, "summary": summary})
    
    # –°–æ–∑–¥–∞–µ–º –æ–±—â–µ–µ —Ä–µ–∑—é–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    if len(chunks) > 1:
        try:
            all_summaries = "\n\n".join([s["summary"] for s in summaries])
            final_prompt = f"""
–ù–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –≤—ã–∂–∏–º–æ–∫ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Ç–∏–ø–∞ "{doc_type}", —Å–æ–∑–¥–∞–π –µ–¥–∏–Ω–æ–µ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞:

{all_summaries}

–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ, –∫–æ—Ç–æ—Ä–æ–µ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
"""
            final_response = ai_token_manager.make_openai_request(
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –°–æ–∑–¥–∞–π –µ–¥–∏–Ω–æ–µ —Ä–µ–∑—é–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã–∂–∏–º–æ–∫ –∏–∑ –µ–≥–æ —á–∞—Å—Ç–µ–π."},
                    {"role": "user", "content": final_prompt}
                ],
                model="gpt-4o",
                user_id=current_user.id,
                assistant_id=None
            )
            document_summary = final_response.choices[0].message.content.strip()
            summaries.append({"chunk": 0, "summary": document_summary, "is_overall": True})
        except Exception as e:
            print(f"[analyze_document] –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—â–µ–≥–æ —Ä–µ–∑—é–º–µ: {e}")
    
    print(f"[analyze_document] Analysis complete for doc_id={doc_id}, user_id={current_user.id}")
    
    # üî• –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –≥–æ—Ä—è—á–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –±–æ—Ç–æ–≤ –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    # (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –µ—Å—Ç—å –Ω–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –±–æ—Ç–æ–≤)
    assistants = db.query(models.Assistant).filter(models.Assistant.user_id == current_user.id).all()
    print(f"[analyze_document] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ {len(assistants)} –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    for assistant in assistants:
        hot_reload_assistant_bots(assistant.id, db)
    
    return {"summaries": summaries, "doc_type": doc_type}

# --- Knowledge Management ---

class KnowledgeIn(BaseModel):
    doc_id: int
    content: str
    type: str  # 'summary' –∏–ª–∏ 'original'
    doc_type: Optional[str] = None
    importance: Optional[int] = 10
    assistant_id: Optional[int] = None  # –î–æ–±–∞–≤–ª—è–µ–º assistant_id

@router.post("/knowledge/confirm")
def confirm_knowledge(data: KnowledgeIn, current_user: models.User = Depends(auth.get_current_user), db: Session = Depends(get_db)):
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –¥–æ–∫—É–º–µ–Ω—Ç –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    doc = db.query(models.Document).filter(models.Document.id == data.doc_id, models.Document.user_id == current_user.id).first()
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")
    # –î–æ–±–∞–≤–∏—Ç—å –∑–Ω–∞–Ω–∏–µ
    knowledge = models.UserKnowledge(
        user_id=current_user.id,
        assistant_id=data.assistant_id,  # –ü—Ä–∏–≤—è–∑—ã–≤–∞–µ–º –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É
        doc_id=data.doc_id,
        content=data.content,
        type=data.type,
        doc_type=data.doc_type,
        importance=data.importance or 10
    )
    db.add(knowledge)
    db.commit()
    db.refresh(knowledge)
    
    # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à –∑–Ω–∞–Ω–∏–π
    invalidate_knowledge_cache(current_user.id, data.assistant_id)
    
    # üî• –ì–û–†–Ø–ß–ê–Ø –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –∏–ª–∏ –≤—Å–µ—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤
    print(f"[CONFIRM_KNOWLEDGE] –î–æ–±–∞–≤–ª–µ–Ω–æ –∑–Ω–∞–Ω–∏–µ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {current_user.id}, –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç {data.assistant_id}")
    if data.assistant_id:
        print(f"[CONFIRM_KNOWLEDGE] –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –±–æ—Ç–æ–≤ –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ {data.assistant_id}")
        hot_reload_assistant_bots(data.assistant_id, db)
    else:
        # –ï—Å–ª–∏ –∑–Ω–∞–Ω–∏–µ –æ–±—â–µ–µ - –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        assistants = db.query(models.Assistant).filter(models.Assistant.user_id == current_user.id).all()
        print(f"[CONFIRM_KNOWLEDGE] –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –±–æ—Ç–æ–≤ –¥–ª—è {len(assistants)} –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
        for assistant in assistants:
            hot_reload_assistant_bots(assistant.id, db)
    
    return {"ok": True, "id": knowledge.id}

@router.get("/knowledge/confirmed")
def get_confirmed_knowledge(
    assistant_id: int = Query(None),
    page: int = Query(1, ge=1, description="–ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã"),
    limit: int = Query(50, ge=1, le=100, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"),
    current_user: models.User = Depends(auth.get_current_user), 
    db: Session = Depends(get_db)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
    
    # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å EAGER LOADING –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è N+1
    query = db.query(models.UserKnowledge).options(
        joinedload(models.UserKnowledge.document)
    ).filter(models.UserKnowledge.user_id == current_user.id)
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É
    if assistant_id:
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
        assistant = db.query(models.Assistant).filter(
            models.Assistant.id == assistant_id,
            models.Assistant.user_id == current_user.id
        ).first()
        if not assistant:
            raise HTTPException(status_code=404, detail="Assistant not found")
        
        query = query.filter(models.UserKnowledge.assistant_id == assistant_id)
    
    # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
    total = query.count()
    
    # –ü–∞–≥–∏–Ω–∞—Ü–∏—è
    offset = (page - 1) * limit
    entries = query.order_by(models.UserKnowledge.id.desc()).offset(offset).limit(limit).all()
    
    items = []
    for entry in entries:
        try:
            content = json.loads(entry.content) if entry.type == 'summary' else entry.content
        except Exception:
            content = entry.content
        items.append({
            'id': entry.id,  # –î–æ–±–∞–≤–ª—è–µ–º ID –∏–∑ —Ç–∞–±–ª–∏—Ü—ã UserKnowledge
            'doc_id': entry.doc_id,
            'type': entry.type,
            'content': content,
            'doc_type': entry.doc_type,
            'importance': entry.importance,
            'last_used': entry.last_used.isoformat() if entry.last_used else None,
            'usage_count': entry.usage_count,
            'assistant_id': entry.assistant_id  # –î–æ–±–∞–≤–ª—è–µ–º assistant_id
        })
    
    return {
        "items": items,
        "total": total,
        "page": page,
        "pages": (total + limit - 1) // limit,
        "limit": limit
    }

@router.put("/knowledge/{knowledge_id}")
def update_knowledge(knowledge_id: int, data: KnowledgeIn, current_user: models.User = Depends(auth.get_current_user), db: Session = Depends(get_db)):
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–Ω–∞–Ω–∏–µ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    knowledge = db.query(models.UserKnowledge).filter(
        models.UserKnowledge.id == knowledge_id,
        models.UserKnowledge.user_id == current_user.id
    ).first()
    
    if not knowledge:
        raise HTTPException(status_code=404, detail="Knowledge not found")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∑–Ω–∞–Ω–∏–µ
    knowledge.content = data.content
    knowledge.type = data.type
    knowledge.doc_type = data.doc_type or knowledge.doc_type
    knowledge.importance = data.importance or knowledge.importance
    knowledge.assistant_id = data.assistant_id
    
    db.commit()
    
    # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à –∑–Ω–∞–Ω–∏–π
    invalidate_knowledge_cache(current_user.id, knowledge.assistant_id)
    
    # üî• –ì–û–†–Ø–ß–ê–Ø –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –∏–ª–∏ –≤—Å–µ—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤
    if knowledge.assistant_id:
        hot_reload_assistant_bots(knowledge.assistant_id, db)
    else:
        # –ï—Å–ª–∏ –∑–Ω–∞–Ω–∏–µ –æ–±—â–µ–µ - –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        assistants = db.query(models.Assistant).filter(models.Assistant.user_id == current_user.id).all()
        for assistant in assistants:
            hot_reload_assistant_bots(assistant.id, db)
    
    return {"ok": True}

@router.delete("/knowledge/{knowledge_id}")
def delete_knowledge(knowledge_id: int, current_user: models.User = Depends(auth.get_current_user), db: Session = Depends(get_db)):
    """
    –£–ª—É—á—à–µ–Ω–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –∑–Ω–∞–Ω–∏–π —Å –ø–æ–ª–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π –≤—Å–µ—Ö —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    from utils.bot_cleanup import enhanced_knowledge_deletion
    
    try:
        success = enhanced_knowledge_deletion(knowledge_id, current_user.id, db)
        
        if success:
            logger.info(f"[DELETE_KNOWLEDGE] ‚úÖ Knowledge {knowledge_id} completely deleted with full cleanup")
            return {"ok": True}
        else:
            raise HTTPException(status_code=404, detail="Knowledge not found or deletion failed")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[DELETE_KNOWLEDGE] Error: {e}")
        raise HTTPException(status_code=500, detail="Internal server error during knowledge deletion")

@router.get("/knowledge/stats")
def get_knowledge_stats(current_user: models.User = Depends(auth.get_current_user), db: Session = Depends(get_db)):
    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    knowledge_count = db.query(models.UserKnowledge).filter(
        models.UserKnowledge.user_id == current_user.id
    ).count()
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    most_used = db.query(
        models.UserKnowledge,
        models.Document.filename
    ).join(
        models.Document, 
        models.UserKnowledge.doc_id == models.Document.id
    ).filter(
        models.UserKnowledge.user_id == current_user.id
    ).order_by(
        models.UserKnowledge.usage_count.desc()
    ).limit(5).all()
    
    top_docs = []
    for k, filename in most_used:
        top_docs.append({
            "id": k.id,
            "doc_id": k.doc_id,
            "filename": filename,
            "doc_type": k.doc_type,
            "usage_count": k.usage_count,
            "last_used": k.last_used.isoformat() if k.last_used else None,
            "importance": k.importance
        })
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –∏—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    doc_types = db.query(
        models.UserKnowledge.doc_type,
        func.count(models.UserKnowledge.id).label('count')
    ).filter(
        models.UserKnowledge.user_id == current_user.id
    ).group_by(
        models.UserKnowledge.doc_type
    ).all()
    
    types_count = {}
    for doc_type, count in doc_types:
        types_count[doc_type or '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π'] = count
    
    return {
        "total_documents": knowledge_count,
        "most_used": top_docs,
        "document_types": types_count,
        "total_content_length": sum(len(k.content or '') for k, _ in most_used),
        "avg_importance": sum(k.importance or 0 for k, _ in most_used) / len(most_used) if most_used else 0
    }

@router.post("/knowledge/track-usage")
def track_document_usage(
    used_docs: List[str] = Body(..., description="–°–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"),
    current_user: models.User = Depends(auth.get_current_user), 
    db: Session = Depends(get_db)
):
    """–û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –æ—Ç–≤–µ—Ç–∞—Ö AI"""
    if not used_docs:
        return {"message": "–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è"}
    
    updated_count = 0
    
    for doc_content in used_docs:
        # –ò—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É (—á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)
        knowledge_entries = db.query(models.UserKnowledge).filter(
            models.UserKnowledge.user_id == current_user.id
        ).all()
        
        for entry in knowledge_entries:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
            # –ò–ª–∏ –Ω–∞–æ–±–æ—Ä–æ—Ç - —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            if (entry.content in doc_content or doc_content in entry.content):
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                entry.usage_count = (entry.usage_count or 0) + 1
                entry.last_used = datetime.utcnow()
                updated_count += 1
                break
    
    db.commit()
    
    return {
        "message": f"–û–±–Ω–æ–≤–ª–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è {updated_count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
        "updated_count": updated_count
    }