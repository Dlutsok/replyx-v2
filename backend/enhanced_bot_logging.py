#!/usr/bin/env python3
"""
üîç –£–õ–£–ß–®–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø –î–õ–Ø –¢–†–ê–°–°–ò–†–û–í–ö–ò –ò–°–¢–û–ß–ù–ò–ö–û–í –û–¢–í–ï–¢–û–í –ë–û–¢–û–í

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –¥–æ–±–∞–≤–ª—è–µ—Ç –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è:
- –û—Ç–∫—É–¥–∞ –±–æ—Ç –±–µ—Ä–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞
- –ö–∞–∫–∏–µ —á–∞–Ω–∫–∏ embeddings –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è
- –ö–∞–∫–∏–µ fallback-–º–µ—Ö–∞–Ω–∏–∑–º—ã —Å—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç
- –ö–∞–∫–∏–µ –∫—ç—à–∏ –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω—ã
"""

import logging
import json
import hashlib
from datetime import datetime
from typing import Dict, List, Any, Optional
from sqlalchemy.orm import Session

# –°–æ–∑–¥–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π logger –¥–ª—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏ –±–æ—Ç–æ–≤
bot_tracer = logging.getLogger('bot_tracer')
bot_tracer.setLevel(logging.INFO)

# –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª–æ–≤—ã–π handler –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏
if not bot_tracer.handlers:
    file_handler = logging.FileHandler('bot_response_trace.log', encoding='utf-8')
    formatter = logging.Formatter(
        '%(asctime)s | %(levelname)s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(formatter)
    bot_tracer.addHandler(file_handler)

class BotResponseTracer:
    """–¢—Ä–∞—Å—Å–∏—Ä–æ–≤—â–∏–∫ –æ—Ç–≤–µ—Ç–æ–≤ –±–æ—Ç–æ–≤ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    
    def __init__(self, user_id: int, assistant_id: int, query: str):
        self.user_id = user_id
        self.assistant_id = assistant_id
        self.query = query
        self.trace_id = hashlib.md5(
            f"{user_id}_{assistant_id}_{query}_{datetime.now().isoformat()}".encode()
        ).hexdigest()[:8]
        
        self.trace_data = {
            'trace_id': self.trace_id,
            'timestamp': datetime.now().isoformat(),
            'user_id': user_id,
            'assistant_id': assistant_id,
            'query': query,
            'sources_checked': [],
            'sources_used': [],
            'fallbacks_triggered': [],
            'cache_hits': [],
            'final_response': None
        }
        
        bot_tracer.info(f"üîç TRACE_START [{self.trace_id}] User:{user_id} Assistant:{assistant_id} Query:'{query}'")
    
    def log_embeddings_search(self, found_chunks: List[Dict], search_params: Dict):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ embeddings"""
        
        source_info = {
            'type': 'embeddings_search',
            'params': search_params,
            'found_chunks': len(found_chunks),
            'chunks_details': []
        }
        
        for i, chunk in enumerate(found_chunks):
            chunk_info = {
                'index': i,
                'similarity': chunk.get('similarity', 0),
                'doc_type': chunk.get('doc_type', 'unknown'),
                'importance': chunk.get('importance', 0),
                'token_count': chunk.get('token_count', 0),
                'text_preview': chunk.get('text', '')[:100] + '...' if chunk.get('text') else ''
            }
            source_info['chunks_details'].append(chunk_info)
        
        self.trace_data['sources_checked'].append(source_info)
        
        if found_chunks:
            self.trace_data['sources_used'].append({
                'type': 'embeddings',
                'count': len(found_chunks),
                'avg_similarity': sum(c.get('similarity', 0) for c in found_chunks) / len(found_chunks)
            })
            
            bot_tracer.info(f"üìä EMBEDDINGS_FOUND [{self.trace_id}] Found {len(found_chunks)} chunks, "
                          f"avg similarity: {sum(c.get('similarity', 0) for c in found_chunks) / len(found_chunks):.3f}")
        else:
            bot_tracer.info(f"‚ùå EMBEDDINGS_EMPTY [{self.trace_id}] No relevant chunks found")
    
    def log_fallback_knowledge(self, knowledge_entries: List, fallback_type: str = "user_knowledge"):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ fallback –∫ —Å—Ç–∞—Ä–æ–π —Å–∏—Å—Ç–µ–º–µ –∑–Ω–∞–Ω–∏–π"""
        
        fallback_info = {
            'type': fallback_type,
            'entries_count': len(knowledge_entries),
            'entries_details': []
        }
        
        for entry in knowledge_entries[:3]:  # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 3
            entry_info = {
                'id': getattr(entry, 'id', 'unknown'),
                'doc_type': getattr(entry, 'doc_type', 'unknown'),
                'importance': getattr(entry, 'importance', 0),
                'content_preview': getattr(entry, 'content', '')[:100] + '...' if hasattr(entry, 'content') else ''
            }
            fallback_info['entries_details'].append(entry_info)
        
        self.trace_data['fallbacks_triggered'].append(fallback_info)
        
        bot_tracer.info(f"‚ö†Ô∏è  FALLBACK_TRIGGERED [{self.trace_id}] Type: {fallback_type}, "
                       f"entries: {len(knowledge_entries)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –¥–∞–Ω–Ω—ã–µ
        for entry in knowledge_entries[:5]:
            content = getattr(entry, 'content', '').lower()
            if any(phrase in content for phrase in ['–≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã', '–ø–Ω-–ø—Ç', '09:00', '18:00']):
                bot_tracer.warning(f"üö® POTENTIAL_STALE_DATA [{self.trace_id}] "
                                 f"Found working hours info in fallback entry {getattr(entry, 'id', 'unknown')}")
    
    def log_cache_hit(self, cache_type: str, cache_key: str, hit: bool):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –ø–æ–ø–∞–¥–∞–Ω–∏—è/–ø—Ä–æ–º–∞—Ö–∏ –∫—ç—à–∞"""
        
        cache_info = {
            'type': cache_type,
            'key_hash': hashlib.md5(cache_key.encode()).hexdigest()[:8],
            'hit': hit
        }
        
        if hit:
            self.trace_data['cache_hits'].append(cache_info)
            bot_tracer.info(f"üíæ CACHE_HIT [{self.trace_id}] Type: {cache_type}")
        else:
            bot_tracer.info(f"üíæ CACHE_MISS [{self.trace_id}] Type: {cache_type}")
    
    def log_system_prompt_usage(self, system_prompt: str):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ö–∞—Ä–¥–∫–æ–¥ –≤ –ø—Ä–æ–º–ø—Ç–µ
        prompt_lower = system_prompt.lower()
        suspicious_phrases = ['–≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã', '–ø–Ω-–ø—Ç', '09:00', '18:00', '—Ä–∞–±–æ—á–∏–µ —á–∞—Å—ã']
        
        found_phrases = [phrase for phrase in suspicious_phrases if phrase in prompt_lower]
        
        if found_phrases:
            bot_tracer.warning(f"üö® HARDCODED_INFO [{self.trace_id}] "
                             f"Found suspicious phrases in system prompt: {found_phrases}")
            
            self.trace_data['sources_used'].append({
                'type': 'system_prompt_hardcode',
                'phrases': found_phrases
            })
    
    def log_final_response(self, response: str, model_used: str = None):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç"""
        
        self.trace_data['final_response'] = {
            'text': response[:200] + '...' if len(response) > 200 else response,
            'length': len(response),
            'model': model_used,
            'contains_working_hours': any(phrase in response.lower() 
                                        for phrase in ['–≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã', '–ø–Ω-–ø—Ç', '09:00', '18:00'])
        }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –æ—Ç–≤–µ—Ç–∞
        if not self.trace_data['sources_used'] and not self.trace_data['fallbacks_triggered']:
            bot_tracer.warning(f"ü§î UNKNOWN_SOURCE [{self.trace_id}] "
                             f"Response generated without tracked knowledge sources")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —É—Å—Ç–∞—Ä–µ–≤—à—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –æ—Ç–≤–µ—Ç–µ
        if self.trace_data['final_response']['contains_working_hours']:
            bot_tracer.warning(f"üö® WORKING_HOURS_IN_RESPONSE [{self.trace_id}] "
                             f"Response contains working hours information")
        
        bot_tracer.info(f"‚úÖ TRACE_END [{self.trace_id}] Response length: {len(response)}, "
                       f"Sources used: {len(self.trace_data['sources_used'])}, "
                       f"Fallbacks: {len(self.trace_data['fallbacks_triggered'])}")
    
    def get_trace_summary(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≤–æ–¥–∫—É —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏"""
        return {
            'trace_id': self.trace_id,
            'query': self.query,
            'sources_summary': {
                'embeddings_chunks': sum(1 for s in self.trace_data['sources_used'] if s['type'] == 'embeddings'),
                'fallback_entries': sum(f['entries_count'] for f in self.trace_data['fallbacks_triggered']),
                'cache_hits': len(self.trace_data['cache_hits']),
                'hardcoded_sources': sum(1 for s in self.trace_data['sources_used'] if 'hardcode' in s['type'])
            },
            'potential_issues': {
                'no_tracked_sources': not self.trace_data['sources_used'] and not self.trace_data['fallbacks_triggered'],
                'contains_working_hours': self.trace_data.get('final_response', {}).get('contains_working_hours', False),
                'used_fallback': len(self.trace_data['fallbacks_triggered']) > 0
            }
        }

def patch_bot_response_with_tracing():
    """
    –ü–∞—Ç—á–∏—Ç —Ñ—É–Ω–∫—Ü–∏—é get_bot_ai_response –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏
    """
    
    # –≠—Ç–æ—Ç –∫–æ–¥ –Ω—É–∂–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –≤ api/bots.py
    patch_code = '''
# –í –Ω–∞—á–∞–ª–µ —Ñ—É–Ω–∫—Ü–∏–∏ get_bot_ai_response –¥–æ–±–∞–≤–∏—Ç—å:
from enhanced_bot_logging import BotResponseTracer

tracer = BotResponseTracer(user_id, assistant_id, message)

# –ü–æ—Å–ª–µ –ø–æ–∏—Å–∫–∞ embeddings:
tracer.log_embeddings_search(relevant_chunks, {
    'top_k': 5,
    'min_similarity': 0.7,
    'user_id': user_id,
    'assistant_id': assistant_id
})

# –ü–æ—Å–ª–µ fallback –∫ UserKnowledge:
if not relevant_chunks and knowledge_entries:
    tracer.log_fallback_knowledge(knowledge_entries, "user_knowledge_fallback")

# –ü—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫—ç—à–∞ AI –æ—Ç–≤–µ—Ç–æ–≤:
tracer.log_cache_hit("ai_response", messages_hash, cached_response is not None)

# –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞:
tracer.log_system_prompt_usage(system_prompt)

# –í –∫–æ–Ω—Ü–µ, –ø–µ—Ä–µ–¥ –≤–æ–∑–≤—Ä–∞—Ç–æ–º –æ—Ç–≤–µ—Ç–∞:
tracer.log_final_response(ai_response, ai_model)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º trace –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
trace_summary = tracer.get_trace_summary()
if trace_summary['potential_issues']['contains_working_hours']:
    logger.warning(f"Potential stale data in response for user {user_id}")
'''
    
    return patch_code

def analyze_traces(hours: int = 24) -> Dict[str, Any]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–æ–≥–∏ —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏ –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥
    
    Args:
        hours: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 24)
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∞–Ω–∞–ª–∏–∑–æ–º –ø—Ä–æ–±–ª–µ–º
    """
    
    import re
    from datetime import datetime, timedelta
    
    cutoff_time = datetime.now() - timedelta(hours=hours)
    
    issues = {
        'stale_data_responses': [],
        'unknown_source_responses': [],
        'fallback_usage': [],
        'hardcoded_info_usage': []
    }
    
    try:
        with open('bot_response_trace.log', 'r', encoding='utf-8') as f:
            for line in f:
                if 'WORKING_HOURS_IN_RESPONSE' in line:
                    match = re.search(r'\[(.*?)\].*TRACE_ID \[(.*?)\]', line)
                    if match:
                        timestamp_str, trace_id = match.groups()
                        try:
                            log_time = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
                            if log_time >= cutoff_time:
                                issues['stale_data_responses'].append({
                                    'trace_id': trace_id,
                                    'timestamp': timestamp_str,
                                    'line': line.strip()
                                })
                        except ValueError:
                            continue
                
                elif 'UNKNOWN_SOURCE' in line:
                    match = re.search(r'\[(.*?)\].*TRACE_ID \[(.*?)\]', line)
                    if match:
                        timestamp_str, trace_id = match.groups()
                        try:
                            log_time = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
                            if log_time >= cutoff_time:
                                issues['unknown_source_responses'].append({
                                    'trace_id': trace_id,
                                    'timestamp': timestamp_str,
                                    'line': line.strip()
                                })
                        except ValueError:
                            continue
                
                elif 'FALLBACK_TRIGGERED' in line:
                    match = re.search(r'\[(.*?)\].*TRACE_ID \[(.*?)\]', line)
                    if match:
                        timestamp_str, trace_id = match.groups()
                        try:
                            log_time = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
                            if log_time >= cutoff_time:
                                issues['fallback_usage'].append({
                                    'trace_id': trace_id,
                                    'timestamp': timestamp_str,
                                    'line': line.strip()
                                })
                        except ValueError:
                            continue
                
                elif 'HARDCODED_INFO' in line:
                    match = re.search(r'\[(.*?)\].*TRACE_ID \[(.*?)\]', line)
                    if match:
                        timestamp_str, trace_id = match.groups()
                        try:
                            log_time = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
                            if log_time >= cutoff_time:
                                issues['hardcoded_info_usage'].append({
                                    'trace_id': trace_id,
                                    'timestamp': timestamp_str,
                                    'line': line.strip()
                                })
                        except ValueError:
                            continue
    
    except FileNotFoundError:
        pass
    
    return issues

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    print("üîç –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤ —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞:")
    
    issues = analyze_traces(24)
    
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º:")
    print(f"  ‚Ä¢ –û—Ç–≤–µ—Ç—ã —Å —É—Å—Ç–∞—Ä–µ–≤—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏: {len(issues['stale_data_responses'])}")
    print(f"  ‚Ä¢ –û—Ç–≤–µ—Ç—ã —Å –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏: {len(issues['unknown_source_responses'])}")
    print(f"  ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ fallback: {len(issues['fallback_usage'])}")
    print(f"  ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ö–∞—Ä–¥–∫–æ–¥–∞: {len(issues['hardcoded_info_usage'])}")
    
    if issues['stale_data_responses']:
        print(f"\nüö® –û–¢–í–ï–¢–´ –° –£–°–¢–ê–†–ï–í–®–ò–ú–ò –î–ê–ù–ù–´–ú–ò:")
        for issue in issues['stale_data_responses'][:5]:
            print(f"  {issue['timestamp']} [{issue['trace_id']}]")