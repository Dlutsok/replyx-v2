"""
ðŸ—„ï¸ Ð¡Ð˜Ð¡Ð¢Ð•ÐœÐ Ð‘Ð­ÐšÐÐŸÐžÐ’ Ð‘ÐÐ—Ð« Ð”ÐÐÐÐ«Ð¥ ChatAI
ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¸ Ñ€ÑƒÑ‡Ð½Ñ‹Ðµ Ð±ÑÐºÐ°Ð¿Ñ‹ Ñ Ñ€Ð¾Ñ‚Ð°Ñ†Ð¸ÐµÐ¹ Ð¸ ÑÐ¶Ð°Ñ‚Ð¸ÐµÐ¼
"""

import os
import subprocess
import gzip
import shutil
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
import json
import boto3
from botocore.exceptions import ClientError

logger = logging.getLogger(__name__)

class DatabaseBackup:
    """Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð±ÑÐºÐ°Ð¿Ð¾Ð² PostgreSQL Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¸ Ð¾Ð±Ð»Ð°Ñ‡Ð½Ð¾Ð³Ð¾ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ"""
    
    def __init__(self):
        # ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð¸Ð· Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ
        self.db_host = os.getenv('DB_HOST', 'localhost')
        self.db_port = os.getenv('DB_PORT', '5432')
        self.db_name = os.getenv('DB_NAME', 'chat_ai')
        self.db_user = os.getenv('DB_USER', 'dan')
        self.db_password = os.getenv('DB_PASSWORD', '')
        
        # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð±ÑÐºÐ°Ð¿Ð¾Ð²
        self.backup_dir = Path(os.getenv('BACKUP_DIR', './data/backups'))
        self.max_local_backups = int(os.getenv('MAX_LOCAL_BACKUPS', '7'))  # 7 Ð´Ð½ÐµÐ¹
        self.max_weekly_backups = int(os.getenv('MAX_WEEKLY_BACKUPS', '4'))  # 4 Ð½ÐµÐ´ÐµÐ»Ð¸
        self.max_monthly_backups = int(os.getenv('MAX_MONTHLY_BACKUPS', '12'))  # 12 Ð¼ÐµÑÑÑ†ÐµÐ²
        
        # S3 ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
        self.s3_enabled = os.getenv('S3_BACKUP_ENABLED', 'false').lower() == 'true'
        self.s3_bucket = os.getenv('S3_BACKUP_BUCKET')
        self.s3_region = os.getenv('S3_BACKUP_REGION', 'us-east-1')
        self.aws_access_key = os.getenv('AWS_ACCESS_KEY_ID')
        self.aws_secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        (self.backup_dir / 'daily').mkdir(exist_ok=True)
        (self.backup_dir / 'weekly').mkdir(exist_ok=True)
        (self.backup_dir / 'monthly').mkdir(exist_ok=True)
        
        # S3 ÐºÐ»Ð¸ÐµÐ½Ñ‚
        self.s3_client = None
        if self.s3_enabled and self.s3_bucket:
            try:
                self.s3_client = boto3.client(
                    's3',
                    region_name=self.s3_region,
                    aws_access_key_id=self.aws_access_key,
                    aws_secret_access_key=self.aws_secret_key
                )
                logger.info(f"S3 Ð±ÑÐºÐ°Ð¿Ñ‹ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ñ‹: {self.s3_bucket}")
            except Exception as e:
                logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ S3: {e}")
                self.s3_enabled = False
    
    def create_backup(self, backup_type: str = 'daily') -> Optional[Dict]:
        """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð±ÑÐºÐ°Ð¿Ð° Ð‘Ð”"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_filename = f"chatai_backup_{backup_type}_{timestamp}.sql"
            backup_path = self.backup_dir / backup_type / backup_filename
            
            logger.info(f"Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ {backup_type} Ð±ÑÐºÐ°Ð¿Ð°: {backup_filename}")
            
            # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñƒ pg_dump
            cmd = [
                'pg_dump',
                '--host', self.db_host,
                '--port', self.db_port,
                '--username', self.db_user,
                '--dbname', self.db_name,
                '--no-password',  # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ .pgpass Ð¸Ð»Ð¸ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ
                '--verbose',
                '--clean',        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ DROP ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹
                '--if-exists',    # IF EXISTS Ð´Ð»Ñ DROP
                '--create',       # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð‘Ð”
                '--format=plain', # SQL Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚
                '--file', str(backup_path)
            ]
            
            # Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ð¿Ð°Ñ€Ð¾Ð»Ñ
            env = os.environ.copy()
            if self.db_password:
                env['PGPASSWORD'] = self.db_password
            
            # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ð±ÑÐºÐ°Ð¿
            result = subprocess.run(
                cmd,
                env=env,
                capture_output=True,
                text=True,
                timeout=3600  # 1 Ñ‡Ð°Ñ Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼
            )
            
            if result.returncode != 0:
                logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð±ÑÐºÐ°Ð¿Ð°: {result.stderr}")
                return None
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð°
            backup_size = backup_path.stat().st_size
            
            # Ð¡Ð¶Ð¸Ð¼Ð°ÐµÐ¼ Ð±ÑÐºÐ°Ð¿
            compressed_path = backup_path.with_suffix('.sql.gz')
            with open(backup_path, 'rb') as f_in:
                with gzip.open(compressed_path, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            
            # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð½ÐµÑÐ¶Ð°Ñ‚Ñ‹Ð¹ Ñ„Ð°Ð¹Ð»
            backup_path.unlink()
            
            compressed_size = compressed_path.stat().st_size
            compression_ratio = (1 - compressed_size / backup_size) * 100
            
            backup_info = {
                'timestamp': timestamp,
                'type': backup_type,
                'filename': compressed_path.name,
                'path': str(compressed_path),
                'size_original': backup_size,
                'size_compressed': compressed_size,
                'compression_ratio': round(compression_ratio, 2),
                'created_at': datetime.now().isoformat()
            }
            
            logger.info(f"Ð‘ÑÐºÐ°Ð¿ ÑÐ¾Ð·Ð´Ð°Ð½: {compressed_path.name} "
                       f"({self._format_size(compressed_size)}, "
                       f"ÑÐ¶Ð°Ñ‚Ð¸Ðµ {compression_ratio:.1f}%)")
            
            # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð² S3 ÐµÑÐ»Ð¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾
            if self.s3_enabled:
                self._upload_to_s3(compressed_path, backup_type)
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
            self._save_backup_metadata(backup_info)
            
            # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ€Ñ‹Ðµ Ð±ÑÐºÐ°Ð¿Ñ‹
            self._cleanup_old_backups(backup_type)
            
            return backup_info
            
        except subprocess.TimeoutExpired:
            logger.error("Ð¢Ð°Ð¹Ð¼Ð°ÑƒÑ‚ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð±ÑÐºÐ°Ð¿Ð° (> 1 Ñ‡Ð°ÑÐ°)")
            return None
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð±ÑÐºÐ°Ð¿Ð°: {e}")
            return None
    
    def restore_backup(self, backup_path: str) -> bool:
        """Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¸Ð· Ð±ÑÐºÐ°Ð¿Ð°"""
        try:
            backup_file = Path(backup_path)
            if not backup_file.exists():
                logger.error(f"Ð¤Ð°Ð¹Ð» Ð±ÑÐºÐ°Ð¿Ð° Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½: {backup_path}")
                return False
            
            logger.warning(f"Ð’ÐžÐ¡Ð¡Ð¢ÐÐÐžÐ’Ð›Ð•ÐÐ˜Ð• Ð‘Ð” Ð¸Ð· {backup_file.name}")
            logger.warning("Ð­Ñ‚Ð¾ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ Ð£Ð”ÐÐ›Ð˜Ð¢ Ð²ÑÐµ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ!")
            
            # Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ Ñ„Ð°Ð¹Ð» Ð´Ð»Ñ Ñ€Ð°ÑÐ¿Ð°ÐºÐ¾Ð²ÐºÐ¸
            if backup_file.suffix == '.gz':
                temp_sql = backup_file.with_suffix('')
                with gzip.open(backup_file, 'rb') as f_in:
                    with open(temp_sql, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                sql_file = temp_sql
            else:
                sql_file = backup_file
            
            # ÐšÐ¾Ð¼Ð°Ð½Ð´Ð° Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ
            cmd = [
                'psql',
                '--host', self.db_host,
                '--port', self.db_port,
                '--username', self.db_user,
                '--dbname', 'postgres',  # ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ÑÑ Ðº postgres Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð‘Ð”
                '--file', str(sql_file)
            ]
            
            env = os.environ.copy()
            if self.db_password:
                env['PGPASSWORD'] = self.db_password
            
            result = subprocess.run(
                cmd,
                env=env,
                capture_output=True,
                text=True,
                timeout=3600
            )
            
            # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ Ñ„Ð°Ð¹Ð»
            if backup_file.suffix == '.gz' and temp_sql.exists():
                temp_sql.unlink()
            
            if result.returncode != 0:
                logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ: {result.stderr}")
                return False
            
            logger.info(f"Ð‘Ð” ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð° Ð¸Ð· {backup_file.name}")
            return True
            
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð‘Ð”: {e}")
            return False
    
    def _upload_to_s3(self, backup_path: Path, backup_type: str):
        """Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð±ÑÐºÐ°Ð¿Ð° Ð² S3"""
        try:
            if not self.s3_client:
                return
            
            s3_key = f"chatai-backups/{backup_type}/{backup_path.name}"
            
            self.s3_client.upload_file(
                str(backup_path),
                self.s3_bucket,
                s3_key,
                ExtraArgs={
                    'StorageClass': 'STANDARD_IA',  # Ð”ÐµÑˆÐµÐ²Ð¾Ðµ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ
                    'ServerSideEncryption': 'AES256'
                }
            )
            
            logger.info(f"Ð‘ÑÐºÐ°Ð¿ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ Ð² S3: s3://{self.s3_bucket}/{s3_key}")
            
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð² S3: {e}")
    
    def _save_backup_metadata(self, backup_info: Dict):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð±ÑÐºÐ°Ð¿Ð°"""
        try:
            metadata_file = self.backup_dir / 'backup_metadata.json'
            
            # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
            if metadata_file.exists():
                with open(metadata_file, 'r') as f:
                    metadata = json.load(f)
            else:
                metadata = {'backups': []}
            
            # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ð¹ Ð±ÑÐºÐ°Ð¿
            metadata['backups'].append(backup_info)
            
            # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ (Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 100)
            metadata['backups'] = metadata['backups'][-100:]
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼
            with open(metadata_file, 'w') as f:
                json.dump(metadata, f, indent=2)
                
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…: {e}")
    
    def _cleanup_old_backups(self, backup_type: str):
        """ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° ÑÑ‚Ð°Ñ€Ñ‹Ñ… Ð±ÑÐºÐ°Ð¿Ð¾Ð²"""
        try:
            backup_folder = self.backup_dir / backup_type
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð±ÑÐºÐ°Ð¿Ð¾Ð²
            backup_files = list(backup_folder.glob('*.sql.gz'))
            backup_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð»Ð¸Ð¼Ð¸Ñ‚
            if backup_type == 'daily':
                limit = self.max_local_backups
            elif backup_type == 'weekly':
                limit = self.max_weekly_backups
            elif backup_type == 'monthly':
                limit = self.max_monthly_backups
            else:
                limit = 10
            
            # Ð£Ð´Ð°Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ€Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹
            for old_backup in backup_files[limit:]:
                logger.info(f"Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ ÑÑ‚Ð°Ñ€Ð¾Ð³Ð¾ {backup_type} Ð±ÑÐºÐ°Ð¿Ð°: {old_backup.name}")
                old_backup.unlink()
                
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸ ÑÑ‚Ð°Ñ€Ñ‹Ñ… Ð±ÑÐºÐ°Ð¿Ð¾Ð²: {e}")
    
    def get_backup_list(self) -> List[Dict]:
        """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÐ¿Ð¸ÑÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð±ÑÐºÐ°Ð¿Ð¾Ð²"""
        try:
            metadata_file = self.backup_dir / 'backup_metadata.json'
            
            if not metadata_file.exists():
                return []
            
            with open(metadata_file, 'r') as f:
                metadata = json.load(f)
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ„Ð°Ð¹Ð»Ð¾Ð²
            valid_backups = []
            for backup in metadata.get('backups', []):
                backup_path = Path(backup['path'])
                if backup_path.exists():
                    backup['exists'] = True
                    backup['size_formatted'] = self._format_size(backup['size_compressed'])
                    valid_backups.append(backup)
                else:
                    backup['exists'] = False
            
            return sorted(valid_backups, key=lambda x: x['created_at'], reverse=True)
            
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ ÑÐ¿Ð¸ÑÐºÐ° Ð±ÑÐºÐ°Ð¿Ð¾Ð²: {e}")
            return []
    
    def _format_size(self, size_bytes: int) -> str:
        """Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð° Ñ„Ð°Ð¹Ð»Ð°"""
        if size_bytes == 0:
            return "0 B"
        
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024
        
        return f"{size_bytes:.1f} TB"
    
    def create_scheduled_backups(self):
        """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð·Ð°Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð±ÑÐºÐ°Ð¿Ð¾Ð² (Ð²Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑ‚ÑÑ Ð¿Ð¾ cron)"""
        now = datetime.now()
        
        # Ð•Ð¶ÐµÐ´Ð½ÐµÐ²Ð½Ñ‹Ð¹ Ð±ÑÐºÐ°Ð¿
        daily_backup = self.create_backup('daily')
        if not daily_backup:
            logger.error("ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ ÐµÐ¶ÐµÐ´Ð½ÐµÐ²Ð½Ð¾Ð³Ð¾ Ð±ÑÐºÐ°Ð¿Ð°")
            return False
        
        # Ð•Ð¶ÐµÐ½ÐµÐ´ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð±ÑÐºÐ°Ð¿ (Ð²Ð¾ÑÐºÑ€ÐµÑÐµÐ½ÑŒÐµ)
        if now.weekday() == 6:  # Ð’Ð¾ÑÐºÑ€ÐµÑÐµÐ½ÑŒÐµ
            weekly_backup = self.create_backup('weekly')
            if not weekly_backup:
                logger.error("ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ ÐµÐ¶ÐµÐ½ÐµÐ´ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ð±ÑÐºÐ°Ð¿Ð°")
        
        # Ð•Ð¶ÐµÐ¼ÐµÑÑÑ‡Ð½Ñ‹Ð¹ Ð±ÑÐºÐ°Ð¿ (Ð¿ÐµÑ€Ð²Ð¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾ Ð¼ÐµÑÑÑ†Ð°)
        if now.day == 1:
            monthly_backup = self.create_backup('monthly')
            if not monthly_backup:
                logger.error("ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ ÐµÐ¶ÐµÐ¼ÐµÑÑÑ‡Ð½Ð¾Ð³Ð¾ Ð±ÑÐºÐ°Ð¿Ð°")
        
        return True

# Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑ€
db_backup = DatabaseBackup()