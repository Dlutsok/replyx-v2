"""
–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å embeddings –∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º –∑–Ω–∞–Ω–∏–π
–†–µ–∞–ª–∏–∑—É–µ—Ç lazy-reload –ø–æ–¥—Ö–æ–¥ –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—Ö–æ–¥–∞ —Ç–æ–∫–µ–Ω–æ–≤
"""

import hashlib
import json
import logging
import numpy as np
from typing import List, Dict, Tuple, Optional
from sqlalchemy.orm import Session
from sqlalchemy import text, desc
from datetime import datetime, timedelta

from database import models
from sqlalchemy import text
import importlib
Vector = None
try:
    _pgv = importlib.import_module('pgvector.sqlalchemy')  # type: ignore
    Vector = getattr(_pgv, 'Vector', None)
except Exception:
    Vector = None
from ai.ai_token_manager import ai_token_manager

logger = logging.getLogger(__name__)

class EmbeddingsService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–∏—Å–∫–∞ embeddings"""
    
    def __init__(self):
        self.embedding_model = "text-embedding-3-small"  # –ë–æ–ª–µ–µ –¥–µ—à–µ–≤–∞—è –º–æ–¥–µ–ª—å
        self.embedding_dimension = 1536
        self.max_chunk_tokens = 500  # –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∞–Ω–∫–µ
        self.max_total_context_tokens = 2000  # –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–∞
        # –ï–¥–∏–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –º–æ–∂–Ω–æ –≤—ã–Ω–µ—Å—Ç–∏ –≤ –∫–æ–Ω—Ñ–∏–≥, –ø–æ–∫–∞ –¥–µ—Ä–∂–∏–º –∑–¥–µ—Å—å
    def build_context_messages(self, chunks: List[Dict], max_context_tokens: int) -> Tuple[List[str], int]:
        """–ï–¥–∏–Ω–∞—è —É–ø–∞–∫–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞, MMR —É–∂–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ –Ω–∞ –≤—Ö–æ–¥–µ, –∞–∫–∫—É—Ä–∞—Ç–Ω–∞—è —Ä–µ–∑–∫–∞ –ø–æ –±—é–¥–∂–µ—Ç—É."""
        context_parts: List[str] = []
        total_tokens = 0
        for chunk in chunks:
            chunk_tokens = chunk.get('token_count', self.estimate_tokens(chunk['text']))
            if total_tokens + chunk_tokens > max_context_tokens:
                break
            context_parts.append(chunk['text'])
            total_tokens += chunk_tokens
        return context_parts, total_tokens
        
    def generate_embedding(self, text: str, user_id: int) -> List[float]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç embedding –¥–ª—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ OpenAI API"""
        try:
            response = ai_token_manager.make_openai_request(
                messages=[],  # –î–ª—è embeddings messages –Ω–µ –Ω—É–∂–Ω—ã
                model=self.embedding_model,
                user_id=user_id,
                is_embedding=True,
                input_text=text
            )
            
            # OpenAI –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç embedding –≤ response.data[0].embedding
            if hasattr(response, 'data') and len(response.data) > 0:
                return response.data[0].embedding
            else:
                logger.error(f"Unexpected embedding response format: {response}")
                return None
                
        except Exception as e:
            logger.error(f"Error generating embedding: {e}")
            return None
    
    def get_cached_query_embedding(self, query: str, db: Session) -> Optional[List[float]]:
        """–ü–æ–ª—É—á–∞–µ—Ç embedding –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ –∫—ç—à–∞ –∏–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–π"""
        query_hash = hashlib.md5(query.encode()).hexdigest()
        
        # –ò—â–µ–º –≤ –∫—ç—à–µ
        cached = db.query(models.QueryEmbeddingCache).filter(
            models.QueryEmbeddingCache.query_hash == query_hash
        ).first()
        
        if cached:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            cached.last_used = datetime.utcnow()
            cached.usage_count += 1
            db.commit()
            logger.debug(f"Cache hit for query: {query[:50]}...")
            try:
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ —á–∏—Å–µ–ª –¥–ª—è –æ–±–µ–∏—Ö —Å—Ö–µ–º (pgvector/ARRAY(Float))
                return list(cached.embedding)
            except Exception:
                return None
        
        return None
    
    def cache_query_embedding(self, query: str, embedding: List[float], db: Session):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç embedding –∑–∞–ø—Ä–æ—Å–∞ –≤ –∫—ç—à"""
        query_hash = hashlib.md5(query.encode()).hexdigest()
        
        try:
            cache_entry = models.QueryEmbeddingCache(
                query_hash=query_hash,
                query_text=query,
                embedding=embedding,
                created_at=datetime.utcnow(),
                last_used=datetime.utcnow(),
                usage_count=1
            )
            db.add(cache_entry)
            db.commit()
            logger.debug(f"Cached embedding for query: {query[:50]}...")
        except Exception as e:
            logger.error(f"Error caching query embedding: {e}")
            db.rollback()
    
    def split_text_into_chunks(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if len(text) <= chunk_size:
            return [text]
        
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        chunks: List[str] = []
        current: List[str] = []
        current_len = 0
        for p in paragraphs:
            if current_len + len(p) <= chunk_size:
                current.append(p)
                current_len += len(p) + 2
            else:
                if current:
                    chunks.append('\n\n'.join(current))
                # –µ—Å–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ‚Äî —Ä–µ–∂–µ–º –µ–≥–æ –≥—Ä—É–±–æ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
                if len(p) > chunk_size:
                    sentences = p.split('. ')
                    buf: List[str] = []
                    buf_len = 0
                    for s in sentences:
                        if buf_len + len(s) <= chunk_size:
                            buf.append(s)
                            buf_len += len(s) + 2
                        else:
                            if buf:
                                chunks.append('. '.join(buf).strip())
                            buf = [s]
                            buf_len = len(s)
                    if buf:
                        chunks.append('. '.join(buf).strip())
                    current = []
                    current_len = 0
                else:
                    current = [p]
                    current_len = len(p)
        if current:
            chunks.append('\n\n'.join(current))
        # –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –ø—Ä–æ—Å—Ç–æ–µ: –¥–æ–±–∞–≤–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∞–±–∑–∞—Ü –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —á–∞–Ω–∫–∞ –≤ –Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ
        if overlap > 0 and len(chunks) > 1:
            overlapped: List[str] = []
            prev_tail = ''
            for ch in chunks:
                if prev_tail:
                    merged = (prev_tail + '\n\n' + ch)
                    overlapped.append(merged[:chunk_size])
                else:
                    overlapped.append(ch[:chunk_size])
                # prev_tail = –ø–æ—Å–ª–µ–¥–Ω–∏–π –∞–±–∑–∞—Ü —Ç–µ–∫—É—â–µ–≥–æ —á–∞–Ω–∫–∞
                parts = ch.split('\n\n')
                prev_tail = parts[-1][:overlap] if parts else ''
            chunks = overlapped
        return chunks
    
    def estimate_tokens(self, text: str) -> int:
        """–¢–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è gpt-4o-mini —á–µ—Ä–µ–∑ tiktoken; fallback –Ω–∞ 4 —Å–∏–º–≤–æ–ª–∞ = 1 —Ç–æ–∫–µ–Ω."""
        try:
            import tiktoken  # type: ignore
            enc = tiktoken.encoding_for_model("gpt-4o-mini")
            return len(enc.encode(text))
        except Exception:
            return len(text) // 4

    def _tokenize_for_similarity(self, text: str) -> set:
        words = [w.lower() for w in text.split() if len(w) > 2]
        return set(words)

    def _jaccard_similarity(self, a: set, b: set) -> float:
        if not a or not b:
            return 0.0
        inter = len(a & b)
        union = len(a | b)
        return inter / union if union else 0.0

    def _select_diverse_chunks(self, chunks: List[Dict], k: int, max_jaccard: float = 0.7) -> List[Dict]:
        """–ì—Ä—É–±–∞—è MMR/–¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –æ—Ç–±–∏—Ä–∞–µ–º —á–∞–Ω–∫–∏, –∏–∑–±–µ–≥–∞—è —Å–∏–ª—å–Ω—ã—Ö –ø–æ–≤—Ç–æ—Ä–æ–≤ —Ç–µ–∫—Å—Ç–∞."""
        selected: List[Dict] = []
        selected_tokens: List[set] = []
        for ch in chunks:
            ch_tokens = self._tokenize_for_similarity(ch['text'])
            is_similar = any(self._jaccard_similarity(ch_tokens, st) > max_jaccard for st in selected_tokens)
            if is_similar:
                continue
            selected.append(ch)
            selected_tokens.append(ch_tokens)
            if len(selected) >= k:
                break
        return selected
    
    def compute_chunk_hash(self, text: str) -> str:
        return hashlib.md5(text.encode('utf-8')).hexdigest()

    def upsert_embedding_chunk(self, *,
                               user_id: int,
                               assistant_id: Optional[int],
                               doc_id: int,
                               chunk_index: int,
                               chunk_text: str,
                               embedding: List[float],
                               doc_type: str,
                               importance: int,
                               token_count: int,
                               source: str,
                               db: Session,
                               qa_id: Optional[int] = None) -> bool:
        """–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è: –≤—Å—Ç–∞–≤–∏—Ç—å/–æ–±–Ω–æ–≤–∏—Ç—å —á–∞–Ω–∫ –ø–æ (doc_id, chunk_hash)."""
        try:
            chunk_hash = self.compute_chunk_hash(chunk_text)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏, –≤—ã–±–∏—Ä–∞—è —Ç–æ–ª—å–∫–æ id (–∏–∑–±–µ–≥–∞–µ–º —á—Ç–µ–Ω–∏—è embedding-–∫–æ–ª–æ–Ω–∫–∏)
            if qa_id:
                existing_id_row = db.query(models.KnowledgeEmbedding.id).filter(
                    models.KnowledgeEmbedding.qa_id == qa_id,
                    models.KnowledgeEmbedding.chunk_hash == chunk_hash,
                ).first()
            else:
                existing_id_row = db.query(models.KnowledgeEmbedding.id).filter(
                    models.KnowledgeEmbedding.doc_id == doc_id,
                    models.KnowledgeEmbedding.chunk_hash == chunk_hash,
                ).first()
            if existing_id_row:
                # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å bulk-–æ–ø–µ—Ä–∞—Ü–∏–µ–π –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –æ–±—ä–µ–∫—Ç–∞
                if qa_id:
                    filter_query = db.query(models.KnowledgeEmbedding).filter(
                        models.KnowledgeEmbedding.qa_id == qa_id,
                        models.KnowledgeEmbedding.chunk_hash == chunk_hash,
                    )
                else:
                    filter_query = db.query(models.KnowledgeEmbedding).filter(
                        models.KnowledgeEmbedding.doc_id == doc_id,
                        models.KnowledgeEmbedding.chunk_hash == chunk_hash,
                    )
                filter_query.update({
                    models.KnowledgeEmbedding.chunk_index: chunk_index,
                    models.KnowledgeEmbedding.chunk_text: chunk_text,
                    models.KnowledgeEmbedding.embedding: embedding,
                    models.KnowledgeEmbedding.doc_type: doc_type,
                    models.KnowledgeEmbedding.importance: importance,
                    models.KnowledgeEmbedding.token_count: token_count,
                    models.KnowledgeEmbedding.updated_at: datetime.utcnow(),
                    models.KnowledgeEmbedding.source: source,
                }, synchronize_session=False)
                return True
            # –í—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
            knowledge_embedding = models.KnowledgeEmbedding(
                user_id=user_id,
                assistant_id=assistant_id,
                doc_id=doc_id if not qa_id else None,
                qa_id=qa_id,
                chunk_index=chunk_index,
                chunk_text=chunk_text,
                embedding=embedding,
                doc_type=doc_type,
                importance=importance,
                token_count=token_count,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow(),
                chunk_hash=chunk_hash,
                source=source,
            )
            db.add(knowledge_embedding)
            return True
        except Exception as e:
            logger.error(f"Upsert chunk failed: {e}")
            return False

    def index_document(self, doc_id: int, user_id: int, assistant_id: Optional[int],
                      text: str, doc_type: str, importance: int = 10, db: Session = None) -> int:
        """–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç, —Å–æ–∑–¥–∞–≤–∞—è embeddings –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤"""
        logger.info(f"üìÑ [DOCUMENT_INDEXING] Starting: doc_id={doc_id}, user_id={user_id}, assistant_id={assistant_id}, text_length={len(text)}")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
        chunks = self.split_text_into_chunks(text, chunk_size=800, overlap=50)
        logger.info(f"üìÑ [DOCUMENT_INDEXING] Document split into {len(chunks)} chunks")
        
        indexed_count = 0
        
        for i, chunk in enumerate(chunks):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —á–∞–Ω–∫–∏
            if len(chunk.strip()) < 50:
                continue
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding –¥–ª—è —á–∞–Ω–∫–∞
            embedding = self.generate_embedding(chunk, user_id)
            if not embedding:
                logger.warning(f"Failed to generate embedding for chunk {i}")
                continue
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            token_count = self.estimate_tokens(chunk)
            
            # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
            if self.upsert_embedding_chunk(
                user_id=user_id,
                assistant_id=assistant_id,
                doc_id=doc_id,
                chunk_index=i,
                chunk_text=chunk,
                embedding=embedding,
                doc_type=doc_type,
                importance=importance,
                token_count=token_count,
                source='document',
                db=db,
            ):
                indexed_count += 1
                logger.debug(f"üìÑ [DOCUMENT_INDEXING] Successfully indexed chunk {i}/{len(chunks)}: {chunk[:50]}... (assistant_id={assistant_id})")
            else:
                logger.warning(f"üìÑ [DOCUMENT_INDEXING] Failed to index chunk {i}/{len(chunks)}")
        
        # –ö–æ–º–º–∏—Ç–∏–º –≤—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        try:
            db.commit()
            logger.info(f"üìÑ [DOCUMENT_INDEXING] ‚úÖ Successfully indexed {indexed_count} chunks for document {doc_id} (assistant_id={assistant_id})")
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Ä—Å–∏—é –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
            if assistant_id:
                self.increment_knowledge_version(assistant_id, db)
            else:
                # –ï—Å–ª–∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –Ω–µ —É–∫–∞–∑–∞–Ω, –æ–±–Ω–æ–≤–ª—è–µ–º –¥–ª—è –≤—Å–µ—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                assistants = db.query(models.Assistant).filter(
                    models.Assistant.user_id == user_id
                ).all()
                for assistant in assistants:
                    self.increment_knowledge_version(assistant.id, db)
            
            return indexed_count
            
        except Exception as e:
            logger.error(f"Error committing embeddings: {e}")
            db.rollback()
            return 0
    
    def increment_knowledge_version(self, assistant_id: int, db: Session):
        """–£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –≤–µ—Ä—Å–∏—é –∑–Ω–∞–Ω–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –¥–ª—è lazy reload"""
        try:
            assistant = db.query(models.Assistant).filter(
                models.Assistant.id == assistant_id
            ).first()
            
            if assistant:
                assistant.knowledge_version = (assistant.knowledge_version or 0) + 1
                db.commit()
                logger.debug(f"Incremented knowledge version for assistant {assistant_id} to {assistant.knowledge_version}")
        except Exception as e:
            logger.error(f"Error incrementing knowledge version: {e}")
            db.rollback()
    
    def search_relevant_chunks(self, query: str, user_id: int, assistant_id: Optional[int],
                              top_k: int = 5, min_similarity: float = 0.7, db: Session = None,
                              include_qa: bool = False, qa_limit: int = 2) -> List[Dict]:
        """–ò—â–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –∑–Ω–∞–Ω–∏–π –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""

        try:
            logger.info(f"üîç [EMBEDDINGS_SEARCH] Starting search for user_id={user_id}, assistant_id={assistant_id}, query='{query[:50]}...'")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ embeddings –≤ –±–∞–∑–µ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            total_embeddings = db.query(models.KnowledgeEmbedding).filter(
                models.KnowledgeEmbedding.user_id == user_id,
                models.KnowledgeEmbedding.qa_id.is_(None)
            ).count()

            if assistant_id:
                assistant_embeddings = db.query(models.KnowledgeEmbedding).filter(
                    models.KnowledgeEmbedding.user_id == user_id,
                    models.KnowledgeEmbedding.assistant_id == assistant_id,
                    models.KnowledgeEmbedding.qa_id.is_(None)
                ).count()
                logger.info(f"üìä [EMBEDDINGS_SEARCH] Total embeddings for user: {total_embeddings}, for assistant {assistant_id}: {assistant_embeddings}")
            else:
                logger.info(f"üìä [EMBEDDINGS_SEARCH] Total embeddings for user: {total_embeddings}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ Q&A embeddings –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω –ø–æ–∏—Å–∫ –ø–æ Q&A
            if include_qa:
                qa_embeddings = db.query(models.KnowledgeEmbedding).filter(
                    models.KnowledgeEmbedding.user_id == user_id,
                    models.KnowledgeEmbedding.qa_id.isnot(None)
                ).count()
                logger.info(f"üìä [EMBEDDINGS_SEARCH] Q&A embeddings for user: {qa_embeddings}")
            # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –∫—ç—à–∞ —Ç–æ–ø-K —á–∞–Ω–∫–æ–≤ –ø–æ (query_hash, assistant, knowledge_version)
            from cache.redis_cache import chatai_cache
            query_hash = hashlib.md5(query.encode()).hexdigest()
            knowledge_version = 0
            try:
                if assistant_id:
                    assistant = db.query(models.Assistant).filter(models.Assistant.id == assistant_id).first()
                    if assistant:
                        knowledge_version = assistant.knowledge_version or 0
                cached = chatai_cache.get_retrieved_chunks(user_id, assistant_id or 0, knowledge_version, query_hash)
                if cached:
                    return cached
            except Exception:
                pass

            # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.get_cached_query_embedding(query, db)
            if not query_embedding:
                query_embedding = self.generate_embedding(query, user_id)
                if not query_embedding:
                    logger.error("Failed to generate query embedding")
                    return []
                
                # –ö—ç—à–∏—Ä—É–µ–º embedding –∑–∞–ø—Ä–æ—Å–∞
                self.cache_query_embedding(query, query_embedding, db)
            
            # –ï—Å–ª–∏ pgvector –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –º–∏–≥—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º SQL –ø–æ–∏—Å–∫ –ø–æ –≤–µ–∫—Ç–æ—Ä—É
            if Vector:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π SQL —á–µ—Ä–µ–∑ psycopg2 –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å pgvector (–¥–æ–∫—É–º–µ–Ω—Ç—ã)
                import psycopg2
                from core.app_config import DATABASE_URL

                try:
                    # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å postgresql+psycopg2:// –¥–ª—è psycopg2
                    psycopg2_url = DATABASE_URL.replace('postgresql+psycopg2://', 'postgresql://')
                    conn = psycopg2.connect(psycopg2_url)
                    cursor = conn.cursor()

                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º embedding –≤ —Å—Ç—Ä–æ–∫—É pgvector —Ñ–æ—Ä–º–∞—Ç–∞
                    embedding_str = "[" + ",".join(map(str, query_embedding)) + "]"

                    # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å assistant_id, –∑–∞—Ç–µ–º –±–µ–∑ –Ω–µ–≥–æ (fallback)
                    rows = []

                    if assistant_id:
                        # –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å: —Å—Ç—Ä–æ–≥–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ assistant_id
                        sql1 = """
                            SELECT id, doc_id, chunk_text, doc_type, importance, token_count,
                                   1 - (embedding <=> %s::vector) AS similarity
                            FROM knowledge_embeddings
                            WHERE user_id = %s
                            AND qa_id IS NULL
                            AND assistant_id = %s
                            ORDER BY embedding <=> %s::vector
                            LIMIT %s
                        """

                        params1 = [embedding_str, user_id, assistant_id, embedding_str, top_k * 5]
                        cursor.execute(sql1, params1)
                        rows = cursor.fetchall()

                        logger.info(f"Found {len(rows)} chunks with assistant_id={assistant_id}")

                        # –õ–æ–≥–∏—Ä—É–µ–º similarity –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö chunks –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                        for i, row in enumerate(rows[:3]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3
                            similarity = float(row[6]) if row[6] is not None else 0.0
                            logger.info(f"  Chunk {i+1}: similarity={similarity:.4f}, text='{row[2][:50]}...'")

                        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º assistant_id, –ø–æ–ø—Ä–æ–±—É–µ–º –æ–±—â–∏–µ (assistant_id IS NULL)
                        if not rows:
                            sql2 = """
                                SELECT id, doc_id, chunk_text, doc_type, importance, token_count,
                                       1 - (embedding <=> %s::vector) AS similarity
                                FROM knowledge_embeddings
                                WHERE user_id = %s
                                AND qa_id IS NULL
                                AND assistant_id IS NULL
                                ORDER BY embedding <=> %s::vector
                                LIMIT %s
                            """

                            params2 = [embedding_str, user_id, embedding_str, top_k * 5]
                            cursor.execute(sql2, params2)
                            rows = cursor.fetchall()

                            logger.info(f"Fallback: Found {len(rows)} chunks with assistant_id IS NULL")
                    else:
                        # –ë–µ–∑ assistant_id - –∏—â–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                        sql = """
                            SELECT id, doc_id, chunk_text, doc_type, importance, token_count,
                                   1 - (embedding <=> %s::vector) AS similarity
                            FROM knowledge_embeddings
                            WHERE user_id = %s
                            AND qa_id IS NULL
                            ORDER BY embedding <=> %s::vector
                            LIMIT %s
                        """

                        params = [embedding_str, user_id, embedding_str, top_k * 5]
                        cursor.execute(sql, params)
                        rows = cursor.fetchall()

                        logger.info(f"Found {len(rows)} chunks for user {user_id} (no assistant filter)")

                    cursor.close()
                    conn.close()
                    
                except Exception as e:
                    logger.error(f"Error in direct SQL chunks search: {e}")
                    rows = []
                relevant_chunks = []
                for row in rows:
                    similarity = float(row[6]) if row[6] is not None else 0.0

                    # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ similarity
                    logger.debug(f"üîç [EMBEDDINGS_SEARCH] Chunk similarity: {similarity:.4f}, min_required: {min_similarity}, text: {row[2][:50]}...")

                    if row[6] is not None and row[6] < min_similarity:
                        logger.debug(f"üîç [EMBEDDINGS_SEARCH] ‚ùå Chunk filtered out by min_similarity: {similarity:.4f} < {min_similarity}")
                        continue

                    chunk_tokens = row[5] or self.estimate_tokens(row[2])
                    relevant_chunks.append({
                        'id': row[0],
                        'doc_id': row[1],
                        'text': row[2],
                        'doc_type': row[3],
                        'importance': row[4],
                        'similarity': similarity,
                        'token_count': chunk_tokens,
                    })
                    logger.debug(f"üîç [EMBEDDINGS_SEARCH] ‚úÖ Chunk accepted: {similarity:.4f}")
            else:
                # Fallback: –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å (–º–µ–¥–ª–µ–Ω–Ω–æ)
                query_filter = db.query(models.KnowledgeEmbedding).filter(
                    models.KnowledgeEmbedding.user_id == user_id
                )
                if assistant_id:
                    query_filter = query_filter.filter(
                        (models.KnowledgeEmbedding.assistant_id == assistant_id) |
                        (models.KnowledgeEmbedding.assistant_id.is_(None))
                    )
                embeddings_data = query_filter.all()
                if not embeddings_data:
                    logger.info("No embeddings found for user/assistant")
                    return []

                relevant_chunks = []
                query_vector = np.array(query_embedding)
                for embedding_row in embeddings_data:
                    try:
                        stored_vector = np.array(list(embedding_row.embedding))
                        similarity = np.dot(query_vector, stored_vector) / (
                            np.linalg.norm(query_vector) * np.linalg.norm(stored_vector)
                        )
                        if similarity >= min_similarity:
                            chunk_tokens = embedding_row.token_count or self.estimate_tokens(embedding_row.chunk_text)
                            relevant_chunks.append({
                                'id': embedding_row.id,
                                'text': embedding_row.chunk_text,
                                'doc_type': embedding_row.doc_type,
                                'importance': embedding_row.importance,
                                'similarity': float(similarity),
                                'token_count': chunk_tokens
                            })
                    except Exception as e:
                        logger.warning(f"Error processing embedding {embedding_row.id}: {e}")
                        continue
            
            # –î–∏–≤–µ—Ä—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ (MMR-–≥—Ä—É–±–æ –ø–æ Jaccard), –∑–∞—Ç–µ–º –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏ —Ç–æ–∫–µ–Ω—ã
            diversified = self._select_diverse_chunks(
                sorted(relevant_chunks, key=lambda x: x['similarity'], reverse=True),
                k=top_k,
                max_jaccard=0.7,
            )

            final_chunks = []
            total_tokens = 0
            
            for chunk in diversified[:top_k]:
                if total_tokens + chunk['token_count'] > self.max_total_context_tokens:
                    break
                final_chunks.append(chunk)
                total_tokens += chunk['token_count']
            
            logger.info(f"üîç [EMBEDDINGS_SEARCH] ‚úÖ Found {len(final_chunks)} relevant chunks (total tokens: {total_tokens})")

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π fallback: –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ chunks —Å assistant_id, –ø–æ–ø—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            if not final_chunks and assistant_id:
                logger.info(f"üîç [EMBEDDINGS_SEARCH] üîÑ No chunks found with assistant_id={assistant_id}, trying global fallback...")

                try:
                    # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ö–æ—Ç—å —á—Ç–æ-—Ç–æ —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ assistant_id
                    if Vector:
                        import psycopg2
                        from core.app_config import DATABASE_URL

                        psycopg2_url = DATABASE_URL.replace('postgresql+psycopg2://', 'postgresql://')
                        conn = psycopg2.connect(psycopg2_url)
                        cursor = conn.cursor()

                        embedding_str = "[" + ",".join(map(str, query_embedding)) + "]"

                        sql_fallback = """
                            SELECT id, doc_id, chunk_text, doc_type, importance, token_count,
                                   1 - (embedding <=> %s::vector) AS similarity
                            FROM knowledge_embeddings
                            WHERE user_id = %s
                            AND qa_id IS NULL
                            ORDER BY embedding <=> %s::vector
                            LIMIT %s
                        """

                        params_fallback = [embedding_str, user_id, embedding_str, top_k * 3]
                        cursor.execute(sql_fallback, params_fallback)
                        fallback_rows = cursor.fetchall()

                        cursor.close()
                        conn.close()

                        logger.info(f"üîç [EMBEDDINGS_SEARCH] Global fallback found {len(fallback_rows)} chunks")

                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã fallback
                        fallback_chunks = []
                        for row in fallback_rows:
                            if row[6] is not None and row[6] >= min_similarity:
                                chunk_tokens = row[5] or self.estimate_tokens(row[2])
                                fallback_chunks.append({
                                    'id': row[0],
                                    'doc_id': row[1],
                                    'text': row[2],
                                    'doc_type': row[3],
                                    'importance': row[4],
                                    'similarity': float(row[6]),
                                    'token_count': chunk_tokens,
                                })

                        # –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
                        if fallback_chunks:
                            diversified_fallback = self._select_diverse_chunks(
                                sorted(fallback_chunks, key=lambda x: x['similarity'], reverse=True),
                                k=top_k,
                                max_jaccard=0.7,
                            )

                            final_chunks = []
                            total_tokens = 0

                            for chunk in diversified_fallback[:top_k]:
                                if total_tokens + chunk['token_count'] > self.max_total_context_tokens:
                                    break
                                final_chunks.append(chunk)
                                total_tokens += chunk['token_count']

                            logger.info(f"üîç [EMBEDDINGS_SEARCH] ‚úÖ Global fallback returned {len(final_chunks)} chunks (total tokens: {total_tokens})")

                except Exception as e:
                    logger.warning(f"üîç [EMBEDDINGS_SEARCH] Global fallback failed: {e}")

            # –û–±–Ω–æ–≤–ª—è–µ–º usage_count/last_used –¥–ª—è –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π (–µ—Å–ª–∏ –µ—Å—Ç—å UserKnowledge –ø–æ doc_id)
            try:
                if final_chunks:
                    doc_ids = list({c.get('doc_id') for c in final_chunks if c.get('doc_id') is not None})
                    if doc_ids:
                        db.query(models.UserKnowledge).filter(
                            models.UserKnowledge.user_id == user_id,
                            models.UserKnowledge.doc_id.in_(doc_ids)
                        ).update({
                            models.UserKnowledge.last_used: datetime.utcnow()
                        }, synchronize_session=False)
                        # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç usage_count –ø–æ –∫–∞–∂–¥–æ–º—É doc_id
                        for did in doc_ids:
                            db.query(models.UserKnowledge).filter(
                                models.UserKnowledge.user_id == user_id,
                                models.UserKnowledge.doc_id == did
                            ).update({
                                models.UserKnowledge.usage_count: models.UserKnowledge.usage_count + 1
                            }, synchronize_session=False)
                        db.commit()
            except Exception as _e:
                logger.debug(f"Failed to update usage counters: {_e}")

            # –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º Q&A —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
            if include_qa and qa_limit > 0:
                qa_results = self.search_relevant_qa(
                    query=query,
                    user_id=user_id,
                    assistant_id=assistant_id,
                    top_k=qa_limit,
                    min_similarity=min_similarity,
                    db=db
                )
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º Q&A —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç chunks –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                for qa in qa_results:
                    qa_chunk = {
                        'id': f"qa_{qa['id']}",
                        'text': f"Q: {qa['question']}\nA: {qa['answer']}",
                        'doc_type': 'qa_knowledge',
                        'importance': qa['importance'],
                        'similarity': qa.get('max_similarity', qa.get('similarity', 0)),
                        'token_count': self.estimate_tokens(f"Q: {qa['question']}\nA: {qa['answer']}"),
                        'type': 'qa_knowledge',
                        'category': qa.get('category', None)
                    }
                    
                    # –í—Å—Ç–∞–≤–ª—è–µ–º Q&A —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –º–µ—Å—Ç–æ –ø–æ similarity
                    inserted = False
                    for i, chunk in enumerate(final_chunks):
                        if qa_chunk['similarity'] > chunk['similarity']:
                            final_chunks.insert(i, qa_chunk)
                            inserted = True
                            break
                    
                    if not inserted:
                        final_chunks.append(qa_chunk)
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                final_chunks = final_chunks[:top_k + qa_limit]
                
                # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–∏–µ —Ç–æ–∫–µ–Ω—ã —Å —É—á–µ—Ç–æ–º Q&A
                recalc_total_tokens = 0
                filtered_chunks = []
                for chunk in final_chunks:
                    if recalc_total_tokens + chunk['token_count'] > self.max_total_context_tokens:
                        break
                    filtered_chunks.append(chunk)
                    recalc_total_tokens += chunk['token_count']
                
                final_chunks = filtered_chunks

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–µ—Ç—Ä–∏–≤–∞ –≤ –∫—ç—à –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–π TTL
            try:
                chatai_cache.cache_retrieved_chunks(user_id, assistant_id or 0, knowledge_version, query_hash, final_chunks, ttl=60)
            except Exception:
                pass

            return final_chunks
            
        except Exception as e:
            logger.error(f"Error searching relevant chunks: {e}")
            return []
    
    def cleanup_old_cache(self, db: Session, days_old: int = 30):
        """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–π –∫—ç—à embeddings –∑–∞–ø—Ä–æ—Å–æ–≤"""
        try:
            cutoff_date = datetime.utcnow() - timedelta(days=days_old)
            
            deleted = db.query(models.QueryEmbeddingCache).filter(
                models.QueryEmbeddingCache.last_used < cutoff_date
            ).delete()
            
            db.commit()
            logger.info(f"Cleaned up {deleted} old cached embeddings")
            
        except Exception as e:
            logger.error(f"Error cleaning up cache: {e}")
            db.rollback()
    
    def delete_document_embeddings(self, doc_id: int, db: Session):
        """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ embeddings –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            deleted = db.query(models.KnowledgeEmbedding).filter(
                models.KnowledgeEmbedding.doc_id == doc_id
            ).delete()
            
            db.commit()
            logger.info(f"Deleted {deleted} embeddings for document {doc_id}")
            return deleted
            
        except Exception as e:
            logger.error(f"Error deleting document embeddings: {e}")
            db.rollback()
            return 0

    def index_qa_knowledge(self, qa_id: int, user_id: int, assistant_id: Optional[int],
                          question: str, answer: str, importance: int = 10, db: Session = None) -> int:
        """–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç Q&A –∑–∞–ø–∏—Å—å, —Å–æ–∑–¥–∞–≤–∞—è embeddings –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ –∏ –æ—Ç–≤–µ—Ç–∞"""
        logger.info(f"Starting Q&A indexing: qa_id={qa_id}, user_id={user_id}")
        
        indexed_count = 0
        
        # –°–æ–∑–¥–∞–µ–º embedding –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞
        question_embedding = self.generate_embedding(question, user_id)
        if question_embedding:
            question_tokens = self.estimate_tokens(question)
            if self.upsert_embedding_chunk(
                user_id=user_id,
                assistant_id=assistant_id,
                doc_id=0,  # dummy value since doc_id will be None
                chunk_index=0,
                chunk_text=question,
                embedding=question_embedding,
                doc_type='qa_question',
                importance=importance,
                token_count=question_tokens,
                source='qa_knowledge',
                qa_id=qa_id,
                db=db,
            ):
                indexed_count += 1
        
        # –°–æ–∑–¥–∞–µ–º embedding –¥–ª—è –æ—Ç–≤–µ—Ç–∞
        answer_embedding = self.generate_embedding(answer, user_id)
        if answer_embedding:
            answer_tokens = self.estimate_tokens(answer)
            if self.upsert_embedding_chunk(
                user_id=user_id,
                assistant_id=assistant_id,
                doc_id=0,  # dummy value since doc_id will be None
                chunk_index=1,
                chunk_text=answer,
                embedding=answer_embedding,
                doc_type='qa_answer',
                importance=importance,
                token_count=answer_tokens,
                source='qa_knowledge',
                qa_id=qa_id,
                db=db,
            ):
                indexed_count += 1
        
        # –ö–æ–º–º–∏—Ç–∏–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
        try:
            db.commit()
            logger.info(f"Successfully indexed {indexed_count} Q&A embeddings for qa_id {qa_id}")
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Ä—Å–∏—é –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
            if assistant_id:
                self.increment_knowledge_version(assistant_id, db)
            else:
                # –ï—Å–ª–∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –Ω–µ —É–∫–∞–∑–∞–Ω, –æ–±–Ω–æ–≤–ª—è–µ–º –¥–ª—è –≤—Å–µ—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                assistants = db.query(models.Assistant).filter(
                    models.Assistant.user_id == user_id
                ).all()
                for assistant in assistants:
                    self.increment_knowledge_version(assistant.id, db)
            
            return indexed_count
            
        except Exception as e:
            logger.error(f"Error committing Q&A embeddings: {e}")
            db.rollback()
            return 0

    def search_relevant_qa(self, query: str, user_id: int, assistant_id: Optional[int],
                          top_k: int = 3, min_similarity: float = 0.7, db: Session = None) -> List[Dict]:
        """–ò—â–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ Q&A –∑–∞–ø–∏—Å–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.get_cached_query_embedding(query, db)
            if not query_embedding:
                query_embedding = self.generate_embedding(query, user_id)
                if not query_embedding:
                    logger.error("Failed to generate query embedding for Q&A search")
                    return []
                
                # –ö—ç—à–∏—Ä—É–µ–º embedding –∑–∞–ø—Ä–æ—Å–∞
                self.cache_query_embedding(query, query_embedding, db)
            
            # –ü–æ–∏—Å–∫ –ø–æ Q&A embeddings
            if Vector:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π SQL —á–µ—Ä–µ–∑ psycopg2 –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å pgvector
                import psycopg2
                from core.app_config import DATABASE_URL
                
                try:
                    # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å postgresql+psycopg2:// –¥–ª—è psycopg2
                    psycopg2_url = DATABASE_URL.replace('postgresql+psycopg2://', 'postgresql://')
                    conn = psycopg2.connect(psycopg2_url)
                    cursor = conn.cursor()
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º embedding –≤ —Å—Ç—Ä–æ–∫—É pgvector —Ñ–æ—Ä–º–∞—Ç–∞
                    embedding_str = "[" + ",".join(map(str, query_embedding)) + "]"
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ —É–ª—É—á—à–µ–Ω–Ω—É—é –ª–æ–≥–∏–∫—É –∫–∞–∫ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                    rows = []

                    if assistant_id:
                        # –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å: —Å—Ç—Ä–æ–≥–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ assistant_id
                        sql1 = """
                            SELECT DISTINCT qa.id, qa.question, qa.answer, qa.category, qa.importance,
                                   MAX(1 - (ke.embedding <=> %s::vector)) AS max_similarity
                            FROM qa_knowledge qa
                            JOIN knowledge_embeddings ke ON qa.id = ke.qa_id
                            WHERE qa.user_id = %s
                            AND qa.is_active = true
                            AND ke.assistant_id = %s
                            GROUP BY qa.id, qa.question, qa.answer, qa.category, qa.importance
                            ORDER BY max_similarity DESC
                            LIMIT %s
                        """

                        params1 = [embedding_str, user_id, assistant_id, top_k * 2]
                        cursor.execute(sql1, params1)
                        rows = cursor.fetchall()

                        logger.info(f"Found {len(rows)} Q&A with assistant_id={assistant_id}")

                        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º assistant_id, –ø–æ–ø—Ä–æ–±—É–µ–º –æ–±—â–∏–µ (assistant_id IS NULL)
                        if not rows:
                            sql2 = """
                                SELECT DISTINCT qa.id, qa.question, qa.answer, qa.category, qa.importance,
                                       MAX(1 - (ke.embedding <=> %s::vector)) AS max_similarity
                                FROM qa_knowledge qa
                                JOIN knowledge_embeddings ke ON qa.id = ke.qa_id
                                WHERE qa.user_id = %s
                                AND qa.is_active = true
                                AND ke.assistant_id IS NULL
                                GROUP BY qa.id, qa.question, qa.answer, qa.category, qa.importance
                                ORDER BY max_similarity DESC
                                LIMIT %s
                            """

                            params2 = [embedding_str, user_id, top_k * 2]
                            cursor.execute(sql2, params2)
                            rows = cursor.fetchall()

                            logger.info(f"Q&A Fallback: Found {len(rows)} Q&A with assistant_id IS NULL")
                    else:
                        # –ë–µ–∑ assistant_id - –∏—â–µ–º –≤—Å–µ Q&A –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                        sql = """
                            SELECT DISTINCT qa.id, qa.question, qa.answer, qa.category, qa.importance,
                                   MAX(1 - (ke.embedding <=> %s::vector)) AS max_similarity
                            FROM qa_knowledge qa
                            JOIN knowledge_embeddings ke ON qa.id = ke.qa_id
                            WHERE qa.user_id = %s
                            AND qa.is_active = true
                            GROUP BY qa.id, qa.question, qa.answer, qa.category, qa.importance
                            ORDER BY max_similarity DESC
                            LIMIT %s
                        """

                        params = [embedding_str, user_id, top_k * 2]
                        cursor.execute(sql, params)
                        rows = cursor.fetchall()

                        logger.info(f"Found {len(rows)} Q&A for user {user_id} (no assistant filter)")
                    
                    cursor.close()
                    conn.close()
                    
                except Exception as e:
                    logger.error(f"Error in direct SQL Q&A search: {e}")
                    # Fallback to numpy search
                    rows = []
                
                relevant_qa = []
                for row in rows:
                    if row[5] is not None and row[5] >= min_similarity:
                        relevant_qa.append({
                            'id': row[0],
                            'question': row[1],
                            'answer': row[2],
                            'category': row[3],
                            'importance': row[4],
                            'max_similarity': float(row[5]),
                            'type': 'qa_knowledge'
                        })
                
                return relevant_qa[:top_k]
                
            else:
                # Fallback –±–µ–∑ pgvector
                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ Q&A –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è/–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
                qa_query = db.query(models.QAKnowledge).filter(
                    models.QAKnowledge.user_id == user_id,
                    models.QAKnowledge.is_active == True
                )
                if assistant_id:
                    qa_query = qa_query.filter(
                        (models.QAKnowledge.assistant_id == assistant_id) |
                        (models.QAKnowledge.assistant_id.is_(None))
                    )
                
                qa_records = qa_query.all()
                if not qa_records:
                    logger.info("No Q&A records found for user/assistant")
                    return []
                
                # –ü–æ–ª—É—á–∞–µ–º embeddings –¥–ª—è —ç—Ç–∏—Ö Q&A
                qa_ids = [qa.id for qa in qa_records]
                embeddings_data = db.query(models.KnowledgeEmbedding).filter(
                    models.KnowledgeEmbedding.qa_id.in_(qa_ids)
                ).all()
                
                if not embeddings_data:
                    logger.info("No Q&A embeddings found")
                    return []
                
                # –í—ã—á–∏—Å–ª—è–µ–º similarity
                relevant_qa = {}
                query_vector = np.array(query_embedding)
                
                for embedding_row in embeddings_data:
                    try:
                        stored_vector = np.array(list(embedding_row.embedding))
                        similarity = np.dot(query_vector, stored_vector) / (
                            np.linalg.norm(query_vector) * np.linalg.norm(stored_vector)
                        )
                        
                        qa_id = embedding_row.qa_id
                        if qa_id not in relevant_qa or similarity > relevant_qa[qa_id]['similarity']:
                            qa_record = next((qa for qa in qa_records if qa.id == qa_id), None)
                            if qa_record and similarity >= min_similarity:
                                relevant_qa[qa_id] = {
                                    'id': qa_record.id,
                                    'question': qa_record.question,
                                    'answer': qa_record.answer,
                                    'category': qa_record.category,
                                    'importance': qa_record.importance,
                                    'similarity': float(similarity),
                                    'type': 'qa_knowledge'
                                }
                    except Exception as e:
                        logger.warning(f"Error processing Q&A embedding {embedding_row.id}: {e}")
                        continue
                
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ similarity –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-K
                sorted_qa = sorted(relevant_qa.values(), key=lambda x: x['similarity'], reverse=True)
                return sorted_qa[:top_k]
                
        except Exception as e:
            logger.error(f"Error searching relevant Q&A: {e}")
            return []

    def search_combined_knowledge(self, query: str, user_id: int, assistant_id: Optional[int],
                                 doc_chunks_limit: int = 3, qa_limit: int = 2, 
                                 min_similarity: float = 0.7, db: Session = None) -> Tuple[List[Dict], List[Dict]]:
        """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –∏ Q&A –∑–Ω–∞–Ω–∏—è–º"""
        try:
            # –ü–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º (—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª)
            doc_chunks = self.search_relevant_chunks(
                query=query,
                user_id=user_id,
                assistant_id=assistant_id,
                top_k=doc_chunks_limit,
                min_similarity=min_similarity,
                db=db
            )
            
            # –ü–æ–∏—Å–∫ –ø–æ Q&A
            qa_results = self.search_relevant_qa(
                query=query,
                user_id=user_id,
                assistant_id=assistant_id,
                top_k=qa_limit,
                min_similarity=min_similarity,
                db=db
            )
            
            logger.info(f"Combined search found {len(doc_chunks)} document chunks and {len(qa_results)} Q&A records")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º usage_count –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö Q&A
            if qa_results:
                try:
                    qa_ids = [qa['id'] for qa in qa_results]
                    db.query(models.QAKnowledge).filter(
                        models.QAKnowledge.id.in_(qa_ids)
                    ).update({
                        models.QAKnowledge.last_used: datetime.utcnow(),
                        models.QAKnowledge.usage_count: models.QAKnowledge.usage_count + 1
                    }, synchronize_session=False)
                    db.commit()
                except Exception as e:
                    logger.debug(f"Failed to update Q&A usage counters: {e}")
            
            return doc_chunks, qa_results
            
        except Exception as e:
            logger.error(f"Error in combined knowledge search: {e}")
            return [], []

    def delete_qa_embeddings(self, qa_id: int, db: Session):
        """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ embeddings –¥–ª—è Q&A –∑–∞–ø–∏—Å–∏"""
        try:
            deleted = db.query(models.KnowledgeEmbedding).filter(
                models.KnowledgeEmbedding.qa_id == qa_id
            ).delete()
            
            db.commit()
            logger.info(f"Deleted {deleted} embeddings for Q&A {qa_id}")
            return deleted
            
        except Exception as e:
            logger.error(f"Error deleting Q&A embeddings: {e}")
            db.rollback()
            return 0


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
embeddings_service = EmbeddingsService()