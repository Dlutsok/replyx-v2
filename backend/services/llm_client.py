"""
LLM –∫–ª–∏–µ–Ω—Ç —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç —Ç—Ä–∞—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ—Å—Ç–∞—Ö
–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º –Ω–∞ —Ñ–µ–π–∫ –≤ —Ç–µ—Å—Ç–∞—Ö
"""

import os
import logging
from typing import Dict, Any, List, Optional, Union
from abc import ABC, abstractmethod

from .ws_config import (
    LLM_PROVIDER, 
    FAKE_LLM_MODE, 
    IS_TEST_ENV, 
    BLOCK_EXTERNAL_IO, 
    ENVIRONMENT
)

logger = logging.getLogger(__name__)


class BaseLLM(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤—Å–µ—Ö LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    
    @abstractmethod
    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> Dict[str, Any]:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ LLM –∏ –ø–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π [{"role": "user", "content": "text"}]
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (dialog_id, temperature, etc.)
            
        Returns:
            {"role": "assistant", "content": "response", "metadata": {...}}
        """
        pass
    
    @property
    @abstractmethod
    def provider_name(self) -> str:
        """–ò–º—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        pass


class FakeLLM(BaseLLM):
    """
    –§–µ–π–∫–æ–≤—ã–π LLM –¥–ª—è —Ç–µ—Å—Ç–æ–≤ - –ù–ï –¢–†–ê–¢–ò–¢ –¢–û–ö–ï–ù–´
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ä–µ–∂–∏–º—ã: echo, stub, script
    """
    
    def __init__(self, mode: str = None):
        self.mode = mode or FAKE_LLM_MODE
        logger.info(f"üîí LLM provider: FAKE/{self.mode} (tokens will NOT be spent)")
    
    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–µ–π–∫–æ–≤—ã–π –æ—Ç–≤–µ—Ç –±–µ–∑ –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ –≤–Ω–µ—à–Ω–∏–º API"""
        
        if self.mode == "echo":
            # –≠—Ö–æ-—Ä–µ–∂–∏–º: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º
            last_message = messages[-1]["content"] if messages else "empty"
            response = f"[FAKE/echo] {last_message}"
            
        elif self.mode == "stub":
            # –ó–∞–≥–ª—É—à–∫–∞: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç
            response = "[FAKE/stub] –ü—Ä–∏–≤–µ—Ç! –≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞."
            
        elif self.mode == "script":
            # –°–∫—Ä–∏–ø—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º: –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∑–∞–ø–∏—Å–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
            dialog_id = kwargs.get("dialog_id")
            response = f"[FAKE/script] –°–∫—Ä–∏–ø—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –¥–∏–∞–ª–æ–≥–∞ {dialog_id}"
            
        else:
            response = f"[FAKE/{self.mode}] Unknown fake mode, defaulting to stub"
            
        return {
            "role": "assistant", 
            "content": response,
            "metadata": {
                "provider": "fake",
                "mode": self.mode,
                "tokens_used": 0,
                "cost": 0.0,
                "is_fake": True
            }
        }
    
    @property
    def provider_name(self) -> str:
        return f"FAKE/{self.mode}"


class OpenAILLM(BaseLLM):
    """–†–µ–∞–ª—å–Ω—ã–π OpenAI –ø—Ä–æ–≤–∞–π–¥–µ—Ä - –¢–†–ê–¢–ò–¢ –¢–û–ö–ï–ù–´ –í PRODUCTION"""
    
    def __init__(self, api_key: str):
        # –ó–∞—â–∏—Ç–∞ –æ—Ç —Å–ª—É—á–∞–π–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Ç–µ—Å—Ç–∞—Ö
        if BLOCK_EXTERNAL_IO:
            raise RuntimeError(
                f"üö´ External IO blocked in environment: {ENVIRONMENT}. "
                "OpenAI calls are prohibited in test/CI environments to prevent token spending."
            )
            
        if not api_key:
            raise ValueError("OpenAI API key is required")
            
        self.api_key = api_key
        logger.info("üí∞ LLM provider: OpenAI (REAL tokens will be spent)")
    
    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> Dict[str, Any]:
        """–†–µ–∞–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ OpenAI API"""
        if BLOCK_EXTERNAL_IO:
            raise RuntimeError("External IO is blocked - OpenAI calls prohibited")
            
        # TODO: Implement real OpenAI API call
        # –ü–æ–∫–∞ –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        logger.warning("‚ö†Ô∏è OpenAI implementation not yet complete - using stub")
        return {
            "role": "assistant",
            "content": "[OpenAI-stub] Real implementation needed",
            "metadata": {
                "provider": "openai",
                "tokens_used": 0,
                "cost": 0.0,
                "is_fake": False
            }
        }
    
    @property
    def provider_name(self) -> str:
        return "OpenAI"


class AnthropicLLM(BaseLLM):
    """–†–µ–∞–ª—å–Ω—ã–π Anthropic –ø—Ä–æ–≤–∞–π–¥–µ—Ä - –¢–†–ê–¢–ò–¢ –¢–û–ö–ï–ù–´ –í PRODUCTION"""
    
    def __init__(self, api_key: str):
        # –ó–∞—â–∏—Ç–∞ –æ—Ç —Å–ª—É—á–∞–π–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Ç–µ—Å—Ç–∞—Ö
        if BLOCK_EXTERNAL_IO:
            raise RuntimeError(
                f"üö´ External IO blocked in environment: {ENVIRONMENT}. "
                "Anthropic calls are prohibited in test/CI environments to prevent token spending."
            )
            
        if not api_key:
            raise ValueError("Anthropic API key is required")
            
        self.api_key = api_key
        logger.info("üí∞ LLM provider: Anthropic (REAL tokens will be spent)")
    
    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> Dict[str, Any]:
        """–†–µ–∞–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ Anthropic API"""
        if BLOCK_EXTERNAL_IO:
            raise RuntimeError("External IO is blocked - Anthropic calls prohibited")
            
        # TODO: Implement real Anthropic API call
        logger.warning("‚ö†Ô∏è Anthropic implementation not yet complete - using stub")
        return {
            "role": "assistant", 
            "content": "[Anthropic-stub] Real implementation needed",
            "metadata": {
                "provider": "anthropic",
                "tokens_used": 0,
                "cost": 0.0,
                "is_fake": False
            }
        }
    
    @property
    def provider_name(self) -> str:
        return "Anthropic"


def get_llm(dry_run: bool = False, provider_override: str = None) -> BaseLLM:
    """
    –§–∞–±—Ä–∏–∫–∞ LLM –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞—â–∏—Ç–æ–π –æ—Ç —Ç—Ä–∞—Ç —Ç–æ–∫–µ–Ω–æ–≤
    
    Args:
        dry_run: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å FakeLLM –¥–∞–∂–µ –≤ –ø—Ä–æ–¥—É–∫—à–µ–Ω–µ
        provider_override: –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (–¥–ª—è —Ç–µ—Å—Ç–æ–≤)
        
    Returns:
        BaseLLM: –ö–ª–∏–µ–Ω—Ç LLM (—Ä–µ–∞–ª—å–Ω—ã–π –∏–ª–∏ —Ñ–µ–π–∫)
        
    Raises:
        RuntimeError: –ï—Å–ª–∏ –≤–Ω–µ—à–Ω–∏–µ –≤—ã–∑–æ–≤—ã –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã –Ω–æ –ø—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
    """
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
    provider = provider_override or LLM_PROVIDER
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ fake –≤ —Ç–µ—Å—Ç–∞—Ö –∏–ª–∏ –ø—Ä–∏ dry_run
    if IS_TEST_ENV or BLOCK_EXTERNAL_IO or dry_run or provider == "fake":
        return FakeLLM()
    
    # –ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã
    try:
        if provider == "openai":
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º AI Token Pool –≤–º–µ—Å—Ç–æ env –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            logger.info("OpenAI LLM –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω—ã –∏–∑ AI Token Pool")
            return OpenAILLM(api_key="from_token_pool")
            
        elif provider == "anthropic":
            api_key = os.getenv("ANTHROPIC_API_KEY")
            if not api_key:
                logger.warning("ANTHROPIC_API_KEY not found, falling back to FakeLLM")
                return FakeLLM()
            return AnthropicLLM(api_key)
            
        else:
            logger.warning(f"Unknown LLM provider: {provider}, falling back to FakeLLM")
            return FakeLLM()
            
    except Exception as e:
        logger.error(f"Failed to initialize {provider} LLM: {e}, falling back to FakeLLM")
        return FakeLLM()


def check_dry_run_request(headers: Dict[str, str] = None, query_params: Dict[str, str] = None) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Ç—Ä–µ–±—É–µ—Ç –ª–∏ –∑–∞–ø—Ä–æ—Å dry-run —Ä–µ–∂–∏–º
    
    Args:
        headers: HTTP –∑–∞–≥–æ–ª–æ–≤–∫–∏
        query_params: GET –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        
    Returns:
        bool: True –µ—Å–ª–∏ –Ω—É–∂–µ–Ω dry-run
    """
    headers = headers or {}
    query_params = query_params or {}
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ X-Dry-Run (case-insensitive)
    dry_run_header = next(
        (v.lower() in ("1", "true", "yes") for k, v in headers.items() 
         if k.lower() == "x-dry-run"), 
        False
    )
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º query –ø–∞—Ä–∞–º–µ—Ç—Ä
    dry_run_query = query_params.get("dry_run", "").lower() in ("1", "true", "yes")
    
    return dry_run_header or dry_run_query


# –£–¥–æ–±–Ω—ã–µ –∞–ª–∏–∞—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def generate_ai_response(
    messages: List[Dict[str, str]], 
    dialog_id: int = None, 
    dry_run: bool = False,
    **kwargs
) -> Dict[str, Any]:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ AI –æ—Ç–≤–µ—Ç–∞
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç —Ç—Ä–∞—Ç —Ç–æ–∫–µ–Ω–æ–≤
    """
    llm = get_llm(dry_run=dry_run)
    
    response = await llm.chat(
        messages=messages,
        dialog_id=dialog_id,
        **kwargs
    )
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    response["metadata"]["dialog_id"] = dialog_id
    response["metadata"]["provider_used"] = llm.provider_name
    
    return response


# –≠–∫—Å–ø–æ—Ä—Ç –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ –∏ —Ñ—É–Ω–∫—Ü–∏–π
__all__ = [
    "BaseLLM",
    "FakeLLM", 
    "OpenAILLM",
    "AnthropicLLM",
    "get_llm",
    "generate_ai_response",
    "check_dry_run_request"
]