#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –º–∏–≥—Ä–∞—Ü–∏–π Alembic –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
"""

import os
import re
import ast
import json
import hashlib
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple
import argparse

# –ü—É—Ç—å –∫ –º–∏–≥—Ä–∞—Ü–∏—è–º
MIGRATIONS_DIR = Path(__file__).parent.parent.parent / "alembic" / "versions"

class MigrationAnalyzer:
    def __init__(self, migrations_dir: Path):
        self.migrations_dir = migrations_dir
        self.migrations = []
        self.issues = []
        
    def load_migrations(self) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –º–∏–≥—Ä–∞—Ü–∏–∏ –∏–∑ –ø–∞–ø–∫–∏"""
        print(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –º–∏–≥—Ä–∞—Ü–∏–∏ –≤: {self.migrations_dir}")
        
        migrations = []
        for file_path in self.migrations_dir.glob("*.py"):
            if file_path.name == "__init__.py":
                continue
                
            try:
                migration_info = self._parse_migration_file(file_path)
                migrations.append(migration_info)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {file_path.name}: {e}")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∞–π–ª–∞
        migrations.sort(key=lambda x: x['created_at'])
        self.migrations = migrations
        return migrations
    
    def _parse_migration_file(self, file_path: Path) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç —Ñ–∞–π–ª –º–∏–≥—Ä–∞—Ü–∏–∏"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
        revision_match = re.search(r'revision = [\'"]([^\'"]+)[\'"]', content)
        down_revision_match = re.search(r'down_revision = [\'"]([^\'"]+)[\'"]', content)
        create_date_match = re.search(r'create_date = datetime\.datetime\(([^)]+)\)', content)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º SQL –æ–ø–µ—Ä–∞—Ü–∏–∏
        operations = self._extract_operations(content)
        
        return {
            'filename': file_path.name,
            'filepath': file_path,
            'revision': revision_match.group(1) if revision_match else None,
            'down_revision': down_revision_match.group(1) if down_revision_match else None,
            'create_date': create_date_match.group(1) if create_date_match else None,
            'created_at': file_path.stat().st_mtime,
            'size': file_path.stat().st_size,
            'content': content,
            'operations': operations,
            'is_merge': 'merge' in file_path.name.lower(),
            'complexity_score': self._calculate_complexity(operations)
        }
    
    def _extract_operations(self, content: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏ –∏–∑ –º–∏–≥—Ä–∞—Ü–∏–∏"""
        operations = []
        
        # –ò—â–µ–º –≤—ã–∑–æ–≤—ã op.*
        op_patterns = [
            (r'op\.create_table\([\'"]([^\'"]+)[\'"]', 'create_table'),
            (r'op\.drop_table\([\'"]([^\'"]+)[\'"]', 'drop_table'),  
            (r'op\.add_column\([\'"]([^\'"]+)[\'"]', 'add_column'),
            (r'op\.drop_column\([\'"]([^\'"]+)[\'"]', 'drop_column'),
            (r'op\.create_index\([\'"]([^\'"]+)[\'"]', 'create_index'),
            (r'op\.drop_index\([\'"]([^\'"]+)[\'"]', 'drop_index'),
            (r'op\.alter_column\(', 'alter_column'),
            (r'op\.execute\(', 'execute_sql'),
        ]
        
        for pattern, op_type in op_patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                table_name = match.group(1) if match.groups() else None
                operations.append({
                    'type': op_type,
                    'table': table_name,
                    'line': content[:match.start()].count('\n') + 1
                })
        
        return operations
    
    def _calculate_complexity(self, operations: List[Dict[str, Any]]) -> int:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–∏–≥—Ä–∞—Ü–∏–∏"""
        complexity = 0
        weights = {
            'create_table': 2,
            'drop_table': 3,
            'add_column': 1,
            'drop_column': 2, 
            'create_index': 1,
            'drop_index': 1,
            'alter_column': 3,
            'execute_sql': 5
        }
        
        for op in operations:
            complexity += weights.get(op['type'], 1)
        
        return complexity
    
    def analyze_issues(self) -> List[Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –≤ –º–∏–≥—Ä–∞—Ü–∏—è—Ö"""
        issues = []
        
        # 1. Merge heads
        merge_migrations = [m for m in self.migrations if m['is_merge']]
        if merge_migrations:
            issues.append({
                'type': 'merge_conflicts',
                'severity': 'high',
                'count': len(merge_migrations),
                'description': f'–ù–∞–π–¥–µ–Ω–æ {len(merge_migrations)} merge –º–∏–≥—Ä–∞—Ü–∏–π',
                'files': [m['filename'] for m in merge_migrations]
            })
        
        # 2. –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ–π
        table_operations = {}
        for migration in self.migrations:
            for op in migration['operations']:
                if op['table']:
                    table_name = op['table']
                    if table_name not in table_operations:
                        table_operations[table_name] = []
                    table_operations[table_name].append({
                        'migration': migration['filename'],
                        'operation': op['type']
                    })
        
        # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—ã —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏
        for table, ops in table_operations.items():
            if len(ops) > 5:  # –ü–æ—Ä–æ–≥ –¥–ª—è "—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –æ–ø–µ—Ä–∞—Ü–∏–π"
                issues.append({
                    'type': 'excessive_table_operations',
                    'severity': 'medium',
                    'table': table,
                    'operations_count': len(ops),
                    'description': f'–¢–∞–±–ª–∏—Ü–∞ {table} –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª–∞—Å—å {len(ops)} —Ä–∞–∑',
                    'operations': ops
                })
        
        # 3. –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ (create/drop –æ–¥–Ω–æ–π –∏ —Ç–æ–π –∂–µ —Ç–∞–±–ª–∏—Ü—ã)
        table_lifecycle = {}
        for migration in self.migrations:
            for op in migration['operations']:
                if op['table'] and op['type'] in ['create_table', 'drop_table']:
                    table = op['table']
                    if table not in table_lifecycle:
                        table_lifecycle[table] = []
                    table_lifecycle[table].append({
                        'migration': migration['filename'],
                        'operation': op['type'],
                        'date': migration['created_at']
                    })
        
        for table, lifecycle in table_lifecycle.items():
            creates = [op for op in lifecycle if op['operation'] == 'create_table']
            drops = [op for op in lifecycle if op['operation'] == 'drop_table']
            
            if len(creates) > 1 or len(drops) > 1:
                issues.append({
                    'type': 'table_recreation',
                    'severity': 'high',
                    'table': table,
                    'description': f'–¢–∞–±–ª–∏—Ü–∞ {table} —Å–æ–∑–¥–∞–≤–∞–ª–∞—Å—å/—É–¥–∞–ª—è–ª–∞—Å—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑',
                    'lifecycle': lifecycle
                })
        
        # 4. –û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ –º–∏–≥—Ä–∞—Ü–∏–∏ (–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –º–µ–¥–ª–µ–Ω–Ω—ã–µ)
        large_migrations = [m for m in self.migrations if m['complexity_score'] > 20]
        if large_migrations:
            issues.append({
                'type': 'complex_migrations',
                'severity': 'medium',
                'count': len(large_migrations),
                'description': f'–ù–∞–π–¥–µ–Ω–æ {len(large_migrations)} —Å–ª–æ–∂–Ω—ã—Ö –º–∏–≥—Ä–∞—Ü–∏–π',
                'migrations': [(m['filename'], m['complexity_score']) for m in large_migrations]
            })
        
        # 5. –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ down_revision —Å–≤—è–∑–∏
        orphaned_migrations = [m for m in self.migrations if not m['down_revision'] and not m['is_merge']]
        if len(orphaned_migrations) > 1:  # –ü–µ—Ä–≤–∞—è –º–∏–≥—Ä–∞—Ü–∏—è –º–æ–∂–µ—Ç –Ω–µ –∏–º–µ—Ç—å down_revision
            issues.append({
                'type': 'orphaned_migrations', 
                'severity': 'medium',
                'count': len(orphaned_migrations) - 1,
                'description': f'–ú–∏–≥—Ä–∞—Ü–∏–∏ –±–µ–∑ down_revision —Å–≤—è–∑–∏',
                'files': [m['filename'] for m in orphaned_migrations[1:]]
            })
        
        self.issues = issues
        return issues
    
    def generate_consolidation_plan(self) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–ª–∞–Ω –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –º–∏–≥—Ä–∞—Ü–∏–π"""
        if not self.migrations:
            return {}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –º–∏–≥—Ä–∞—Ü–∏–∏ –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º
        periods = self._group_by_periods()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—é
        consolidation_candidates = []
        
        for period_name, period_migrations in periods.items():
            if len(period_migrations) > 5:  # –ü–µ—Ä–∏–æ–¥ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –º–∏–≥—Ä–∞—Ü–∏—è–º–∏
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–∂–Ω–æ –ª–∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å
                table_changes = {}
                for migration in period_migrations:
                    for op in migration['operations']:
                        if op['table']:
                            table_name = op['table']
                            if table_name not in table_changes:
                                table_changes[table_name] = []
                            table_changes[table_name].append(op)
                
                consolidation_candidates.append({
                    'period': period_name,
                    'migrations_count': len(period_migrations),
                    'migrations': [m['filename'] for m in period_migrations],
                    'table_changes': table_changes,
                    'complexity_total': sum(m['complexity_score'] for m in period_migrations),
                    'can_consolidate': self._can_consolidate_period(period_migrations)
                })
        
        return {
            'total_migrations': len(self.migrations),
            'consolidation_candidates': consolidation_candidates,
            'estimated_reduction': self._estimate_reduction(consolidation_candidates),
            'recommended_approach': self._recommend_consolidation_approach()
        }
    
    def _group_by_periods(self) -> Dict[str, List[Dict[str, Any]]]:
        """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –º–∏–≥—Ä–∞—Ü–∏–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –ø–µ—Ä–∏–æ–¥–∞–º"""
        periods = {}
        
        for migration in self.migrations:
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –º–µ—Å—è—Ü–∞–º
            date = datetime.fromtimestamp(migration['created_at'])
            period_key = f"{date.year}-{date.month:02d}"
            
            if period_key not in periods:
                periods[period_key] = []
            periods[period_key].append(migration)
        
        return periods
    
    def _can_consolidate_period(self, migrations: List[Dict[str, Any]]) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–æ–∂–Ω–æ –ª–∏ –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –º–∏–≥—Ä–∞—Ü–∏–∏ –ø–µ—Ä–∏–æ–¥–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –µ—Å–ª–∏ –Ω–µ—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
        table_operations = {}
        
        for migration in migrations:
            for op in migration['operations']:
                if op['table']:
                    table = op['table']
                    if table not in table_operations:
                        table_operations[table] = []
                    table_operations[table].append(op['type'])
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è
        for table, ops in table_operations.items():
            # –ï—Å–ª–∏ –µ—Å—Ç—å create –∏ drop –æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã - —Å–ª–æ–∂–Ω–æ –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å
            if 'create_table' in ops and 'drop_table' in ops:
                return False
            
            # –ï—Å–ª–∏ –º–Ω–æ–≥–æ alter_column - –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–æ–∂–Ω–æ
            if ops.count('alter_column') > 3:
                return False
        
        return True
    
    def _estimate_reduction(self, candidates: List[Dict[str, Any]]) -> Dict[str, int]:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –º–∏–≥—Ä–∞—Ü–∏–π"""
        total_can_consolidate = sum(
            len(c['migrations']) for c in candidates if c['can_consolidate']
        )
        
        estimated_result = max(1, total_can_consolidate // 3)  # –ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä—É–µ–º –≤ ~3 —Ä–∞–∑–∞ –º–µ–Ω—å—à–µ
        
        return {
            'current_count': len(self.migrations),
            'can_consolidate': total_can_consolidate,
            'estimated_result': len(self.migrations) - total_can_consolidate + estimated_result,
            'reduction_percent': round((total_can_consolidate - estimated_result) / len(self.migrations) * 100, 1)
        }
    
    def _recommend_consolidation_approach(self) -> str:
        """–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç –ø–æ–¥—Ö–æ–¥ –∫ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏"""
        if len(self.migrations) < 10:
            return "manual_review"
        elif len(self.migrations) < 30:
            return "period_consolidation"  # –ü–æ –ø–µ—Ä–∏–æ–¥–∞–º
        else:
            return "schema_snapshot"  # –°–æ–∑–¥–∞—Ç—å —á–∏—Å—Ç—ã–π —Å–Ω–∞–ø—à–æ—Ç —Å—Ö–µ–º—ã
    
    def generate_report(self, output_file: str = None) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É"""
        report = {
            'analysis_date': datetime.now().isoformat(),
            'migrations_total': len(self.migrations),
            'migrations_directory': str(self.migrations_dir),
            'summary': {
                'total_files': len(self.migrations),
                'merge_migrations': len([m for m in self.migrations if m['is_merge']]),
                'total_complexity': sum(m['complexity_score'] for m in self.migrations),
                'average_complexity': round(sum(m['complexity_score'] for m in self.migrations) / len(self.migrations), 2) if self.migrations else 0,
                'total_size_kb': round(sum(m['size'] for m in self.migrations) / 1024, 2)
            },
            'issues': self.issues,
            'consolidation_plan': self.generate_consolidation_plan(),
            'migrations_list': [
                {
                    'filename': m['filename'],
                    'complexity': m['complexity_score'],
                    'operations_count': len(m['operations']),
                    'is_merge': m['is_merge'],
                    'size_bytes': m['size']
                }
                for m in self.migrations
            ]
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç –≤ —Ñ–∞–π–ª
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            print(f"–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}")
        
        return report
    
    def print_summary(self):
        """–í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –≤ –∫–æ–Ω—Å–æ–ª—å"""
        print(f"\n{'='*60}")
        print(f"–ê–ù–ê–õ–ò–ó –ú–ò–ì–†–ê–¶–ò–ô CHATAI")
        print(f"{'='*60}")
        print(f"–í—Å–µ–≥–æ –º–∏–≥—Ä–∞—Ü–∏–π: {len(self.migrations)}")
        print(f"Merge –º–∏–≥—Ä–∞—Ü–∏–π: {len([m for m in self.migrations if m['is_merge']])}")
        
        if self.issues:
            print(f"\nüî• –ù–ê–ô–î–ï–ù–ù–´–ï –ü–†–û–ë–õ–ï–ú–´:")
            for issue in self.issues:
                severity_icon = "üö®" if issue['severity'] == 'high' else "‚ö†Ô∏è"
                print(f"{severity_icon} {issue['type']}: {issue['description']}")
        else:
            print(f"\n‚úÖ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        consolidation_plan = self.generate_consolidation_plan()
        if consolidation_plan.get('consolidation_candidates'):
            reduction = consolidation_plan['estimated_reduction']
            print(f"\nüìä –ü–õ–ê–ù –ö–û–ù–°–û–õ–ò–î–ê–¶–ò–ò:")
            print(f"–¢–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {reduction['current_count']}")
            print(f"–ú–æ–∂–Ω–æ –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å: {reduction['can_consolidate']}")
            print(f"–ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {reduction['estimated_result']}")
            print(f"–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –Ω–∞: {reduction['reduction_percent']}%")
        
        print(f"\n{'='*60}")

def main():
    parser = argparse.ArgumentParser(description='–ê–Ω–∞–ª–∏–∑ –º–∏–≥—Ä–∞—Ü–∏–π Alembic')
    parser.add_argument('--migrations-dir', '-d', 
                       default=MIGRATIONS_DIR,
                       help='–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –º–∏–≥—Ä–∞—Ü–∏—è–º–∏')
    parser.add_argument('--output', '-o',
                       help='–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSON –æ—Ç—á–µ—Ç–∞')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥')
    
    args = parser.parse_args()
    
    analyzer = MigrationAnalyzer(Path(args.migrations_dir))
    
    print("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–∏–≥—Ä–∞—Ü–∏–∏...")
    migrations = analyzer.load_migrations()
    
    print("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–±–ª–µ–º—ã...")
    issues = analyzer.analyze_issues()
    
    if args.verbose:
        print(f"\n–ó–∞–≥—Ä—É–∂–µ–Ω–æ –º–∏–≥—Ä–∞—Ü–∏–π: {len(migrations)}")
        for migration in migrations:
            print(f"  {migration['filename']}: {migration['complexity_score']} –æ–ø–µ—Ä–∞—Ü–∏–π")
    
    # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
    analyzer.print_summary()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç
    report = analyzer.generate_report(args.output)
    
    if not args.output:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ default —Ñ–∞–π–ª
        default_output = Path(__file__).parent / f"migration_analysis_{datetime.now().strftime('%Y%m%d_%H%M')}.json"
        analyzer.generate_report(str(default_output))

if __name__ == '__main__':
    main()