#!/usr/bin/env python3
"""
–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ë–î –¥–ª—è ReplyX
–°–æ–±–∏—Ä–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏, –¥–µ—Ç–µ–∫—Ç–∏—Ç N+1, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
"""

import os
import sys
import json
import time
import psutil
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import argparse

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.append(str(Path(__file__).parent.parent.parent))

@dataclass
class DatabaseMetrics:
    timestamp: str
    active_connections: int
    idle_connections: int
    waiting_connections: int
    total_connections: int
    database_size_mb: float
    largest_tables: List[Dict[str, Any]]
    slow_queries: List[Dict[str, Any]]
    unused_indexes: List[Dict[str, Any]]
    n_plus_one_suspects: List[Dict[str, Any]]
    cache_hit_ratio: float
    transaction_rate: float
    avg_query_time_ms: float
    cpu_usage_percent: float
    memory_usage_mb: float
    disk_io_read_mb: float
    disk_io_write_mb: float

class DatabaseMonitor:
    def __init__(self, db_url: str = None):
        self.db_url = db_url or self._get_database_url()
        self.connection = None
        self.logger = self._setup_logging()
        
    def _get_database_url(self) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç URL –ë–î –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            from core.app_config import DATABASE_URL
            return DATABASE_URL
        except ImportError:
            return os.environ.get('DATABASE_URL', 'postgresql://localhost/replyx_db')
    
    def _setup_logging(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        logger = logging.getLogger('db_monitor')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –ª–æ–≥–æ–≤
            log_dir = Path(__file__).parent.parent.parent / "logs" / "monitoring"
            log_dir.mkdir(parents=True, exist_ok=True)
            
            # –§–∞–π–ª —Å —Ä–æ—Ç–∞—Ü–∏–µ–π
            handler = logging.FileHandler(
                log_dir / f"db_performance_{datetime.now().strftime('%Y%m%d')}.log"
            )
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
            # –ö–æ–Ω—Å–æ–ª—å
            console_handler = logging.StreamHandler()
            console_handler.setFormatter(formatter)
            logger.addHandler(console_handler)
        
        return logger
    
    def connect(self) -> bool:
        """–ü–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ –ë–î"""
        try:
            import psycopg2
            from urllib.parse import urlparse
            
            # –ü–∞—Ä—Å–∏–º URL –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
            parsed = urlparse(self.db_url)
            
            self.connection = psycopg2.connect(
                host=parsed.hostname,
                port=parsed.port or 5432,
                database=parsed.path[1:] if parsed.path else 'postgres',
                user=parsed.username,
                password=parsed.password
            )
            
            self.logger.info("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            return True
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
            return False
    
    def get_connection_stats(self) -> Dict[str, int]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π"""
        query = """
        SELECT 
            state,
            COUNT(*) as count
        FROM pg_stat_activity 
        WHERE datname = current_database()
        GROUP BY state;
        """
        
        try:
            with self.connection.cursor() as cur:
                cur.execute(query)
                rows = cur.fetchall()
                
                stats = {
                    'active': 0,
                    'idle': 0,
                    'idle_in_transaction': 0,
                    'waiting': 0,
                    'total': 0
                }
                
                for state, count in rows:
                    if state == 'active':
                        stats['active'] = count
                    elif state == 'idle':
                        stats['idle'] = count
                    elif state == 'idle in transaction':
                        stats['idle_in_transaction'] = count
                    elif state == 'waiting':
                        stats['waiting'] = count
                    
                    stats['total'] += count
                
                return stats
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π: {e}")
            return {}
    
    def get_database_size(self) -> float:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –ë–î –≤ MB"""
        query = """
        SELECT pg_database_size(current_database()) / 1024.0 / 1024.0 as size_mb;
        """
        
        try:
            with self.connection.cursor() as cur:
                cur.execute(query)
                return cur.fetchone()[0]
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –ë–î: {e}")
            return 0.0
    
    def get_largest_tables(self, limit: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–∞–º—ã–µ –±–æ–ª—å—à–∏–µ —Ç–∞–±–ª–∏—Ü—ã"""
        query = """
        SELECT 
            schemaname,
            tablename,
            pg_total_relation_size(schemaname||'.'||tablename) / 1024.0 / 1024.0 as total_size_mb,
            pg_relation_size(schemaname||'.'||tablename) / 1024.0 / 1024.0 as table_size_mb,
            (pg_total_relation_size(schemaname||'.'||tablename) - pg_relation_size(schemaname||'.'||tablename)) / 1024.0 / 1024.0 as index_size_mb,
            n_tup_ins + n_tup_upd + n_tup_del as total_dml_ops,
            n_live_tup as live_rows,
            n_dead_tup as dead_rows
        FROM pg_tables pt
        LEFT JOIN pg_stat_user_tables pst ON pt.tablename = pst.relname
        WHERE schemaname = 'public'
        ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
        LIMIT %s;
        """
        
        try:
            with self.connection.cursor() as cur:
                cur.execute(query, (limit,))
                rows = cur.fetchall()
                
                return [
                    {
                        'schema': row[0],
                        'table': row[1],
                        'total_size_mb': float(row[2]),
                        'table_size_mb': float(row[3]),
                        'index_size_mb': float(row[4]),
                        'total_dml_ops': int(row[5] or 0),
                        'live_rows': int(row[6] or 0),
                        'dead_rows': int(row[7] or 0)
                    }
                    for row in rows
                ]
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤ —Ç–∞–±–ª–∏—Ü: {e}")
            return []
    
    def get_slow_queries(self, min_calls: int = 5) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏–∑ pg_stat_statements"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
        check_extension = """
        SELECT EXISTS(SELECT 1 FROM pg_extension WHERE extname = 'pg_stat_statements');
        """
        
        try:
            with self.connection.cursor() as cur:
                cur.execute(check_extension)
                if not cur.fetchone()[0]:
                    self.logger.warning("–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ pg_stat_statements –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
                    return []
                
                # –ü–æ–ª—É—á–∞–µ–º –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
                query = """
                SELECT 
                    LEFT(query, 100) as query_preview,
                    calls,
                    total_time,
                    mean_time,
                    max_time,
                    stddev_time,
                    rows,
                    100.0 * shared_blks_hit / NULLIF(shared_blks_hit + shared_blks_read, 0) as hit_percent
                FROM pg_stat_statements
                WHERE calls >= %s
                    AND mean_time > 100  -- –ú–µ–¥–ª–µ–Ω–Ω–µ–µ 100ms
                ORDER BY mean_time DESC
                LIMIT 20;
                """
                
                cur.execute(query, (min_calls,))
                rows = cur.fetchall()
                
                return [
                    {
                        'query_preview': row[0],
                        'calls': int(row[1]),
                        'total_time': float(row[2]),
                        'mean_time': float(row[3]),
                        'max_time': float(row[4]),
                        'stddev_time': float(row[5]) if row[5] else 0,
                        'rows': int(row[6]),
                        'cache_hit_percent': float(row[7]) if row[7] else 0
                    }
                    for row in rows
                ]
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {e}")
            return []
    
    def get_unused_indexes(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∏–Ω–¥–µ–∫—Å—ã"""
        query = """
        SELECT 
            schemaname,
            tablename,
            indexname,
            idx_scan,
            pg_size_pretty(pg_relation_size(indexrelid)) as index_size,
            pg_relation_size(indexrelid) / 1024.0 / 1024.0 as index_size_mb
        FROM pg_stat_user_indexes
        WHERE idx_scan = 0
            AND indexrelid NOT IN (
                -- –ò—Å–∫–ª—é—á–∞–µ–º PK –∏ UNIQUE –∏–Ω–¥–µ–∫—Å—ã
                SELECT indexrelid FROM pg_constraint WHERE contype IN ('p','u')
            )
        ORDER BY pg_relation_size(indexrelid) DESC;
        """
        
        try:
            with self.connection.cursor() as cur:
                cur.execute(query)
                rows = cur.fetchall()
                
                return [
                    {
                        'schema': row[0],
                        'table': row[1],
                        'index': row[2],
                        'scans': int(row[3]),
                        'size': row[4],
                        'size_mb': float(row[5])
                    }
                    for row in rows
                ]
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤: {e}")
            return []
    
    def detect_n_plus_one_suspects(self) -> List[Dict[str, Any]]:
        """–î–µ—Ç–µ–∫—Ç–∏—Ç –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã N+1"""
        if not hasattr(self, '_query_log'):
            return []
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –ø–∞—Ç—Ç–µ—Ä–Ω—ã N+1
        suspects = []
        query_patterns = {}
        
        for query_info in self._query_log:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å (—É–±–∏—Ä–∞–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)
            normalized = self._normalize_query(query_info['query'])
            
            if normalized not in query_patterns:
                query_patterns[normalized] = []
            
            query_patterns[normalized].append(query_info)
        
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø–æ—Ö–æ–∂–∏–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏
        for pattern, queries in query_patterns.items():
            if len(queries) > 10:  # –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –º–Ω–æ–≥–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                suspects.append({
                    'pattern': pattern,
                    'count': len(queries),
                    'avg_time': sum(q['duration'] for q in queries) / len(queries),
                    'total_time': sum(q['duration'] for q in queries),
                    'suspected_n_plus_one': True
                })
        
        return suspects
    
    def _normalize_query(self, query: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        import re
        
        # –£–±–∏—Ä–∞–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        normalized = re.sub(r'\b\d+\b', '?', query)  # –ß–∏—Å–ª–∞
        normalized = re.sub(r"'[^']*'", "'?'", normalized)  # –°—Ç—Ä–æ–∫–∏
        normalized = re.sub(r'\s+', ' ', normalized)  # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        
        return normalized.strip().lower()
    
    def get_cache_hit_ratio(self) -> float:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø–æ–ø–∞–¥–∞–Ω–∏–π –≤ –∫—ç—à"""
        query = """
        SELECT 
            100.0 * sum(blks_hit) / NULLIF(sum(blks_hit + blks_read), 0) as cache_hit_ratio
        FROM pg_stat_database
        WHERE datname = current_database();
        """
        
        try:
            with self.connection.cursor() as cur:
                cur.execute(query)
                result = cur.fetchone()[0]
                return float(result) if result else 0.0
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è cache hit ratio: {e}")
            return 0.0
    
    def get_transaction_rate(self) -> float:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –≤ —Å–µ–∫—É–Ω–¥—É"""
        query = """
        SELECT 
            xact_commit + xact_rollback as total_transactions
        FROM pg_stat_database
        WHERE datname = current_database();
        """
        
        try:
            with self.connection.cursor() as cur:
                cur.execute(query)
                current_transactions = cur.fetchone()[0]
                
                # –î–ª—è —Ä–∞—Å—á–µ—Ç–∞ rate –Ω—É–∂–Ω—ã –¥–≤–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è
                if hasattr(self, '_prev_transactions'):
                    rate = (current_transactions - self._prev_transactions) / 60  # –∑–∞ –º–∏–Ω—É—Ç—É
                    self._prev_transactions = current_transactions
                    return float(rate)
                else:
                    self._prev_transactions = current_transactions
                    return 0.0
                    
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è transaction rate: {e}")
            return 0.0
    
    def get_avg_query_time(self) -> float:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤"""
        try:
            with self.connection.cursor() as cur:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º pg_stat_statements
                check_query = "SELECT EXISTS(SELECT 1 FROM pg_extension WHERE extname = 'pg_stat_statements')"
                cur.execute(check_query)
                
                if not cur.fetchone()[0]:
                    return 0.0
                
                query = """
                SELECT AVG(mean_time) as avg_query_time
                FROM pg_stat_statements
                WHERE calls > 1;
                """
                
                cur.execute(query)
                result = cur.fetchone()[0]
                return float(result) if result else 0.0
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–ø—Ä–æ—Å–æ–≤: {e}")
            return 0.0
    
    def get_system_metrics(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–µ—Ä–≤–µ—Ä–∞"""
        try:
            # CPU usage
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # Memory usage
            memory = psutil.virtual_memory()
            memory_used_mb = memory.used / 1024 / 1024
            
            # Disk I/O
            disk_io = psutil.disk_io_counters()
            disk_read_mb = disk_io.read_bytes / 1024 / 1024 if disk_io else 0
            disk_write_mb = disk_io.write_bytes / 1024 / 1024 if disk_io else 0
            
            return {
                'cpu_usage_percent': cpu_percent,
                'memory_usage_mb': memory_used_mb,
                'disk_io_read_mb': disk_read_mb,
                'disk_io_write_mb': disk_write_mb
            }
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫: {e}")
            return {
                'cpu_usage_percent': 0.0,
                'memory_usage_mb': 0.0,
                'disk_io_read_mb': 0.0,
                'disk_io_write_mb': 0.0
            }
    
    def collect_metrics(self) -> DatabaseMetrics:
        """–°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏"""
        self.logger.info("–°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –ë–î...")
        
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –µ—Å–ª–∏ –µ—â–µ –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω—ã
        if not self.connection:
            if not self.connect():
                return None
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏
        connection_stats = self.get_connection_stats()
        system_metrics = self.get_system_metrics()
        
        metrics = DatabaseMetrics(
            timestamp=datetime.now().isoformat(),
            active_connections=connection_stats.get('active', 0),
            idle_connections=connection_stats.get('idle', 0),
            waiting_connections=connection_stats.get('waiting', 0),
            total_connections=connection_stats.get('total', 0),
            database_size_mb=self.get_database_size(),
            largest_tables=self.get_largest_tables(),
            slow_queries=self.get_slow_queries(),
            unused_indexes=self.get_unused_indexes(),
            n_plus_one_suspects=self.detect_n_plus_one_suspects(),
            cache_hit_ratio=self.get_cache_hit_ratio(),
            transaction_rate=self.get_transaction_rate(),
            avg_query_time_ms=self.get_avg_query_time(),
            cpu_usage_percent=system_metrics['cpu_usage_percent'],
            memory_usage_mb=system_metrics['memory_usage_mb'],
            disk_io_read_mb=system_metrics['disk_io_read_mb'],
            disk_io_write_mb=system_metrics['disk_io_write_mb']
        )
        
        return metrics
    
    def save_metrics(self, metrics: DatabaseMetrics, output_dir: Path = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–∞–π–ª"""
        if not output_dir:
            output_dir = Path(__file__).parent.parent.parent / "logs" / "monitoring"
            output_dir.mkdir(parents=True, exist_ok=True)
        
        # JSON —Ñ–∞–π–ª —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        json_file = output_dir / f"db_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(metrics), f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"–ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {json_file}")
        
        # CSV —Ñ–∞–π–ª –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤  
        csv_file = output_dir / "db_metrics_timeseries.csv"
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ CSV –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–æ–≤—ã–π
        write_header = not csv_file.exists()
        
        import csv
        with open(csv_file, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=asdict(metrics).keys())
            
            if write_header:
                writer.writeheader()
            
            # –£–ø—Ä–æ—â–∞–µ–º —Å–ª–æ–∂–Ω—ã–µ –ø–æ–ª—è –¥–ª—è CSV
            csv_metrics = asdict(metrics)
            csv_metrics['largest_tables'] = len(metrics.largest_tables)
            csv_metrics['slow_queries'] = len(metrics.slow_queries)
            csv_metrics['unused_indexes'] = len(metrics.unused_indexes)
            csv_metrics['n_plus_one_suspects'] = len(metrics.n_plus_one_suspects)
            
            writer.writerow(csv_metrics)
    
    def generate_report(self, metrics: DatabaseMetrics) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º"""
        report = f"""
üìä –û–¢–ß–ï–¢ –ü–û –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò –ë–î REPLYX
========================================
–í—Ä–µ–º—è: {metrics.timestamp}

üîó –ü–û–î–ö–õ–Æ–ß–ï–ù–ò–Ø:
  –ê–∫—Ç–∏–≤–Ω—ã—Ö: {metrics.active_connections}
  –û–∂–∏–¥–∞—é—â–∏—Ö: {metrics.idle_connections}
  –í –æ–∂–∏–¥–∞–Ω–∏–∏: {metrics.waiting_connections}
  –í—Å–µ–≥–æ: {metrics.total_connections}

üíæ –†–ê–ó–ú–ï–† –ë–î: {metrics.database_size_mb:.2f} MB

üóÇ –°–ê–ú–´–ï –ë–û–õ–¨–®–ò–ï –¢–ê–ë–õ–ò–¶–´:
"""
        
        for table in metrics.largest_tables[:5]:
            report += f"  üìã {table['table']}: {table['total_size_mb']:.2f} MB ({table['live_rows']:,} —Å—Ç—Ä–æ–∫)\n"
        
        report += f"""
‚ö° –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨:
  –ü–æ–ø–∞–¥–∞–Ω–∏–µ –≤ –∫—ç—à: {metrics.cache_hit_ratio:.2f}%
  –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–π/–º–∏–Ω: {metrics.transaction_rate:.2f}
  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–∞: {metrics.avg_query_time_ms:.2f}ms

üêå –ú–ï–î–õ–ï–ù–ù–´–ï –ó–ê–ü–†–û–°–´: {len(metrics.slow_queries)}
"""
        
        for query in metrics.slow_queries[:3]:
            report += f"  üö® {query['query_preview'][:50]}... ({query['mean_time']:.2f}ms, {query['calls']} –≤—ã–∑–æ–≤–æ–≤)\n"
        
        report += f"""
üìà –°–ò–°–¢–ï–ú–ê:
  CPU: {metrics.cpu_usage_percent:.1f}%
  RAM: {metrics.memory_usage_mb:.0f} MB
  –î–∏—Å–∫ I/O: {metrics.disk_io_read_mb:.1f} MB —á—Ç–µ–Ω–∏–µ, {metrics.disk_io_write_mb:.1f} MB –∑–∞–ø–∏—Å—å

‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–´:
  –ù–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤: {len(metrics.unused_indexes)}
  –ü–æ–¥–æ–∑—Ä–µ–Ω–∏–π –Ω–∞ N+1: {len(metrics.n_plus_one_suspects)}
"""
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = []
        
        if metrics.cache_hit_ratio < 95:
            recommendations.append("üìå –ù–∏–∑–∫–∏–π cache hit ratio - —É–≤–µ–ª–∏—á—å—Ç–µ shared_buffers")
        
        if metrics.avg_query_time_ms > 100:
            recommendations.append("üìå –í—ã—Å–æ–∫–æ–µ –≤—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–æ–≤ - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω–¥–µ–∫—Å—ã")
        
        if len(metrics.unused_indexes) > 10:
            recommendations.append("üìå –ú–Ω–æ–≥–æ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–¥–∞–ª–µ–Ω–∏–µ")
        
        if metrics.active_connections > 50:
            recommendations.append("üìå –ú–Ω–æ–≥–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π - –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ connection pooling")
        
        if recommendations:
            report += "\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:\n"
            for rec in recommendations:
                report += f"  {rec}\n"
        
        return report
    
    def start_monitoring(self, interval_minutes: int = 5, max_iterations: int = None):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"""
        self.logger.info(f"–ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º {interval_minutes} –º–∏–Ω—É—Ç")
        
        iteration = 0
        
        try:
            while True:
                if max_iterations and iteration >= max_iterations:
                    break
                
                # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                metrics = self.collect_metrics()
                
                if metrics:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
                    self.save_metrics(metrics)
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
                    report = self.generate_report(metrics)
                    print(report)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
                    self.check_critical_issues(metrics)
                
                # –ñ–¥–µ–º —Å–ª–µ–¥—É—é—â—É—é –∏—Ç–µ—Ä–∞—Ü–∏—é
                if max_iterations is None or iteration < max_iterations - 1:
                    time.sleep(interval_minutes * 60)
                
                iteration += 1
                
        except KeyboardInterrupt:
            self.logger.info("–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")
    
    def check_critical_issues(self, metrics: DatabaseMetrics):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã"""
        issues = []
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∑–∫–∏–π cache hit ratio
        if metrics.cache_hit_ratio < 90:
            issues.append(f"–ö–†–ò–¢–ò–ß–ù–û: Cache hit ratio {metrics.cache_hit_ratio:.2f}% - –æ—á–µ–Ω—å –Ω–∏–∑–∫–∏–π!")
        
        # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π
        if metrics.active_connections > 100:
            issues.append(f"–ö–†–ò–¢–ò–ß–ù–û: {metrics.active_connections} –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π!")
        
        # –û—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        slow_queries = [q for q in metrics.slow_queries if q['mean_time'] > 1000]
        if slow_queries:
            issues.append(f"–ö–†–ò–¢–ò–ß–ù–û: {len(slow_queries)} –∑–∞–ø—Ä–æ—Å–æ–≤ –º–µ–¥–ª–µ–Ω–Ω–µ–µ 1 —Å–µ–∫—É–Ω–¥—ã!")
        
        # –í—ã—Å–æ–∫–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ CPU
        if metrics.cpu_usage_percent > 90:
            issues.append(f"–ö–†–ò–¢–ò–ß–ù–û: CPU –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ {metrics.cpu_usage_percent:.1f}%!")
        
        if issues:
            self.logger.critical("–û–ë–ù–ê–†–£–ñ–ï–ù–´ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–ë–õ–ï–ú–´:")
            for issue in issues:
                self.logger.critical(f"  üö® {issue}")
    
    def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ"""
        if self.connection:
            self.connection.close()
            self.logger.info("–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –∑–∞–∫—Ä—ã—Ç–æ")

def main():
    parser = argparse.ArgumentParser(description='–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ë–î ReplyX')
    parser.add_argument('--db-url', help='URL –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î')
    parser.add_argument('--interval', type=int, default=5, 
                       help='–ò–Ω—Ç–µ—Ä–≤–∞–ª –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –≤ –º–∏–Ω—É—Ç–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 5)')
    parser.add_argument('--max-iterations', type=int,
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ)')
    parser.add_argument('--output-dir', type=Path,
                       help='–ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫')
    parser.add_argument('--single-shot', action='store_true',
                       help='–°–æ–±—Ä–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –æ–¥–∏–Ω —Ä–∞–∑ –∏ –≤—ã–π—Ç–∏')
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä
    monitor = DatabaseMonitor(args.db_url)
    
    try:
        if args.single_shot:
            # –û–¥–Ω–æ—Ä–∞–∑–æ–≤—ã–π —Å–±–æ—Ä –º–µ—Ç—Ä–∏–∫
            metrics = monitor.collect_metrics()
            if metrics:
                monitor.save_metrics(metrics, args.output_dir)
                report = monitor.generate_report(metrics)
                print(report)
        else:
            # –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
            monitor.start_monitoring(
                interval_minutes=args.interval,
                max_iterations=args.max_iterations
            )
    finally:
        monitor.close()

if __name__ == '__main__':
    main()